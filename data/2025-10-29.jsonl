{"id": "2510.24649", "pdf": "https://arxiv.org/pdf/2510.24649", "abs": "https://arxiv.org/abs/2510.24649", "authors": ["James N. Cobley"], "title": "Local Electromagnetic Fields Enable Fast Redox Sensing by Physically Accelerating Cysteine Oxidation", "categories": ["q-bio.BM"], "comment": "11 pages, 2 figures", "summary": "Hydrogen peroxide oxidises cysteine residues to control protein function, yet\nbulk rate constants predict hours for changes that occur in cells in seconds.\nHere, this work shows that local electromagnetic fields (EMFs), ubiquitous in\nproteins, membranes and nanodomains, can lawfully modulate the Eyring barrier\nand orientate reactants, accelerating cysteine oxidation without changing the\nunderlying chemistry. Embedding a field term into the Eyring expression,\ndemonstrated that plausible local EMFs with realistic dipole changes accelerate\nrate constants by orders of magnitude. This local acceleration reconciles the\ndiscrepancy between predicted vs. observed rates of H2O2-mediated cysteine\noxidation. The framework generates falsifiable predictions, such as vibrational\nStark readouts in thiolate peroxide complexes should fall within predicted\nranges, and reframes rate-constants as mutable, field conditioned parameters.\nCysteine redox sensing is fast not because the chemistry is exotic, but because\nthe physics is local."}
{"id": "2510.23679", "pdf": "https://arxiv.org/pdf/2510.23679", "abs": "https://arxiv.org/abs/2510.23679", "authors": ["Simone Colli", "Emiliano Maresi", "Vincenzo Bonnici"], "title": "PanDelos-plus: A parallel algorithm for computing sequence homology in pangenomic analysis", "categories": ["q-bio.GN", "cs.DC"], "comment": null, "summary": "The identification of homologous gene families across multiple genomes is a\ncentral task in bacterial pangenomics traditionally requiring computationally\ndemanding all-against-all comparisons. PanDelos addresses this challenge with\nan alignment-free and parameter-free approach based on k-mer profiles,\ncombining high speed, ease of use, and competitive accuracy with\nstate-of-the-art methods. However, the increasing availability of genomic data\nrequires tools that can scale efficiently to larger datasets. To address this\nneed, we present PanDelos-plus, a fully parallel, gene-centric redesign of\nPanDelos. The algorithm parallelizes the most computationally intensive phases\n(Best Hit detection and Bidirectional Best Hit extraction) through data\ndecomposition and a thread pool strategy, while employing lightweight data\nstructures to reduce memory usage. Benchmarks on synthetic datasets show that\nPanDelos-plus achieves up to 14x faster execution and reduces memory usage by\nup to 96%, while maintaining accuracy. These improvements enable\npopulation-scale comparative genomics to be performed on standard multicore\nworkstations, making large-scale bacterial pangenome analysis accessible for\nroutine use in everyday research."}
{"id": "2510.24029", "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios."}
{"id": "2510.24602", "pdf": "https://arxiv.org/pdf/2510.24602", "abs": "https://arxiv.org/abs/2510.24602", "authors": ["Federica Ferretti", "Mehran Kardar", "Arvind Murugan"], "title": "Learning to generalize in evolution through annealed population heterogeneity", "categories": ["q-bio.PE", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "Evolutionary systems must learn to generalize, often extrapolating from a\nlimited set of selective conditions to anticipate future environmental changes.\nThe mechanisms enabling such generalization remain poorly understood, despite\ntheir importance to predict ecological robustness, drug resistance, or design\nfuture-proof vaccination strategies. Here, we demonstrate that annealed\npopulation heterogeneity, wherein distinct individuals in the population\nexperience different instances of a complex environment over time, can act as a\nform of implicit regularization and facilitate evolutionary generalization.\nMathematically, annealed heterogeneity introduces a variance-weighted\ndemographic noise term that penalizes across-environment fitness variance and\neffectively rescales the population size, thereby biasing evolution toward\ngeneralist solutions. This process is indeed analogous to a variant of the\nmini-batching strategy employed in stochastic gradient descent, where an\neffective multiplicative noise produces an inductive bias by triggering\nnoise-induced transitions.\n  Through numerical simulations and theoretical analysis we discuss the\nconditions under which variation in how individuals experience environmental\nselection can naturally promote evolutionary strategies that generalize across\nenvironments and anticipate novel challenges."}
{"id": "2510.23687", "pdf": "https://arxiv.org/pdf/2510.23687", "abs": "https://arxiv.org/abs/2510.23687", "authors": ["Anna Hinterberger", "Jonas Bohn", "Dasha Trofimova", "Nicolas Knabe", "Julia Dettling", "Tobias Norajitra", "Fabian Isensee", "Johannes Betge", "Stefan O. Schönberg", "Dominik Nörenberg", "Sergio Grosu", "Sonja Loges", "Ralf Floca", "Jakob Nikolas Kather", "Klaus Maier-Hein", "Freba Grawe"], "title": "Gut decisions based on the liver: A radiomics approach to boost colorectal cancer screening", "categories": ["q-bio.QM", "eess.IV"], "comment": "Equal contribution between first, second, fifteenth, and sixteenth\n  authors", "summary": "Non-invasive colorectal cancer (CRC) screening represents a key opportunity\nto improve colonoscopy participation rates and reduce CRC mortality. This study\nexplores the potential of the gut-liver axis for predicting colorectal\nneoplasia through liver-derived radiomic features extracted from routine CT\nimages as a novel opportunistic screening approach. In this retrospective\nstudy, we analyzed data from 1,997 patients who underwent colonoscopy and\nabdominal CT. Patients either had no colorectal neoplasia (n=1,189) or\ncolorectal neoplasia (n_total=808; adenomas n=423, CRC n=385). Radiomics\nfeatures were extracted from 3D liver segmentations using the Radiomics\nProcessing ToolKit (RPTK), which performed feature extraction, filtering, and\nclassification. The dataset was split into training (n=1,397) and test (n=600)\ncohorts. Five machine learning models were trained with 5-fold cross-validation\non the 20 most informative features, and the best model ensemble was selected\nbased on the validation AUROC. The best radiomics-based XGBoost model achieved\na test AUROC of 0.810, clearly outperforming the best clinical-only model (test\nAUROC: 0.457). Subclassification between colorectal cancer and adenoma showed\nlower accuracy (test AUROC: 0.674). Our findings establish proof-of-concept\nthat liver-derived radiomics from routine abdominal CT can predict colorectal\nneoplasia. Beyond offering a pragmatic, widely accessible adjunct to CRC\nscreening, this approach highlights the gut-liver axis as a novel biomarker\nsource for opportunistic screening and sparks new mechanistic hypotheses for\nfuture translational research."}
{"id": "2510.24647", "pdf": "https://arxiv.org/pdf/2510.24647", "abs": "https://arxiv.org/abs/2510.24647", "authors": ["Hugo Rydel-Johnston", "Alex Kafkas"], "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics."}
{"id": "2510.23975", "pdf": "https://arxiv.org/pdf/2510.23975", "abs": "https://arxiv.org/abs/2510.23975", "authors": ["Kevin Michalewicz", "Mauricio Barahona", "Barbara Bravi"], "title": "Machine learning approaches for interpretable antibody property prediction using structural data", "categories": ["q-bio.QM", "physics.bio-ph", "stat.ME", "stat.ML"], "comment": null, "summary": "Understanding the relationship between antibody sequence, structure and\nfunction is essential for the design of antibody-based therapeutics and\nresearch tools. Recently, machine learning (ML) models mostly based on the\napplication of large language models to sequence information have been\ndeveloped to predict antibody properties. Yet there are open directions to\nincorporate structural information, not only to enhance prediction but also to\noffer insights into the underlying molecular mechanisms. This chapter provides\nan overview of these approaches and describes two ML frameworks that integrate\nstructural data (via graph representations) with neural networks to predict\nproperties of antibodies: ANTIPASTI predicts binding affinity (a global\nproperty) whereas INFUSSE predicts residue flexibility (a local property). We\nsurvey the principles underpinning these models; the ways in which they encode\nstructural knowledge; and the strategies that can be used to extract\nbiologically relevant statistical signals that can help discover and\ndisentangle molecular determinants of the properties of interest."}
{"id": "2510.24709", "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system."}
{"id": "2510.23639", "pdf": "https://arxiv.org/pdf/2510.23639", "abs": "https://arxiv.org/abs/2510.23639", "authors": ["Jonathan Amar", "Edward Liu", "Alessandra Breschi", "Liangliang Zhang", "Pouya Kheradpour", "Sylvia Li", "Lisa Soleymani Lehmann", "Alessandro Giulianelli", "Matt Edwards", "Yugang Jia", "David Nola", "Raghav Mani", "Pankaj Vats", "Jesse Tetreault", "T. J. Chen", "Cory Y. McLean"], "title": "Integrating Genomics into Multimodal EHR Foundation Models", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare."}
{"id": "2510.24053", "pdf": "https://arxiv.org/pdf/2510.24053", "abs": "https://arxiv.org/abs/2510.24053", "authors": ["Jacob B. Roberts", "Catherine R. Ji", "Isaac Donnell", "Thomas D. Young", "Allison N. Pearson", "Graham A. Hudson", "Leah S. Keiser", "Mia Wesselkamper", "Peter H. Winegar", "Janik Ludwig", "Sarah H. Klass", "Isha V. Sheth", "Ezechinyere C. Ukabiala", "Maria C. T. Astolfi", "Benjamin Eysenbach", "Jay D. Keasling"], "title": "Low-N Protein Activity Optimization with FolDE", "categories": ["cs.LG", "q-bio.QM"], "comment": "18 pages, 4 figures. Preprint. Open-source software available at\n  https://github.com/JBEI/foldy", "summary": "Proteins are traditionally optimized through the costly construction and\nmeasurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)\nalleviates that cost by predicting the best improvements and iteratively\ntesting mutants to inform predictions. However, existing ALDE methods face a\ncritical limitation: selecting the highest-predicted mutants in each round\nyields homogeneous training data insufficient for accurate prediction models in\nsubsequent rounds. Here we present FolDE, an ALDE method designed to maximize\nend-of-campaign success. In simulations across 20 protein targets, FolDE\ndiscovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)\nand is 55% more likely to find top 1% mutants. FolDE achieves this primarily\nthrough naturalness-based warm-starting, which augments limited activity\nmeasurements with protein language model outputs to improve activity\nprediction. We also introduce a constant-liar batch selector, which improves\nbatch diversity; this is important in multi-mutation campaigns but had limited\neffect in our benchmarks. The complete workflow is freely available as\nopen-source software, making efficient protein optimization accessible to any\nlaboratory."}
{"id": "2510.24359", "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."}
{"id": "2510.24670", "pdf": "https://arxiv.org/pdf/2510.24670", "abs": "https://arxiv.org/abs/2510.24670", "authors": ["Genesis Research Team", "Alejandro Dobles", "Nina Jovic", "Kenneth Leidal", "Pranav Murugan", "David C. Williams", "Drausin Wulsin", "Nate Gruver", "Christina X. Ji", "Korrawat Pruegsanusak", "Gianluca Scarpellini", "Ansh Sharma", "Wojciech Swiderski", "Andrea Bootsma", "Richard Strong Bowen", "Charlotte Chen", "Jamin Chen", "Marc André Dämgen", "Roy Tal Dew", "Benjamin DiFrancesco", "J. D. Fishman", "Alla Ivanova", "Zach Kagin", "David Li-Bland", "Zuli Liu", "Igor Morozov", "Jeffrey Ouyang-Zhang", "Frank C. Pickard IV", "Kushal S. Shah", "Ben Shor", "Gabriel Monteiro da Silva", "Maxx Tessmer", "Carl Tilbury", "Cyr Vetcher", "Daniel Zeng", "Maruan Al-Shedivat", "Aleksandra Faust", "Evan N. Feinberg", "Michael V. LeVine", "Matteus Pan"], "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining."}
