{"id": "2510.20840", "pdf": "https://arxiv.org/pdf/2510.20840", "abs": "https://arxiv.org/abs/2510.20840", "authors": ["Martin Bicher", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Niki Popper"], "title": "GEPOC ABM, Generic Population Concept -- Agent-Based Model, Version 2.2", "categories": ["q-bio.PE", "91D20", "I.6.3; I.6.5; G.3; J.3; J.4"], "comment": "50 pages, 6 figures, 16 tables", "summary": "The Generic Population Concept - Agent-Based Model, henceforth short, GEPOC\nABM, is one of the models within GEPOC, a generic concept to model a country's\npopulation and its dynamics using causal modelling approaches. The model is\nwell established and had already proven its worth in various use cases from\nevaluation of MMR vaccination rates to SARS-CoV-2 epidemics modelling. In this\nwork we will reproducibly specify the base model, to be specific, version 2.2\nof it, and several extensions. The base model GEPOC ABM depicts the population\nof a country with the features sex and age. It uses a co-simulation-inspired\ntime-update, where person-level discrete-event simulators are synchronised by a\nsimulation layer at macro-steps, making the approach amenable to\nparallelization. A core design choice is structuring person agents around the\nlife-year rather than the calendar year; accordingly, each agent schedules\ndemographic events annually on their birthday. To expand the model's\ncapabilities beyond basic demographic features, GEPOC ABM Geography adds a\nresidence feature in the form of geographical coordinates. Further extensions\ninclude GEPOC ABM IM, which adds internal migration processes in three\nvariants, and GEPOC ABM CL, which models locations where agents may have\ncontacts with each other. In this definition we solely specify the conceptual\nmodels and do not go into any details with respect to implementation or\ngathering/processing of parametrisation data."}
{"id": "2510.21371", "pdf": "https://arxiv.org/pdf/2510.21371", "abs": "https://arxiv.org/abs/2510.21371", "authors": ["Javier López-de-la-Cruz", "Susana Merchán", "Felipe Rivero", "Javier Rodrigo"], "title": "SIR models with demography, random transmission coefficient and non-autonomous vaccination rate", "categories": ["q-bio.PE", "math.DS", "34A12, 34A34, 34F10, 34D45"], "comment": null, "summary": "In this paper we investigate the asymptotic behavior of some SIR models\nincorporating demography, bounded random transmission coefficient and a\ntime-dependent vaccination strategy targeting the susceptible population. In\nthis setting, we establish the existence and uniqueness of non-negative global\nsolution of the models and derive conditions under which either the disease is\neradicated or becomes endemic. In addition, the theoretical results are further\nillustrated by several numerical simulations."}
{"id": "2510.21134", "pdf": "https://arxiv.org/pdf/2510.21134", "abs": "https://arxiv.org/abs/2510.21134", "authors": ["Kevin Church", "Kevin Constantineau", "Jean-Philippe Lessard"], "title": "Spatially inhomogeneous two-cycles in an integrodifference equation", "categories": ["math.DS", "math.FA", "q-bio.PE"], "comment": null, "summary": "In this work, we prove the existence of a 2-cycle in an integrodifference\nequation with a Laplace kernel and logistic growth function, connecting two\nnon-trivial fixed points of the second iterate of the logistic map in the\nnon-chaotic regime. This model was first studied by Kot (1992), and the 2-cycle\nwe establish corresponds to one numerically observed by Bourgeois, Leblanc, and\nLutscher (2018) for the Ricker growth function. We provide strong evidence that\nthe 2-cycle for the Ricker growth function can be rigorously proven using a\nsimilar approach. Finally, we present numerical results indicating that both\n2-cycles exhibit spectral stability."}
{"id": "2510.20866", "pdf": "https://arxiv.org/pdf/2510.20866", "abs": "https://arxiv.org/abs/2510.20866", "authors": ["Christopher S. Bird", "Antonio P. L. Bo", "Wei Lu", "Taylor J. M. Dick"], "title": "Ultra-wideband radar to measure in vivo musculoskeletal forces", "categories": ["q-bio.QM"], "comment": "25 pages, 6 main figures, 5 extended data figures", "summary": "Accurate measures of musculoskeletal forces are critical for clinicians,\nbiomechanists, and engineers, yet direct measurement is highly invasive and\ncurrent estimation methods remain limited in accuracy. Here, we demonstrate the\napplication of ultra-wideband radar to non-invasively estimate musculoskeletal\nforces by measuring changes in the electromagnetic properties of contracting\nmuscles, in muscles with different structural properties, during various static\nand dynamic conditions, and in the presence of fatigue. First, we show that\nultra-wideband radar scans of muscle can reliably track isometric force in a\nunipennate knee extensor (vastus lateralis) and a bipennate ankle dorsiflexor\n(tibialis anterior). Next, we integrate radar signals within machine-learning\nand linear models to estimate musculoskeletal forces during fatiguing\nisometric, and dynamic knee extension contractions, with exceptional accuracy\n(test R2>0.984; errors<3.3%). Finally, we identify frequency-dependent effects\nof musculoskeletal forces on ultra-wideband radar signals, that are independent\nof physiological and structural features known to influence muscle force.\nTogether, these findings establish ultra-wideband radar as a powerful,\nnon-invasive approach for quantifying in vivo musculoskeletal forces, with\ntransformative potential for wearable assistive technologies, biomechanics, and\nrehabilitation."}
{"id": "2510.20831", "pdf": "https://arxiv.org/pdf/2510.20831", "abs": "https://arxiv.org/abs/2510.20831", "authors": ["Mehrnaz Asadi", "Sina Javadzadeh", "Rahil Soroushmojdehi", "S. Alireza Seyyed Mousavi", "Terence D. Sanger"], "title": "BACE: Behavior-Adaptive Connectivity Estimation for Interpretable Graphs of Neural Dynamics", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Understanding how distributed brain regions coordinate to produce behavior\nrequires models that are both predictive and interpretable. We introduce\nBehavior-Adaptive Connectivity Estimation (BACE), an end-to-end framework that\nlearns phase-specific, directed inter-regional connectivity directly from\nmulti-region intracranial local field potentials (LFP). BACE aggregates many\nmicro-contacts within each anatomical region via per-region temporal encoders,\napplies a learnable adjacency specific to each behavioral phase, and is trained\non a forecasting objective. On synthetic multivariate time series with known\ngraphs, BACE accurately recovers ground-truth directed interactions while\nachieving forecasting performance comparable to state-of-the-art baselines.\nApplied to human subcortical LFP recorded simultaneously from eight regions\nduring a cued reaching task, BACE yields an explicit connectivity matrix for\neach within-trial behavioral phase. The resulting behavioral phase-specific\ngraphs reveal behavior-aligned reconfiguration of inter-regional influence and\nprovide compact, interpretable adjacency matrices for comparing network\norganization across behavioral phases. By linking predictive success to\nexplicit connectivity estimates, BACE offers a practical tool for generating\ndata-driven hypotheses about the dynamic coordination of subcortical regions\nduring behavior."}
{"id": "2510.21281", "pdf": "https://arxiv.org/pdf/2510.21281", "abs": "https://arxiv.org/abs/2510.21281", "authors": ["Christian Salomonsen", "Kristoffer K. Wickstrøm", "Samuel Kuttner", "Elisabeth Wetzer"], "title": "Physics-Informed Deep Learning for Improved Input Function Estimation in Motion-Blurred Dynamic [${}^{18}$F]FDG PET Images", "categories": ["q-bio.QM", "cs.CV"], "comment": "12 pages, 4 figures, 1 table. Preprint: Accepted to PRIME @ MICCAI\n  2025. This is the submitted (pre-review) version (url:\n  https://openreview.net/forum?id=twg1nba5ep)", "summary": "Kinetic modeling enables \\textit{in vivo} quantification of tracer uptake and\nglucose metabolism in [${}^{18}$F]Fluorodeoxyglucose ([${}^{18}$F]FDG) dynamic\npositron emission tomography (dPET) imaging of mice. However, kinetic modeling\nrequires the accurate determination of the arterial input function (AIF) during\nimaging, which is time-consuming and invasive. Recent studies have shown the\nefficacy of using deep learning to directly predict the input function,\nsurpassing established methods such as the image-derived input function (IDIF).\nIn this work, we trained a physics-informed deep learning-based input function\nprediction model (PIDLIF) to estimate the AIF directly from the PET images,\nincorporating a kinetic modeling loss during training. The proposed method uses\na two-tissue compartment model over two regions, the myocardium and brain of\nthe mice, and is trained on a dataset of 70 [${}^{18}$F]FDG dPET images of mice\naccompanied by the measured AIF during imaging. The proposed method had\ncomparable performance to the network without a physics-informed loss, and when\nsudden movement causing blurring in the images was simulated, the PIDLIF model\nmaintained high performance in severe cases of image degradation. The proposed\nphysics-informed method exhibits an improved robustness that is promoted by\nphysically constraining the problem, enforcing consistency for\nout-of-distribution samples. In conclusion, the PIDLIF model offers insight\ninto the effects of leveraging physiological distribution mechanics in mice to\nguide a deep learning-based AIF prediction network in images with severe\ndegradation as a result of blurring due to movement during imaging."}
{"id": "2510.20835", "pdf": "https://arxiv.org/pdf/2510.20835", "abs": "https://arxiv.org/abs/2510.20835", "authors": ["Dezhi Luo", "Qingying Gao", "Hokin Deng"], "title": "Rethinking the Simulation vs. Rendering Dichotomy: No Free Lunch in Spatial World Modelling", "categories": ["q-bio.NC"], "comment": "Accepted at NeurIPS 2025 Workshop on Space in Vision, Language, and\n  Embodied AI", "summary": "Spatial world models, representations that support flexible reasoning about\nspatial relations, are central to developing computational models that could\noperate in the physical world, but their precise mechanistic underpinnings are\nnuanced by the borrowing of underspecified or misguided accounts of human\ncognition. This paper revisits the simulation versus rendering dichotomy and\ndraws on evidence from aphantasia to argue that fine-grained perceptual content\nis critical for model-based spatial reasoning. Drawing on recent research into\nthe neural basis of visual awareness, we propose that spatial simulation and\nperceptual experience depend on shared representational geometries captured by\nhigher-order indices of perceptual relations. We argue that recent developments\nin embodied AI support this claim, where rich perceptual details improve\nperformance on physics-based world engagements. To this end, we call for the\ndevelopment of architectures capable of maintaining structured perceptual\nrepresentations as a step toward spatial world modelling in AI."}
{"id": "2510.21484", "pdf": "https://arxiv.org/pdf/2510.21484", "abs": "https://arxiv.org/abs/2510.21484", "authors": ["Uwe Schmitt", "Jethro L. Hemmann", "Nicola Zamboni", "Julia A. Vorholt", "Patrick Kiefer"], "title": "eMZed 3: flexible and interactive development of scalable LC-MS/MS data analysis workflows in Python", "categories": ["q-bio.QM"], "comment": "7 pages, 1 figure", "summary": "Liquid chromatography-mass spectrometry (LC-MS/MS) data analysis requires\nadaptable software solutions to meet diverse analytical needs. We present eMZed\n3, a modern Python framework for flexible and interactive analysis of LC-MS/MS\ndata. eMZed 3 enables users to develop scalable workflows tailored to their\nspecific requirements while leveraging Python's extensive ecosystem of\nlibraries. Building on its predecessor, eMZed 3 is now Python 3-based and\nincludes substantial enhancements, including support for chromatogram-based\nLC-MS data, a new SQLite-based backend supporting optional out-of-memory\nprocessing, and rich interactive visualization tools. Compared to the previous\nversion, eMZed 3 is now split into three packages: emzed (core\nfunctionalities), emzed-gui (interactive data visualization), and emzed-spyder\n(an integrated development environment). This modular architecture allows\nstraightforward integration of the emzed core library into headless Python\nenvironments, including computational notebooks (such as Jupyter) or\nhigh-performance computing clusters. eMZed 3 incorporates well-established\nlibraries such as OpenMS, and is highly suited for both targeted and untargeted\nmetabolomics. Overall, eMZed 3 supports the efficient development of scalable\nand reproducible LC-MS data analysis and is accessible to both novice and\nadvanced programmers.\n  Availability and Implementation: eMZed 3 and its documentation are freely\navailable at https://emzed.ethz.ch, the source code is hosted at\nhttps://gitlab.com/groups/emzed3. An online-executable example workflow is\navailable on Binder at:\nhttps://mybinder.org/v2/gl/emzed3%2Femzed-example-workflow/HEAD?labpath=example.ipynb."}
{"id": "2510.20839", "pdf": "https://arxiv.org/pdf/2510.20839", "abs": "https://arxiv.org/abs/2510.20839", "authors": ["Warisa Sritriratanarak", "Paulo Garcia"], "title": "Consciousness, natural and artificial: an evolutionary advantage for reasoning on reactive substrates", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Precisely defining consciousness and identifying the mechanisms that effect\nit is a long-standing question, particularly relevant with advances in\nartificial intelligence. The scientific community is divided between\nphysicalism and natural dualism. Physicalism posits consciousness is a physical\nprocess that can be modeled computationally; natural dualism rejects this\nhypothesis. Finding a computational model has proven elusive, particularly\nbecause of conflation of consciousness with other cognitive capabilities\nexhibited by humans, such as intelligence and physiological sensations. Here we\nshow such a computational model that precisely models consciousness, natural or\nartificial, identifying the structural and functional mechanisms that effect\nit, confirming the physicalism hypothesis. We found such a model is obtainable\nwhen including the underlying (biological or digital) substrate and accounting\nfor reactive behavior in substrate sub-systems (e.g., autonomous physiological\nresponses). Results show that, unlike all other computational processes,\nconsciousness is not independent of its substrate and possessing it is an\nevolutionary advantage for intelligent entities. Our result shows there is no\nimpediment to the realization of fully artificial consciousness but,\nsurprisingly, that it is also possible to realize artificial intelligence of\narbitrary level without consciousness whatsoever, and that there is no\nadvantage in imbuing artificial systems with consciousness."}
{"id": "2510.21280", "pdf": "https://arxiv.org/pdf/2510.21280", "abs": "https://arxiv.org/abs/2510.21280", "authors": ["Christiaan M. Geldenhuys", "Günther Tonitz", "Thomas R. Niesler"], "title": "WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "q-bio.QM"], "comment": null, "summary": "While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline."}
{"id": "2510.20846", "pdf": "https://arxiv.org/pdf/2510.20846", "abs": "https://arxiv.org/abs/2510.20846", "authors": ["Dennis Tang", "Jon Donnelly", "Alina Jade Barnett", "Lesia Semenova", "Jin Jing", "Peter Hadar", "Ioannis Karakis", "Olga Selioutski", "Kehan Zhao", "M. Brandon Westover", "Cynthia Rudin"], "title": "This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "comment": "MICCAI 2025", "summary": "The presence of interictal epileptiform discharges (IEDs) in\nelectroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even\ntrained neurologists find detecting IEDs difficult, leading many practitioners\nto turn to machine learning for help. While existing machine learning\nalgorithms can achieve strong accuracy on this task, most models are\nuninterpretable and cannot justify their conclusions. Absent the ability to\nunderstand model reasoning, doctors cannot leverage their expertise to identify\nincorrect model predictions and intervene accordingly. To improve the\nhuman-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable\nmodel that follows a simple case-based reasoning process. ProtoEEG-kNN reasons\nby comparing an EEG to similar EEGs from the training set and visually\ndemonstrates its reasoning both in terms of IED morphology (shape) and spatial\ndistribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art\naccuracy in IED detection while providing explanations that experts prefer over\nexisting approaches."}
{"id": "2510.21664", "pdf": "https://arxiv.org/pdf/2510.21664", "abs": "https://arxiv.org/abs/2510.21664", "authors": ["Riya Gupta", "Yiwei Zong", "Dennis H. Murphree"], "title": "Foundation Models in Dermatopathology: Skin Tissue Classification", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The rapid generation of whole-slide images (WSIs) in dermatopathology\nnecessitates automated methods for efficient processing and accurate\nclassification. This study evaluates the performance of two foundation models,\nUNI and Virchow2, as feature extractors for classifying WSIs into three\ndiagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level\nembeddings were aggregated into slide-level features using a mean-aggregation\nstrategy and subsequently used to train multiple machine learning classifiers,\nincluding logistic regression, gradient-boosted trees, and random forest\nmodels. Performance was assessed using precision, recall, true positive rate,\nfalse positive rate, and the area under the receiver operating characteristic\ncurve (AUROC) on the test set. Results demonstrate that patch-level features\nextracted using Virchow2 outperformed those extracted via UNI across most\nslide-level classifiers, with logistic regression achieving the highest\naccuracy (90%) for Virchow2, though the difference was not statistically\nsignificant. The study also explored data augmentation techniques and image\nnormalization to enhance model robustness and generalizability. The\nmean-aggregation approach provided reliable slide-level feature\nrepresentations. All experimental results and metrics were tracked and\nvisualized using WandB.ai, facilitating reproducibility and interpretability.\nThis research highlights the potential of foundation models for automated WSI\nclassification, providing a scalable and effective approach for\ndermatopathological diagnosis while paving the way for future advancements in\nslide-level representation learning."}
{"id": "2510.20847", "pdf": "https://arxiv.org/pdf/2510.20847", "abs": "https://arxiv.org/abs/2510.20847", "authors": ["Jialin Wu", "Shreya Saha", "Yiqing Bo", "Meenakshi Khosla"], "title": "Integrated representational signatures strengthen specificity in brains and models", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "The extent to which different neural or artificial neural networks (models)\nrely on equivalent representations to support similar tasks remains a central\nquestion in neuroscience and machine learning. Prior work has typically\ncompared systems using a single representational similarity metric, yet each\ncaptures only one facet of representational structure. To address this, we\nleverage a suite of representational similarity metrics-each capturing a\ndistinct facet of representational correspondence, such as geometry, unit-level\ntuning, or linear decodability-and assess brain region or model separability\nusing multiple complementary measures. Metrics that preserve geometric or\ntuning structure (e.g., RSA, Soft Matching) yield stronger region-based\ndiscrimination, whereas more flexible mappings such as Linear Predictivity show\nweaker separation. These findings suggest that geometry and tuning encode\nbrain-region- or model-family-specific signatures, while linearly decodable\ninformation tends to be more globally shared across regions or models. To\nintegrate these complementary representational facets, we adapt Similarity\nNetwork Fusion (SNF), a framework originally developed for multi-omics data\nintegration. SNF produces substantially sharper regional and model family-level\nseparation than any single metric and yields robust composite similarity\nprofiles. Moreover, clustering cortical regions using SNF-derived similarity\nscores reveals a clearer hierarchical organization that aligns closely with\nestablished anatomical and functional hierarchies of the visual\ncortex-surpassing the correspondence achieved by individual metrics."}
{"id": "2510.20848", "pdf": "https://arxiv.org/pdf/2510.20848", "abs": "https://arxiv.org/abs/2510.20848", "authors": ["Arnab Dey Sarkar", "Bard Ermentrout"], "title": "Multi-stable oscillations in cortical networks with two classes of inhibition", "categories": ["q-bio.NC", "math.DS"], "comment": null, "summary": "In the classic view of cortical rhythms, the interaction between excitatory\npyramidal neurons (E) and inhibitory parvalbumin neurons (I) has been shown to\nbe sufficient to generate gamma and beta band rhythms. However, it is now clear\nthat there are multiple inhibitory interneuron subtypes and that they play\nimportant roles in the generation of these rhythms. In this paper we develop a\nspiking network that consists of populations of E, I and an additional\ninterneuron type, the somatostatin (S) internerons that receive excitation from\nthe E cells and inhibit both the E cells and the I cells. These S cells are\nmodulated by a third inhibitory subtype, VIP neurons that receive inputs from\nother cortical areas. We reduce the spiking network to a system of nine\ndifferential equations that characterize the mean voltage, firing rate, and\nsynaptic conductance for each population and using this we find many instances\nof multiple rhythms within the network. Using tools from nonlinear dynamics, we\nexplore the roles of each of the two classes of inhibition as well as the role\nof the VIP modulation on the properties of these rhythms."}
{"id": "2510.20855", "pdf": "https://arxiv.org/pdf/2510.20855", "abs": "https://arxiv.org/abs/2510.20855", "authors": ["Guoying Sun", "Weiyu Guo", "Tong Shao", "Yang Yang", "Haijin Zeng", "Jie Liu", "Jingyong Su"], "title": "BrainCognizer: Brain Decoding with Human Visual Cognition Simulation for fMRI-to-Image Reconstruction", "categories": ["q-bio.NC"], "comment": "BIBM 2025 accepted", "summary": "Brain decoding is a key neuroscience field that reconstructs the visual\nstimuli from brain activity with fMRI, which helps illuminate how the brain\nrepresents the world. fMRI-to-image reconstruction has achieved impressive\nprogress by leveraging diffusion models. However, brain signals infused with\nprior knowledge and associations exhibit a significant information asymmetry\nwhen compared to raw visual features, still posing challenges for decoding fMRI\nrepresentations under the supervision of images. Consequently, the\nreconstructed images often lack fine-grained visual fidelity, such as missing\nattributes and distorted spatial relationships. To tackle this challenge, we\npropose BrainCognizer, a novel brain decoding model inspired by human visual\ncognition, which explores multi-level semantics and correlations without\nfine-tuning of generative models. Specifically, BrainCognizer introduces two\nmodules: the Cognitive Integration Module which incorporates prior human\nknowledge to extract hierarchical region semantics; and the Cognitive\nCorrelation Module which captures contextual semantic relationships across\nregions. Incorporating these two modules enhances intra-region semantic\nconsistency and maintains inter-region contextual associations, thereby\nfacilitating fine-grained brain decoding. Moreover, we quantitatively interpret\nour components from a neuroscience perspective and analyze the associations\nbetween different visual patterns and brain functions. Extensive quantitative\nand qualitative experiments demonstrate that BrainCognizer outperforms\nstate-of-the-art approaches on multiple evaluation metrics."}
{"id": "2510.20859", "pdf": "https://arxiv.org/pdf/2510.20859", "abs": "https://arxiv.org/abs/2510.20859", "authors": ["Craig Sanders", "Billy Dickson", "Sahaj Singh Maini", "Robert Nosofsky", "Zoran Tiganj"], "title": "Vision-language models learn the geometry of human perceptual space", "categories": ["q-bio.NC"], "comment": null, "summary": "In cognitive science and AI, a longstanding question is whether machines\nlearn representations that align with those of the human mind. While current\nmodels show promise, it remains an open question whether this alignment is\nsuperficial or reflects a deeper correspondence in the underlying dimensions of\nrepresentation. Here we introduce a methodology to probe the internal geometry\nof vision-language models (VLMs) by having them generate pairwise similarity\njudgments for a complex set of natural objects. Using multidimensional scaling,\nwe recover low-dimensional psychological spaces and find that their axes show a\nstrong correspondence with the principal axes of human perceptual space.\nCritically, when this AI-derived representational geometry is used as the input\nto a classic exemplar model of categorization, it predicts human classification\nbehavior more accurately than a space constructed from human judgments\nthemselves. This suggests that VLMs can capture an idealized or `denoised' form\nof human perceptual structure. Our work provides a scalable method to overcome\na measurement bottleneck in cognitive science and demonstrates that foundation\nmodels can learn a representational geometry that is functionally relevant for\nmodeling key aspects of human cognition, such as categorization."}
{"id": "2510.20966", "pdf": "https://arxiv.org/pdf/2510.20966", "abs": "https://arxiv.org/abs/2510.20966", "authors": ["Arina Medvedeva", "Edoardo Balzani", "Alex H Williams", "Stephen L Keeley"], "title": "Scalable inference of functional neural connectivity at submillisecond timescales", "categories": ["q-bio.NC", "stat.ML"], "comment": "Accepted at NeurIPS 2025", "summary": "The Poisson Generalized Linear Model (GLM) is a foundational tool for\nanalyzing neural spike train data. However, standard implementations rely on\ndiscretizing spike times into binned count data, limiting temporal resolution\nand scalability. Here, we develop Monte Carlo (MC) methods and polynomial\napproximations (PA) to the continuous-time analog of these models, and show\nthem to be advantageous over their discrete-time counterparts. Further, we\npropose using a set of exponentially scaled Laguerre polynomials as an\northogonal temporal basis, which improves filter identification and yields\nclosed-form integral solutions under the polynomial approximation. Applied to\nboth synthetic and real spike-time data from rodent hippocampus, our methods\ndemonstrate superior accuracy and scalability compared to traditional binned\nGLMs, enabling functional connectivity inference in large-scale neural\nrecordings that are temporally precise on the order of synaptic dynamical\ntimescales and in agreement with known anatomical properties of hippocampal\nsubregions. We provide open-source implementations of both MC and PA\nestimators, optimized for GPU acceleration, to facilitate adoption in the\nneuroscience community."}
{"id": "2510.21142", "pdf": "https://arxiv.org/pdf/2510.21142", "abs": "https://arxiv.org/abs/2510.21142", "authors": ["Ethan Hwang", "Hossein Adeli", "Wenxuan Guo", "Andrew Luo", "Nikolaus Kriegeskorte"], "title": "In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain", "categories": ["q-bio.NC"], "comment": null, "summary": "A fine-grained account of functional selectivity in the cortex is essential\nfor understanding how visual information is processed and represented in the\nbrain. Classical studies using designed experiments have identified multiple\ncategory-selective regions; however, these approaches rely on preconceived\nhypotheses about categories. Subsequent data-driven discovery methods have\nsought to address this limitation but are often limited by simple, typically\nlinear encoding models. We propose an in silico approach for data-driven\ndiscovery of novel category-selectivity hypotheses based on an encoder-decoder\ntransformer model. The architecture incorporates a brain-region to\nimage-feature cross-attention mechanism, enabling nonlinear mappings between\nhigh-dimensional deep network features and semantic patterns encoded in the\nbrain activity. We further introduce a method to characterize the selectivity\nof individual parcels by leveraging diffusion-based image generative models and\nlarge-scale datasets to synthesize and select images that maximally activate\neach parcel. Our approach reveals regions with complex, compositional\nselectivity involving diverse semantic concepts, which we validate in silico\nboth within and across subjects. Using a brain encoder as a \"digital twin\"\noffers a powerful, data-driven framework for generating and testing hypotheses\nabout visual selectivity in the human brain - hypotheses that can guide future\nfMRI experiments. Our code is available at:\nhttps://kriegeskorte-lab.github.io/in-silico-mapping/ ."}
{"id": "2510.21265", "pdf": "https://arxiv.org/pdf/2510.21265", "abs": "https://arxiv.org/abs/2510.21265", "authors": ["He Jiangshan", "Xie Hui", "Yang Yuqiang", "Jia Chunli", "Liang Dan", "Zhang Lianghua", "Wang Xiaoyu", "Luo Tianyi", "Dong Zexiao", "Yang Huiting", "Pan Yang", "Zhen Yuan", "Jiang Mingzhe", "Chen Xueli"], "title": "EEG Dynamic Microstate Patterns Induced by Pulsed Wave Transcranial Photobiomodulation Therapy", "categories": ["q-bio.NC"], "comment": "6 figures", "summary": "Transcranial photobiomodulation (tPBM) therapy is an emerging, non-invasive\nneuromodulation technique that has demonstrated considerable potential in the\nfield of neuropsychiatric disorders. Several studies have found that pulsed\nwave (PW) tPBM therapy yields superior biomodulatory effects. However, its\nneural mechanisms are still unknown which poses a significant barrier to the\ndevelopment of an optimized protocol. A randomized, single-blind study\nincluding 29 participants was conducted using a crossover design, with sham and\ncontinuous wave (CW) groups as controls. The EEG microstate analysis was\nutilized to explore the relative variations in temporal parameters and brain\nfunctional connectivity. To further elucidate the dynamic activity patterns of\nmicrostates, a 10-repeat 10-fold cross-validation with nine machine learning\nalgorithms and kernel Shapley additive explanations analysis was employed.\nResults indicated that the pulsed wave mode enhanced the global efficiency,\nlocal efficiency, and betweenness centrality of microstate C in brain\nfunctional networks as well as the mean durations parameter achieving a middle\nto large effect size, with superior effects compared to the sham and continuous\nwave groups. Furthermore, the support vector machine based on the radial basis\nfunction method with kernel Shapley additive explanations analysis demonstrated\nthe best performance with an area under the curve (AUC) reaching 0.956, and\nfound that the 8 of top-10 microstate features related to microstate C\ncontributed most significantly to the PW mode. In conclusion, the EEG\nmicrostate analysis found that PW tPBM therapy modulates the microstate\nC-specific patterns in the human brain, suggesting that microstate dynamics may\nserve as a state-dependent biomarker for the optimization of tPBM protocol."}
{"id": "2510.21588", "pdf": "https://arxiv.org/pdf/2510.21588", "abs": "https://arxiv.org/abs/2510.21588", "authors": ["Farhad Pashakhanloo"], "title": "Contribution of task-irrelevant stimuli to drift of neural representations", "categories": ["q-bio.NC", "cs.LG"], "comment": "NeurIPS 2025", "summary": "Biological and artificial learners are inherently exposed to a stream of data\nand experience throughout their lifetimes and must constantly adapt to, learn\nfrom, or selectively ignore the ongoing input. Recent findings reveal that,\neven when the performance remains stable, the underlying neural representations\ncan change gradually over time, a phenomenon known as representational drift.\nStudying the different sources of data and noise that may contribute to drift\nis essential for understanding lifelong learning in neural systems. However, a\nsystematic study of drift across architectures and learning rules, and the\nconnection to task, are missing. Here, in an online learning setup, we\ncharacterize drift as a function of data distribution, and specifically show\nthat the learning noise induced by task-irrelevant stimuli, which the agent\nlearns to ignore in a given context, can create long-term drift in the\nrepresentation of task-relevant stimuli. Using theory and simulations, we\ndemonstrate this phenomenon both in Hebbian-based learning -- Oja's rule and\nSimilarity Matching -- and in stochastic gradient descent applied to\nautoencoders and a supervised two-layer network. We consistently observe that\nthe drift rate increases with the variance and the dimension of the data in the\ntask-irrelevant subspace. We further show that this yields different\nqualitative predictions for the geometry and dimension-dependency of drift than\nthose arising from Gaussian synaptic noise. Overall, our study links the\nstructure of stimuli, task, and learning rule to representational drift and\ncould pave the way for using drift as a signal for uncovering underlying\ncomputation in the brain."}
{"id": "2510.20958", "pdf": "https://arxiv.org/pdf/2510.20958", "abs": "https://arxiv.org/abs/2510.20958", "authors": ["Asif Islam", "Farhan Ishtiaque", "Md. Muhyminul Haque", "Kaled Masukur Rahman", "Ravi Vaidyanathan", "Khondaker A. Mamun"], "title": "NeuroPilot: A Realtime Brain-Computer Interface system to enhance concentration of students in online learning", "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "comment": "11 pages, 5 figures and 3 tables", "summary": "Prevalence of online learning poses a vital challenge in real-time monitoring\nof students' concentration. Traditional methods such as questionnaire\nassessments require manual interventions and webcam-based monitoring fails to\nprovide accurate insights into learners' mental focus as they are deceived by\nmere screen fixation without cognitive engagement. Existing BCI-based\napproaches lack real-time validation and evaluation procedures. To address\nthese limitations, a Brain-Computer Interface (BCI) system is developed using a\nnon-invasive Electroencephalogram (EEG) headband, FocusCalm, to record\nbrainwave activity under attentive and non-attentive states. 20 minutes of data\nwere collected from each of 20 participants watching a pre-recorded educational\nvideo. The data validation employed a novel intra-video questionnaire\nassessment. Subsequently, collected signals were segmented (sliding window),\nfiltered (butterworth bandpass), and cleaned (removal of high-amplitude and EOG\nartifacts such as eye blinks). Time, frequency, wavelet and statistical\nfeatures have been extracted, followed by recursive feature elimination (RFE)\nwith Support vector machines (SVMs) to classify attention and non-attention\nstates. The leave-one-subject-out (LOSO) cross-validation accuracy has been\ntested to be 88.77%. The system provides feedback alerts upon non-attention\nstate detection and keeps focus profile logs. A pilot study was conducted to\nevaluate the effectiveness of real-time feedback. Five participants completed a\n10-minute session consisting of a 5-minute baseline phase without feedback\nfollowed by a 5-minute feedback phase, during which alerts were issued if\nparticipants remained non-attentive for approximately 8 consecutive seconds. A\npaired t-test (t = 5.73, p = 0.007) indicated a statistically significant\nimprovement in concentration during the feedback phase."}
{"id": "2510.21585", "pdf": "https://arxiv.org/pdf/2510.21585", "abs": "https://arxiv.org/abs/2510.21585", "authors": ["Yassine El Ouahidi", "Jonathan Lys", "Philipp Thölke", "Nicolas Farrugia", "Bastien Pasdeloup", "Vincent Gripon", "Karim Jerbi", "Giulia Lioi"], "title": "REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects", "categories": ["cs.LG", "q-bio.NC"], "comment": "Code available at: https://brain-bzh.github.io/reve/", "summary": "Foundation models have transformed AI by reducing reliance on task-specific\ndata through large-scale pretraining. While successful in language and vision,\ntheir adoption in EEG has lagged due to the heterogeneity of public datasets,\nwhich are collected under varying protocols, devices, and electrode\nconfigurations. Existing EEG foundation models struggle to generalize across\nthese variations, often restricting pretraining to a single setup, resulting in\nsuboptimal performance, in particular under linear probing. We present REVE\n(Representation for EEG with Versatile Embeddings), a pretrained model\nexplicitly designed to generalize across diverse EEG signals. REVE introduces a\nnovel 4D positional encoding scheme that enables it to process signals of\narbitrary length and electrode arrangement. Using a masked autoencoding\nobjective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets\nspanning 25,000 subjects, representing the largest EEG pretraining effort to\ndate. REVE achieves state-of-the-art results on 10 downstream EEG tasks,\nincluding motor imagery classification, seizure detection, sleep staging,\ncognitive load estimation, and emotion recognition. With little to no\nfine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal\nmodeling. We release code, pretrained weights, and tutorials to support\nstandardized EEG research and accelerate progress in clinical neuroscience."}
