{"id": "2510.17826", "pdf": "https://arxiv.org/pdf/2510.17826", "abs": "https://arxiv.org/abs/2510.17826", "authors": ["Carles Navarro", "Mariona Torrens", "Philipp Th√∂lke", "Stefan Doerr", "Gianni De Fabritiis"], "title": "Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Building a working mental model of a protein typically requires weeks of\nreading, cross-referencing crystal and predicted structures, and inspecting\nligand complexes, an effort that is slow, unevenly accessible, and often\nrequires specialized computational skills. We introduce \\emph{Speak to a\nProtein}, a new capability that turns protein analysis into an interactive,\nmultimodal dialogue with an expert co-scientist. The AI system retrieves and\nsynthesizes relevant literature, structures, and ligand data; grounds answers\nin a live 3D scene; and can highlight, annotate, manipulate and see the\nvisualization. It also generates and runs code when needed, explaining results\nin both text and graphics. We demonstrate these capabilities on relevant\nproteins, posing questions about binding pockets, conformational changes, or\nstructure-activity relationships to test ideas in real-time. \\emph{Speak to a\nProtein} reduces the time from question to evidence, lowers the barrier to\nadvanced structural analysis, and enables hypothesis generation by tightly\ncoupling language, code, and 3D structures. \\emph{Speak to a Protein} is freely\naccessible at https://open.playmolecule.org."}
{"id": "2510.17820", "pdf": "https://arxiv.org/pdf/2510.17820", "abs": "https://arxiv.org/abs/2510.17820", "authors": ["Markus Meister"], "title": "The Standard Model of the Retina", "categories": ["q-bio.NC"], "comment": "Submission to Vision Research, special issue on \"Retinal\n  Computations\"", "summary": "The scientific study of the retina has reached a remarkable state of\ncompletion. We can now explain many aspects of early visual processing based on\na relatively simple model of neural circuitry in the retina. The same model,\nwith different parameters, produces a great diversity of neural computations.\nIn this article I lay out what that \"standard model\" is and how it accounts for\nsuch a diversity of phenomena. The emergence of such a powerful standard model\nis unique in systems neuroscience, and I consider what conditions made it\npossible. The standard model now serves as a baseline from which to organize\nfuture retinal research, either by testing the model's assumptions directly, or\nby identifying phenomena that remain unexplained."}
{"id": "2510.18571", "pdf": "https://arxiv.org/pdf/2510.18571", "abs": "https://arxiv.org/abs/2510.18571", "authors": ["Gokturk Aytug Akarlar"], "title": "A Multi-Evidence Framework Rescues Low-Power Prognostic Signals and Rejects Statistical Artifacts in Cancer Genomics", "categories": ["q-bio.GN", "cs.LG", "92B15, 62P10, 62F10, 62H17", "J.3; I.2.6; G.3"], "comment": "17 pages (main text), 4 figures (main text), 7 supplementary figures,\n  4 supplementary tables. Focuses on a computational framework using causal\n  inference and biological validation for underpowered cancer genomic studies", "summary": "Motivation: Standard genome-wide association studies in cancer genomics rely\non statistical significance with multiple testing correction, but\nsystematically fail in underpowered cohorts. In TCGA breast cancer (n=967, 133\ndeaths), low event rates (13.8%) create severe power limitations, producing\nfalse negatives for known drivers and false positives for large passenger\ngenes. Results: We developed a five-criteria computational framework\nintegrating causal inference (inverse probability weighting, doubly robust\nestimation) with orthogonal biological validation (expression, mutation\npatterns, literature evidence). Applied to TCGA-BRCA mortality analysis,\nstandard Cox+FDR detected zero genes at FDR<0.05, confirming complete failure\nin underpowered settings. Our framework correctly identified RYR2 -- a cardiac\ngene with no cancer function -- as a false positive despite nominal\nsignificance (p=0.024), while identifying KMT2C as a complex candidate\nrequiring validation despite marginal significance (p=0.047, q=0.954). Power\nanalysis revealed median power of 15.1% across genes, with KMT2C achieving only\n29.8% power (HR=1.55), explaining borderline statistical significance despite\nstrong biological evidence. The framework distinguished true signals from\nartifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations\n(passenger signature) with no hotspots, while KMT2C showed 6.7% silent\nmutations with 31.4% truncating variants (driver signature). This\nmulti-evidence approach provides a template for analyzing underpowered cohorts,\nprioritizing biological interpretability over purely statistical significance.\n  Availability: All code and analysis pipelines available at\ngithub.com/akarlaraytu/causal-inference-for-cancer-genomics"}
{"id": "2510.17920", "pdf": "https://arxiv.org/pdf/2510.17920", "abs": "https://arxiv.org/abs/2510.17920", "authors": ["Bishal Chhetri", "B. V. Rathish Kumar"], "title": "CBINNS: Cancer Biology-Informed Neural Network for Unknown Parameter Estimation and Missing Physics Identification", "categories": ["q-bio.QM", "cs.AI"], "comment": "29 pages, 24 figures", "summary": "The dynamics of tumor-immune interactions within a complex tumor\nmicroenvironment are typically modeled using a system of ordinary differential\nequations or partial differential equations. These models introduce some\nunknown parameters that need to be estimated accurately and efficiently from\nthe limited and noisy experimental data. Moreover, due to the intricate\nbiological complexity and limitations in experimental measurements,\ntumor-immune dynamics are not fully understood, and therefore, only partial\nknowledge of the underlying physics may be available, resulting in unknown or\nmissing terms within the system of equations. In this study, we develop a\ncancer biology-informed neural network model(CBINN) to infer the unknown\nparameters in the system of equations as well as to discover the missing\nphysics from sparse and noisy measurements. We test the performance of the\nCBINN model on three distinct nonlinear compartmental tumor-immune models and\nevaluate its robustness across multiple synthetic noise levels. By harnessing\nthese highly nonlinear dynamics, our CBINN framework effectively estimates the\nunknown model parameters and uncovers the underlying physical laws or\nmathematical structures that govern these biological systems, even from\nscattered and noisy measurements. The models chosen here represent the dynamic\npatterns commonly observed in compartmental models of tumor-immune\ninteractions, thereby validating the generalizability and efficacy of our\nmethodology."}
{"id": "2510.17822", "pdf": "https://arxiv.org/pdf/2510.17822", "abs": "https://arxiv.org/abs/2510.17822", "authors": ["D. Halatsis", "P. Mamidanna", "J. Pereira", "D. Farina"], "title": "A Biophysical-Model-Informed Source Separation Framework For EMG Decomposition", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Recent advances in neural interfacing have enabled significant improvements\nin human-computer interaction, rehabilitation, and neuromuscular diagnostics.\nMotor unit (MU) decomposition from surface electromyography (sEMG) is a key\ntechnique for extracting neural drive information, but traditional blind source\nseparation (BSS) methods fail to incorporate biophysical constraints, limiting\ntheir accuracy and interpretability. In this work, we introduce a novel\nBiophysical-Model-Informed Source Separation (BMISS) framework, which\nintegrates anatomically accurate forward EMG models into the decomposition\nprocess. By leveraging MRI-based anatomical reconstructions and generative\nmodeling, our approach enables direct inversion of a biophysically accurate\nforward model to estimate both neural drive and motor neuron properties in an\nunsupervised manner. Empirical validation in a controlled simulated setting\ndemonstrates that BMISS achieves higher fidelity motor unit estimation while\nsignificantly reducing computational cost compared to traditional methods. This\nframework paves the way for non-invasive, personalized neuromuscular\nassessments, with potential applications in clinical diagnostics, prosthetic\ncontrol, and neurorehabilitation."}
{"id": "2510.18790", "pdf": "https://arxiv.org/pdf/2510.18790", "abs": "https://arxiv.org/abs/2510.18790", "authors": ["Kevin Michalewicz", "Chen Jin", "Philip Alexander Teare", "Tom Diethe", "Mauricio Barahona", "Barbara Bravi", "Asher Mullokandov"], "title": "Protein generation with embedding learning for motif diversification", "categories": ["q-bio.QM", "physics.bio-ph", "stat.ML"], "comment": null, "summary": "A fundamental challenge in protein design is the trade-off between generating\nstructural diversity while preserving motif biological function. Current\nstate-of-the-art methods, such as partial diffusion in RFdiffusion, often fail\nto resolve this trade-off: small perturbations yield motifs nearly identical to\nthe native structure, whereas larger perturbations violate the geometric\nconstraints necessary for biological function. We introduce Protein Generation\nwith Embedding Learning (PGEL), a general framework that learns\nhigh-dimensional embeddings encoding sequence and structural features of a\ntarget motif in the representation space of a diffusion model's frozen\ndenoiser, and then enhances motif diversity by introducing controlled\nperturbations in the embedding space. PGEL is thus able to loosen geometric\nconstraints while satisfying typical design metrics, leading to more diverse\nyet viable structures. We demonstrate PGEL on three representative cases: a\nmonomer, a protein-protein interface, and a cancer-related transcription factor\ncomplex. In all cases, PGEL achieves greater structural diversity, better\ndesignability, and improved self-consistency, as compared to partial diffusion.\nOur results establish PGEL as a general strategy for embedding-driven protein\ngeneration allowing for systematic, viable diversification of functional\nmotifs."}
{"id": "2510.17833", "pdf": "https://arxiv.org/pdf/2510.17833", "abs": "https://arxiv.org/abs/2510.17833", "authors": ["√Ångela L√≥pez-Cardona", "Sebasti√°n Idesis", "Mireia Masias-Bruns", "Sergi Abadal", "Ioannis Arapakis"], "title": "Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Do brains and language models converge toward the same internal\nrepresentations of the world? Recent years have seen a rise in studies of\nneural activations and model alignment. In this work, we review 25 fMRI-based\nstudies published between 2023 and 2025 and explicitly confront their findings\nwith two key hypotheses: (i) the Platonic Representation Hypothesis -- that as\nmodels scale and improve, they converge to a representation of the real world,\nand (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth)\nlayers often encode richer, more generalizable features. Our findings provide\nconverging evidence that models and brains may share abstract representational\nstructures, supporting both hypotheses and motivating further research on\nbrain-model alignment."}
{"id": "2510.18870", "pdf": "https://arxiv.org/pdf/2510.18870", "abs": "https://arxiv.org/abs/2510.18870", "authors": ["Jeffrey Ouyang-Zhang", "Pranav Murugan", "Daniel J. Diaz", "Gianluca Scarpellini", "Richard Strong Bowen", "Nate Gruver", "Adam Klivans", "Philipp Kr√§henb√ºhl", "Aleksandra Faust", "Maruan Al-Shedivat"], "title": "Triangle Multiplication Is All You Need For Biomolecular Structure Representations", "categories": ["q-bio.QM"], "comment": "Preprint", "summary": "AlphaFold has transformed protein structure prediction, but emerging\napplications such as virtual ligand screening, proteome-wide folding, and de\nnovo binder design demand predictions at a massive scale, where runtime and\nmemory costs become prohibitive. A major bottleneck lies in the Pairformer\nbackbone of AlphaFold3-style models, which relies on computationally expensive\ntriangular primitives-especially triangle attention-for pairwise reasoning. We\nintroduce Pairmixer, a streamlined alternative that eliminates triangle\nattention while preserving higher-order geometric reasoning capabilities that\nare critical for structure prediction. Pairmixer substantially improves\ncomputational efficiency, matching state-of-the-art structure predictors across\nfolding and docking benchmarks, delivering up to 4x faster inference on long\nsequences while reducing training cost by 34%. Its efficiency alleviates the\ncomputational burden of downstream applications such as modeling large protein\ncomplexes, high-throughput ligand and binder screening, and hallucination-based\ndesign. Within BoltzDesign, for example, Pairmixer delivers over 2x faster\nsampling and scales to sequences ~30% longer than the memory limits of\nPairformer."}
{"id": "2510.17837", "pdf": "https://arxiv.org/pdf/2510.17837", "abs": "https://arxiv.org/abs/2510.17837", "authors": ["Jonathan E. Rubin", "Justyna Signerska-Rynkowska", "Jonathan Touboul"], "title": "Dynamic threshold curves and response precision in forced excitable systems", "categories": ["q-bio.NC", "math.DS", "Primary: 37C60, 37N25, Secondary: 37A50, 92C20"], "comment": null, "summary": "We investigate here various properties of the responses of excitable systems\nsubject to periodic forcing and noise. While the properties of intrinsic\noscillators, subject to added periodic signals, are well understood, much less\nis known about the factors that determine the response precision of excitable\nunits, intrinsically at rest, when activated by periodic forcing and stochastic\nnoise. One motivation for considering this issue comes from the behavior of\nauditory neurons. These neurons reportedly have the ability to fire spikes in a\nprecise range of phases in response to incoming sound waves, a behavior for\nwhich the mechanism is unknown. To account for such a response precision, we\nintroduce the notion of dynamic threshold curve (DTC), which estimates at each\ntime the effective likelihood that noise will subsequently generate a spike.\nThe DTC effectively summarizes, in a single curve, a representation of the\nresponse precision of an excitable model, as we demonstrate by showing that the\ndistribution of spike times produced in this setting is well captured by the\nfirst passage time of a simple, Gaussian stochastic process to the distance to\nthe DTC. This result shows that peaks and troughs of the DTC, but also their\nslopes, convey fine information about spike timing in response to noise. In\nparticular, it explains properties of Type 2 and Type 3 excitable cells studied\npreviously and provides a framework to predict the DTC properties necessary to\nsupport the response precision of auditory neurons, as we illustrate in a\nwell-established auditory neuron model."}
{"id": "2510.18103", "pdf": "https://arxiv.org/pdf/2510.18103", "abs": "https://arxiv.org/abs/2510.18103", "authors": ["Nursultan Mamatov", "Philipp Kellmeyer"], "title": "Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "68T07, 92C50", "I.2.6; I.5.1; J.3"], "comment": "38 pages, 5 figures, 2 tables, 3 appendices", "summary": "Accurate early prediction of in-hospital mortality in intensive care units\n(ICUs) is essential for timely clinical intervention and efficient resource\nallocation. This study develops and evaluates machine learning models that\nintegrate both structured clinical data and unstructured textual information,\nspecifically discharge summaries and radiology reports, from the MIMIC-IV\ndatabase. We used LASSO and XGBoost for feature selection, followed by a\nmultivariate logistic regression trained on the top features identified by both\nmodels. Incorporating textual features using TF-IDF and BERT embeddings\nsignificantly improved predictive performance. The final logistic regression\nmodel, which combined structured and textual input, achieved an AUC of 0.918,\ncompared to 0.753 when using structured data alone, a relative improvement 22%.\nThe analysis of the decision curve demonstrated a superior standardized net\nbenefit in a wide range of threshold probabilities (0.2-0.8), confirming the\nclinical utility of the model. These results underscore the added prognostic\nvalue of unstructured clinical notes and support their integration into\ninterpretable feature-driven risk prediction models for ICU patients."}
{"id": "2510.18516", "pdf": "https://arxiv.org/pdf/2510.18516", "abs": "https://arxiv.org/abs/2510.18516", "authors": ["Sangyoon Bae", "Mehdi Azabou", "Jiook Cha", "Blake Richards"], "title": "Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Self-supervised learning (SSL) holds a great deal of promise for applications\nin neuroscience, due to the lack of large-scale, consistently labeled neural\ndatasets. However, most neural datasets contain heterogeneous populations that\nmix stable, predictable cells with highly stochastic, stimulus-contingent ones,\nwhich has made it hard to identify consistent activity patterns during SSL. As\na result, self-supervised pretraining has yet to show clear signs of benefits\nfrom scale on neural data. Here, we present a novel approach to self-supervised\npretraining, POYO-SSL that exploits the heterogeneity of neural data to improve\npre-training and achieve benefits of scale. Specifically, in POYO-SSL we\npretrain only on predictable (statistically regular) neurons-identified on the\npretraining split via simple higher-order statistics (skewness and\nkurtosis)-then we fine-tune on the unpredictable population for downstream\ntasks. On the Allen Brain Observatory dataset, this strategy yields\napproximately 12-13% relative gains over from-scratch training and exhibits\nsmooth, monotonic scaling with model size. In contrast, existing\nstate-of-the-art baselines plateau or destabilize as model size increases. By\nmaking predictability an explicit metric for crafting the data diet, POYO-SSL\nturns heterogeneity from a liability into an asset, providing a robust,\nbiologically grounded recipe for scalable neural decoding and a path toward\nfoundation models of neural dynamics."}
{"id": "2510.18387", "pdf": "https://arxiv.org/pdf/2510.18387", "abs": "https://arxiv.org/abs/2510.18387", "authors": ["Silv√®re S√©gaud", "Charlie Budd", "Matthew Elliot", "Graeme Stasiuk", "Jonathan Shapey", "Yijing Xie", "Tom Vercauteren"], "title": "Quantification of dual-state 5-ALA-induced PpIX fluorescence: Methodology and validation in tissue-mimicking phantoms", "categories": ["physics.med-ph", "eess.IV", "eess.SP", "q-bio.QM"], "comment": null, "summary": "Quantification of protoporphyrin IX (PpIX) fluorescence in human brain\ntumours has the potential to significantly improve patient outcomes in\nneuro-oncology, but represents a formidable imaging challenge. Protoporphyrin\nis a biological molecule which interacts with the tissue micro-environment to\nform two photochemical states in glioma. Each exhibits markedly different\nquantum efficiencies, with distinct but overlapping emission spectra that also\noverlap with tissue autofluorescence. Fluorescence emission is known to be\ndistorted by the intrinsic optical properties of tissue, coupled with marked\nintra-tumoural heterogeneity as a hallmark of glioma tumours. Existing\nquantitative fluorescence systems are developed and validated using simplified\nphantoms that do not simultaneously mimic the complex interactions between\nfluorophores and tissue optical properties or micro-environment. Consequently,\nexisting systems risk introducing systematic errors into PpIX quantification\nwhen used in tissue. In this work, we introduce a novel pipeline for\nquantification of PpIX in glioma, which robustly differentiates both emission\nstates from background autofluorescence without reliance on a priori spectral\ninformation, and accounts for variations in their quantum efficiency. Unmixed\nPpIX emission forms are then corrected for wavelength-dependent optical\ndistortions and weighted for accurate quantification. Significantly, this\npipeline is developed and validated using novel tissue-mimicking phantoms\nreplicating the optical properties of glioma tissues and photochemical\nvariability of PpIX fluorescence in glioma. Our workflow achieves strong\ncorrelation with ground-truth PpIX concentrations (R2 = 0.918+-0.002),\ndemonstrating its potential for robust, quantitative PpIX fluorescence imaging\nin clinical settings."}
{"id": "2510.17841", "pdf": "https://arxiv.org/pdf/2510.17841", "abs": "https://arxiv.org/abs/2510.17841", "authors": ["Ishir Rao"], "title": "Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information", "categories": ["cs.IT", "math.IT", "q-bio.NC"], "comment": "3 pages, 5 figures", "summary": "Electroencephalography (EEG) is widely used to study human brain dynamics,\nyet its quantitative information capacity remains unclear. Here, we combine\ninformation theory and synthetic forward modeling to estimate the mutual\ninformation between latent cortical sources and EEG recordings. Using\nGaussian-channel theory and empirical simulations, we find that scalp EEG\nconveys only tens of bits per sample about low-dimensional neural activity.\nInformation saturates with approximately 64-128 electrodes and scales\nlogarithmically with signal-to-noise ratio (SNR). Linear decoders capture\nnearly all variance that is linearly recoverable, but the mutual information\nthey recover remains far below the analytic channel capacity, indicating that\nmeasurement physics - not algorithmic complexity - is the dominant limitation.\nThese results outline the intrinsic ceiling on how much structure about brain\nstate or thought content can be inferred from EEG."}
{"id": "2510.18575", "pdf": "https://arxiv.org/pdf/2510.18575", "abs": "https://arxiv.org/abs/2510.18575", "authors": ["Yusi Fan", "Tian Wang", "Zhiying Yan", "Chang Liu", "Qiong Zhou", "Qi Lu", "Zhehao Guo", "Ziqi Deng", "Wenyu Zhu", "Ruochi Zhang", "Fengfeng Zhou"], "title": "HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Feature selection is a combinatorial optimization problem that is NP-hard.\nConventional approaches often employ heuristic or greedy strategies, which are\nprone to premature convergence and may fail to capture subtle yet informative\nfeatures. This limitation becomes especially critical in high-dimensional\ndatasets, where complex and interdependent feature relationships prevail. We\nintroduce the HeFS (Helper-Enhanced Feature Selection) framework to refine\nfeature subsets produced by existing algorithms. HeFS systematically searches\nthe residual feature space to identify a Helper Set - features that complement\nthe original subset and improve classification performance. The approach\nemploys a biased initialization scheme and a ratio-guided mutation mechanism\nwithin a genetic algorithm, coupled with Pareto-based multi-objective\noptimization to jointly maximize predictive accuracy and feature\ncomplementarity. Experiments on 18 benchmark datasets demonstrate that HeFS\nconsistently identifies overlooked yet informative features and achieves\nsuperior performance over state-of-the-art methods, including in challenging\ndomains such as gastric cancer classification, drug toxicity prediction, and\ncomputer science applications. The code and datasets are available at\nhttps://healthinformaticslab.org/supp/."}
{"id": "2510.17916", "pdf": "https://arxiv.org/pdf/2510.17916", "abs": "https://arxiv.org/abs/2510.17916", "authors": ["Michael James McCulloch"], "title": "Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy", "categories": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "30 pages, 13 Figures", "summary": "The Free Energy Principle (FEP) states that self-organizing systems must\nminimize variational free energy to persist, but the path from principle to\nimplementable algorithm has remained unclear. We present a constructive proof\nthat the FEP can be realized through exact local credit assignment. The system\ndecomposes gradient computation hierarchically: spatial credit via feedback\nalignment, temporal credit via eligibility traces, and structural credit via a\nTrophic Field Map (TFM) that estimates expected gradient magnitude for each\nconnection block. We prove these mechanisms are exact at their respective\nlevels and validate the central claim empirically: the TFM achieves 0.9693\nPearson correlation with oracle gradients. This exactness produces emergent\ncapabilities including 98.6% retention after task interference, autonomous\nrecovery from 75% structural damage, self-organized criticality (spectral\nradius p ~= 1.0$), and sample-efficient reinforcement learning on continuous\ncontrol tasks without replay buffers. The architecture unifies Prigogine's\ndissipative structures, Friston's free energy minimization, and Hopfield's\nattractor dynamics, demonstrating that exact hierarchical inference over\nnetwork topology can be implemented with local, biologically plausible rules."}
{"id": "2510.18037", "pdf": "https://arxiv.org/pdf/2510.18037", "abs": "https://arxiv.org/abs/2510.18037", "authors": ["Ziyu Lu", "Anna J. Li", "Alexander E. Ladd", "Pascha Matveev", "Aditya Deole", "Eric Shea-Brown", "J. Nathan Kutz", "Nicholas A. Steinmetz"], "title": "Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: Data on the Brain & Mind", "summary": "Neural activity forecasting is central to understanding neural systems and\nenabling closed-loop control. While deep learning has recently advanced the\nstate-of-the-art in the time series forecasting literature, its application to\nneural activity forecasting remains limited. To bridge this gap, we\nsystematically evaluated eight probabilistic deep learning models, including\ntwo foundation models, that have demonstrated strong performance on general\nforecasting benchmarks. We compared them against four classical statistical\nmodels and two baseline methods on spontaneous neural activity recorded from\nmouse cortex via widefield imaging. Across prediction horizons, several deep\nlearning models consistently outperformed classical approaches, with the best\nmodel producing informative forecasts up to 1.5 seconds into the future. Our\nfindings point toward future control applications and open new avenues for\nprobing the intrinsic temporal structure of neural activity."}
{"id": "2510.18808", "pdf": "https://arxiv.org/pdf/2510.18808", "abs": "https://arxiv.org/abs/2510.18808", "authors": ["Marc Gong Bacvanski", "Liu Ziyin", "Tomaso Poggio"], "title": "On Biologically Plausible Learning in Continuous Time", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Biological learning unfolds continuously in time, yet most algorithmic models\nrely on discrete updates and separate inference and learning phases. We study a\ncontinuous-time neural model that unifies several biologically plausible\nlearning algorithms and removes the need for phase separation. Rules including\nstochastic gradient descent (SGD), feedback alignment (FA), direct feedback\nalignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of\nthe dynamics. Simulations show that these continuous-time networks stably learn\nat biological timescales, even under temporal mismatches and integration noise.\nThrough analysis and simulation, we show that learning depends on temporal\noverlap: a synapse updates correctly only when its input and the corresponding\nerror signal coincide in time. When inputs are held constant, learning strength\ndeclines linearly as the delay between input and error approaches the stimulus\nduration, explaining observed robustness and failure across network depths.\nCritically, robust learning requires the synaptic plasticity timescale to\nexceed the stimulus duration by one to two orders of magnitude. For typical\ncortical stimuli (tens of milliseconds), this places the functional plasticity\nwindow in the few-second range, a testable prediction that identifies\nseconds-scale eligibility traces as necessary for error-driven learning in\nbiological circuits."}
