{"id": "2507.06534", "pdf": "https://arxiv.org/pdf/2507.06534", "abs": "https://arxiv.org/abs/2507.06534", "authors": ["Ruoyang Zhao", "Xiaowei Wang", "Jiajia Hu", "Qingqing Sun", "Xinmin Zhao", "Jun Guo", "Feng Zhang", "Min Wu"], "title": "Targeting Melanoma-Specific Tyrosinase: Cyclic Peptide Disrupts Actin Dynamics for Precision Apoptosis Induction", "categories": ["q-bio.BM"], "comment": null, "summary": "Melanoma is an aggressive and highly metastatic cancer that exhibits stubborn\nresistance to conventional therapies, highlighting the need for novel\ntreatments. Existing therapeutic strategies often suffer from systemic\ntoxicity, poor efficacy and fast-gained drug resistance. In this study, we\ndesigned a cyclic peptide system (c-RGDKYQ) that takes the advantage of the\noverexpression of tyrosinase in melanoma cells to trigger enzyme-mediated\noxidation and self-assembly. The assembled peptide nanostructures can\nselectively disrupt the actin cytoskeleton, impairing cancer cellular\nfunctions, e.g., motility, adhesion, and proliferation, ultimately leading to\napoptosis. This approach does not rely on external drug payloads or complex\ndelivery mechanisms. c-RGDKYQ exhibits high selectivity for melanoma cells,\nstrongly suppressing tumor growth in a murine model with minimal systemic\ntoxicity. Our findings illuminate that, through targeting tyrosinase, c-RGDKYQ\nmay be an enzyme-responsive alternative to conventional treatments for\nmelanoma."}
{"id": "2507.06366", "pdf": "https://arxiv.org/pdf/2507.06366", "abs": "https://arxiv.org/abs/2507.06366", "authors": ["Yupu Zhang", "Zelin Xu", "Tingsong Xiao", "Gustavo Seabra", "Yanjun Li", "Chenglong Li", "Zhe Jiang"], "title": "DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting the binding affinity of protein-ligand complexes plays a vital\nrole in drug discovery. Unfortunately, progress has been hindered by the lack\nof large-scale and high-quality binding affinity labels. The widely used\nPDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,\nespecially graph contrastive learning (GCL), provides a unique opportunity to\nbreak the barrier by pre-training graph neural network models based on vast\nunlabeled complexes and fine-tuning the models on much fewer labeled complexes.\nHowever, the problem faces unique challenges, including a lack of a\ncomprehensive unlabeled dataset with well-defined positive/negative complex\npairs and the need to design GCL algorithms that incorporate the unique\ncharacteristics of such data. To fill the gap, we propose DecoyDB, a\nlarge-scale, structure-aware dataset specifically designed for self-supervised\nGCL on protein-ligand complexes. DecoyDB consists of high-resolution ground\ntruth complexes (less than 2.5 Angstrom) and diverse decoy structures with\ncomputationally generated binding poses that range from realistic to suboptimal\n(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation\n(RMSD) from the native pose. We further design a customized GCL framework to\npre-train graph neural networks based on DecoyDB and fine-tune the models with\nlabels from PDBbind. Extensive experiments confirm that models pre-trained with\nDecoyDB achieve superior accuracy, label efficiency, and generalizability."}
{"id": "2507.06458", "pdf": "https://arxiv.org/pdf/2507.06458", "abs": "https://arxiv.org/abs/2507.06458", "authors": ["Arjun Banerjee", "David Martinez", "Camille Dang", "Ethan Tam"], "title": "Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models", "categories": ["cs.LG", "q-bio.BM"], "comment": "15 pages, 13 figures. Accepted to Proceedings of the Workshop on\n  Generative AI for Biology at the 42nd International Conference on Machine\n  Learning (Spotlight)", "summary": "Protein language models (PLMs) encode rich biological information, yet their\ninternal neuron representations are poorly understood. We introduce the first\nautomated framework for labeling every neuron in a PLM with biologically\ngrounded natural language descriptions. Unlike prior approaches relying on\nsparse autoencoders or manual annotation, our method scales to hundreds of\nthousands of neurons, revealing individual neurons are selectively sensitive to\ndiverse biochemical and structural properties. We then develop a novel neuron\nactivation-guided steering method to generate proteins with desired traits,\nenabling convergence to target biochemical properties like molecular weight and\ninstability index as well as secondary and tertiary structural motifs,\nincluding alpha helices and canonical Zinc Fingers. We finally show that\nanalysis of labeled neurons in different model sizes reveals PLM scaling laws\nand a structured neuron space distribution."}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses."}
{"id": "2507.06645", "pdf": "https://arxiv.org/pdf/2507.06645", "abs": "https://arxiv.org/abs/2507.06645", "authors": ["Thomas Klein", "Sascha Meyen", "Wieland Brendel", "Felix A. Wichmann", "Kristof Meding"], "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers", "categories": ["q-bio.NC"], "comment": "19 pages, 10 figures", "summary": "Benchmarking models is a key factor for the rapid progress in machine\nlearning (ML) research. Thus, further progress depends on improving\nbenchmarking metrics. A standard metric to measure the behavioral alignment\nbetween ML models and human observers is error consistency (EC). EC allows for\nmore fine-grained comparisons of behavior than other metrics such as e.g.\naccuracy, and has been used in the influential Brain-Score benchmark to rank\ndifferent DNNs by their behavioral consistency with humans. Previously, EC\nvalues have been reported without confidence intervals. However, empirically\nmeasured EC values are typically noisy -- thus, without confidence intervals,\nvalid benchmarking conclusions are problematic. Here we improve on standard EC\nin two ways: First, we show how to obtain confidence intervals for EC using a\nbootstrapping technique, allowing us to derive significance tests for EC.\nSecond, we propose a new computational model relating the EC between two\nclassifiers to the implicit probability that one of them copies responses from\nthe other. This view of EC allows us to give practical guidance to scientists\nregarding the number of trials required for sufficiently powerful, conclusive\nexperiments. Finally, we use our methodology to revisit popular\nNeuroAI-results. We find that while the general trend of behavioral differences\nbetween humans and machines holds up to scrutiny, many reported differences\nbetween deep vision models are statistically insignificant. Our methodology\nenables researchers to design adequately powered experiments that can reliably\ndetect behavioral differences between models, providing a foundation for more\nrigorous benchmarking of behavioral alignment."}
{"id": "2507.06853", "pdf": "https://arxiv.org/pdf/2507.06853", "abs": "https://arxiv.org/abs/2507.06853", "authors": ["Liang Wang", "Yu Rong", "Tingyang Xu", "Zhenyi Zhong", "Zhiyuan Liu", "Pengju Wang", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "comment": null, "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation."}
{"id": "2507.06280", "pdf": "https://arxiv.org/pdf/2507.06280", "abs": "https://arxiv.org/abs/2507.06280", "authors": ["Thibault Chassereau", "Florence Leclerc", "Eric Herbert"], "title": "Direct Evidence Of Apex Hypha Interactions During Vegetative Growth Of Fungal Thallus Via Comprehensive Network And Trajectory Extraction", "categories": ["q-bio.QM"], "comment": "5 pages, 2 figures, 1 table", "summary": "The mycelium of a filamentous fungus is a growing, branching network of\nnumerous entangled hyphae exhibiting polarised apical growth. Expansion occurs\nduring the vegetative phase from a single ascospore, driven by the need to\nexplore and occupy surrounding space limiting competitors, enhancing nutrient\nuptake, and promoting spore dispersal. Radial, rapid, and rectilinear growth\ncombined with frequent branching appears adaptive. However, passive growth\nwithout interactions or feedback may produce suboptimal networks, as neither\nlocal density nor potential connectivity is considered. Reorientations of the\napex near existing hyphae suggest apex hypha feedback. Yet, the diversity of\nbehaviours, spontaneous fluctuations, and limited apical trajectories studied\nleave open the question of active regulation. To investigate possible apex\nhypha interactions, we analyse a dataset of Podospora anserina thallus growth\nby reconstructing all apical trajectories postbranching and fitting them with a\nclassical Langevin model that incorporates potential interactions. Comparing\nisolated and nonisolated hyphae trajectories allows to identify a clear\nsignature of interaction composed of abrupt deceleration and reorientation.\nThis work opens the path towards a systematic exploration of hyphal\ninteractions."}
{"id": "2507.06358", "pdf": "https://arxiv.org/pdf/2507.06358", "abs": "https://arxiv.org/abs/2507.06358", "authors": ["Victor Boussange", "Philipp Brun", "Johanna T. Malle", "Gabriele Midolo", "Jeanne Portier", "Théophile Sanchez", "Niklaus E. Zimmermann", "Irena Axmanová", "Helge Bruelheide", "Milan Chytrý", "Stephan Kambach", "Zdeňka Lososová", "Martin Večeřa", "Idoia Biurrun", "Klaus T. Ecker", "Jonathan Lenoir", "Jens-Christian Svenning", "Dirk Nikolaus Karger"], "title": "Deep learning-based species-area models reveal multi-scale patterns of species richness and turnover", "categories": ["q-bio.PE", "cs.LG", "92-08, 92B05, 92B15, 92B20, 92D40 (Primary) 62P10, 62P12 (Secondary)"], "comment": "31 pages", "summary": "The number of species within ecosystems is influenced not only by their\nintrinsic characteristics but also by the spatial scale considered. As the\nsampled area expands, species richness increases, a phenomenon described by the\nspecies-area relationship (SAR). The accumulation dynamics of the SAR results\nfrom a complex interplay of biotic and abiotic processes operating at various\nspatial scales. However, the challenge of collecting exhaustive biodiversity\nrecords across spatial scales has hindered a comprehensive understanding of\nthese dynamics. Here, we develop a deep learning approach that leverages\nsampling theory and small-scale ecological surveys to spatially resolve the\nscale-dependency of species richness. We demonstrate its performance by\npredicting the species richness of vascular plant communities across Europe,\nand evaluate the predictions against an independent dataset of plant community\ninventories. Our model improves species richness estimates by 32\\% and delivers\nspatially explicit patterns of species richness and turnover for sampling areas\nranging from square meters to hundreds of square kilometers. Explainable AI\ntechniques further disentangle how drivers of species richness operate across\nspatial scales. The ability of our model to represent the multi-scale nature of\nbiodiversity is essential to deliver robust biodiversity assessments and\nforecasts under global change."}
{"id": "2507.06326", "pdf": "https://arxiv.org/pdf/2507.06326", "abs": "https://arxiv.org/abs/2507.06326", "authors": ["Harsh Ravivarapu", "Gaurav Bagwe", "Xiaoyong Yuan", "Chunxiu Yu", "Lan Zhang"], "title": "Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "q-bio.NC"], "comment": "Accepted by IEEE IMC 2025", "summary": "Deep brain stimulation (DBS) is an established intervention for Parkinson's\ndisease (PD), but conventional open-loop systems lack adaptability, are\nenergy-inefficient due to continuous stimulation, and provide limited\npersonalization to individual neural dynamics. Adaptive DBS (aDBS) offers a\nclosed-loop alternative, using biomarkers such as beta-band oscillations to\ndynamically modulate stimulation. While reinforcement learning (RL) holds\npromise for personalized aDBS control, existing methods suffer from high sample\ncomplexity, unstable exploration in binary action spaces, and limited\ndeployability on resource-constrained hardware.\n  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses\nthe core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a\npredictive reward model to reduce reliance on real-time feedback and employs\nGumbel Softmax-based exploration for stable, differentiable policy updates in\nbinary action spaces. Together, these components improve sample efficiency,\nexploration robustness, and compatibility with resource-constrained\nneuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic\nsimulation of Parkinsonian basal ganglia activity, demonstrating faster\nconvergence, stronger suppression of pathological beta-band power, and\nresilience to post-training FP16 quantization. Our results show that SEA-DBS\noffers a practical and effective RL-based aDBS framework for real-time,\nresource-constrained neuromodulation."}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses."}
{"id": "2507.06336", "pdf": "https://arxiv.org/pdf/2507.06336", "abs": "https://arxiv.org/abs/2507.06336", "authors": ["Adam J Riesselman", "Evan M Cofer", "Therese LaRue", "Wim Meeussen"], "title": "Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data", "categories": ["q-bio.QM", "cs.LG", "cs.RO"], "comment": null, "summary": "Quantifying organism-level phenotypes, such as growth dynamics and biomass\naccumulation, is fundamental to understanding agronomic traits and optimizing\ncrop production. However, quality growing data of plants at scale is difficult\nto generate. Here we use a mobile robotic platform to capture high-resolution\nenvironmental sensing and phenotyping measurements of a large-scale hydroponic\nleafy greens system. We describe a self-supervised modeling approach to build a\nmap from observed growing data to the entire plant growth trajectory. We\ndemonstrate our approach by forecasting future plant height and harvest mass of\ncrops in this system. This approach represents a significant advance in\ncombining robotic automation and machine learning, as well as providing\nactionable insights for agronomic research and operational efficiency."}
{"id": "2507.06964", "pdf": "https://arxiv.org/pdf/2507.06964", "abs": "https://arxiv.org/abs/2507.06964", "authors": ["Francisco Campuzano Jiménez", "Arthur Zwaenepoel", "Els Lea R De Keyzer", "Hannes Svardal"], "title": "IdentityByDescentDispersal.jl: Inferring dispersal rates with identity-by-descent blocks", "categories": ["q-bio.PE"], "comment": null, "summary": "The population density and per-generation dispersal rate of a population are\ncentral parameters in the study of evolution and ecology. The distribution of\nrecent coalescent events between individuals in space can be used to estimate\nsuch quantities through the distribution of identity-by-descent (IBD) blocks.\nAn IBD block is defined as a segment of DNA that has been inherited by a pair\nof individuals from a common ancestor without being broken by recombination.\n  We introduce $\\texttt{IdentityByDescentDispersal.jl}$, a Julia package for\nestimating effective population densities and dispersal rates from observed\nspatial patterns of IBD shared blocks. It implements the inference scheme\nproposed by Ringbauer, Coop, and Barton (2017). The package provides a\nuser-friendly interface, supports efficient gradient-based optimization and\naccommodates arbitrary user-defined demographic models through numerical\nintegration. This software aims to encourage a wider audience to utilize\nspatial genetic data for estimating dispersal rates, thereby motivating further\nresearch and expanding its applications."}
{"id": "2507.06381", "pdf": "https://arxiv.org/pdf/2507.06381", "abs": "https://arxiv.org/abs/2507.06381", "authors": ["James Hazelden", "Laura Driscoll", "Eli Shlizerman", "Eric Shea-Brown"], "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks", "categories": ["cs.LG", "cs.AI", "math.DS", "q-bio.NC"], "comment": null, "summary": "Gradient Descent (GD) and its variants are the primary tool for enabling\nefficient training of recurrent dynamical systems such as Recurrent Neural\nNetworks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics\nthat are formed in these models exhibit features such as neural collapse and\nemergence of latent representations that may support the remarkable\ngeneralization properties of networks. In neuroscience, qualitative features of\nthese representations are used to compare learning in biological and artificial\nsystems. Despite recent progress, there remains a need for theoretical tools to\nrigorously understand the mechanisms shaping learned representations,\nespecially in finite, non-linear models. Here, we show that the gradient flow,\nwhich describes how the model's dynamics evolve over GD, can be decomposed into\na product that involves two operators: a Parameter Operator, K, and a\nLinearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in\nfeed-forward neural networks, while P appears in Lyapunov stability and optimal\ncontrol theory. We demonstrate two applications of our decomposition. First, we\nshow how their interplay gives rise to low-dimensional latent dynamics under\nGD, and, specifically, how the collapse is a result of the network structure,\nover and above the nature of the underlying task. Second, for multi-task\ntraining, we show that the operators can be used to measure how objectives\nrelevant to individual sub-tasks align. We experimentally and theoretically\nvalidate these findings, providing an efficient Pytorch package, \\emph{KPFlow},\nimplementing robust analysis tools for general recurrent architectures. Taken\ntogether, our work moves towards building a next stage of understanding of GD\nlearning in non-linear recurrent models."}
{"id": "2507.06418", "pdf": "https://arxiv.org/pdf/2507.06418", "abs": "https://arxiv.org/abs/2507.06418", "authors": ["Changchun Yang", "Haoyang Li", "Yushuai Wu", "Yilan Zhang", "Yifeng Jiao", "Yu Zhang", "Rihan Huang", "Yuan Cheng", "Yuan Qi", "Xin Guo", "Xin Gao"], "title": "PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer", "categories": ["q-bio.QM", "cs.CV", "stat.AP"], "comment": null, "summary": "While pathology foundation models have transformed cancer image analysis,\nthey often lack integration with molecular data at single-cell resolution,\nlimiting their utility for precision oncology. Here, we present PAST, a\npan-cancer single-cell foundation model trained on 20 million paired\nhistopathology images and single-cell transcriptomes spanning multiple tumor\ntypes and tissue contexts. By jointly encoding cellular morphology and gene\nexpression, PAST learns unified cross-modal representations that capture both\nspatial and molecular heterogeneity at the cellular level. This approach\nenables accurate prediction of single-cell gene expression, virtual molecular\nstaining, and multimodal survival analysis directly from routine pathology\nslides. Across diverse cancers and downstream tasks, PAST consistently exceeds\nthe performance of existing approaches, demonstrating robust generalizability\nand scalability. Our work establishes a new paradigm for pathology foundation\nmodels, providing a versatile tool for high-resolution spatial omics,\nmechanistic discovery, and precision cancer research."}
{"id": "2507.06521", "pdf": "https://arxiv.org/pdf/2507.06521", "abs": "https://arxiv.org/abs/2507.06521", "authors": ["Belinda Neo", "Noel Nannup", "Dale Tilbrook", "Carol Michie", "Cindy Prior", "Eleanor Dunlop", "Brad Farrant", "Won Sun Chen", "Carrington C. J. Shepherd", "Lucinda J. Black"], "title": "Serum 25-hydroxyvitamin D concentration is not associated with mental health among Aboriginal and Torres Strait Islander Peoples in Australia: a cross-sectional exploratory study", "categories": ["q-bio.QM"], "comment": null, "summary": "Objective: To investigate the association between serum 25-hydroxyvitamin D\n[25(OH)D] concentration and mental health, measured using the Kessler\nPsychological Distress Scale 5 (K5), among Aboriginal and Torres Strait\nIslander Peoples. Methods: We used cross-sectional data from the 2012-2013\nAustralian Aboriginal and Torres Strait Islander Health Survey. Multiple linear\nregression was used to test the association between serum 25(OH)D concentration\nand K5, adjusting for age, sex, education, remoteness, socioeconomic status,\nseason of blood collection, smoking, and alcohol intake (n = 1,983). We also\nstratified the analysis by sex and by remoteness. Results: There was no\nstatistically significant association between serum 25(OH) concentration and K5\nin the total population, nor when stratified by sex. When stratified by\nremoteness, higher serum 25(OH)D concentration was statistically significantly\nassociated with lower K5 scores among those living remotely (adjusted \\b{eta}:\n-0.18; 95% CI: -0.35, -0.01). Conclusions: Serum 25(OH)D concentration was\ninversely associated with psychological distress only among those living\nremotely. Implications for Public Health: Given the prevalence of vitamin D\ndeficiency and the observed association between serum 25(OH)D concentration and\npsychological distress among Aboriginal and Torres Strait Islander Peoples\nliving remotely, public health strategies to improve vitamin D status among\nthis population group are warranted."}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses."}
{"id": "2507.06433", "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes."}
{"id": "2507.07032", "pdf": "https://arxiv.org/pdf/2507.07032", "abs": "https://arxiv.org/abs/2507.07032", "authors": ["Hanqun Cao", "Xinyi Zhou", "Zijun Gao", "Chenyu Wang", "Xin Gao", "Zhi Zhang", "Chunbin Gu", "Ge Liu", "Pheng-Ann Heng"], "title": "PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein structure prediction is essential for drug discovery and\nunderstanding biological functions. While recent advancements like AlphaFold\nhave achieved remarkable accuracy, most folding models rely heavily on multiple\nsequence alignments (MSAs) to boost prediction performance. This dependency\nlimits their effectiveness on low-homology proteins and orphan proteins, where\nMSA information is sparse or unavailable. To address this limitation, we\npropose PLAME, a novel MSA design model that leverages evolutionary embeddings\nfrom pretrained protein language models. Unlike existing methods, PLAME\nintroduces pretrained representations to enhance evolutionary information and\nemploys a conservation-diversity loss to enhance generation quality.\nAdditionally, we propose a novel MSA selection method to effectively screen\nhigh-quality MSAs and improve folding performance. We also propose a sequence\nquality assessment metric that provides an orthogonal perspective to evaluate\nMSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,\nPLAME achieves state-of-the-art performance in folding enhancement and sequence\nquality assessment, with consistent improvements demonstrated on AlphaFold3.\nAblation studies validate the effectiveness of the MSA selection method, while\nextensive case studies on various protein types provide insights into the\nrelationship between AlphaFold's prediction quality and MSA characteristics.\nFurthermore, we demonstrate that PLAME can serve as an adapter achieving\nAlphaFold2-level accuracy with the ESMFold's inference speed."}
