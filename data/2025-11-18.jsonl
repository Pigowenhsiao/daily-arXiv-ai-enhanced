{"id": "2511.11860", "pdf": "https://arxiv.org/pdf/2511.11860", "abs": "https://arxiv.org/abs/2511.11860", "authors": ["Seungha Um", "Tulika Kakati", "Lilia M Iakoucheva", "Yile Chen", "Sean Mooney"], "title": "Understanding Molecular Basis of PTPN11-Related Diseases", "categories": ["q-bio.BM"], "comment": null, "summary": "The PTPN11 gene encodes the Src homology 2 domain-containing protein tyrosine phosphatase (SHP2), a key regulator of cell growth, differentiation, and apoptosis through its modulation of various signaling pathways, including the RAS/MAPK signaling pathway. Missense variants in PTPN11 disrupt SHP2's proper catalytic activity and the regulation of signaling pathways, leading to disorders such as Noonan syndrome (NS), LEOPARD syndrome (LS), or juvenile myelomonocytic leukemia (JMML). These missense variants have molecular disruptions resulting in gains and losses of function at both the molecular and phenotypic levels. Depending on their location within SHP2, missense substitutions disrupt inter-domain regulation or impair phosphatase function, resulting in altered phosphatase activity. In this study, we investigate the molecular basis underlying the differential pathogenicity of PTPN11 missense variants and predict the structural consequences of these variants using MutPred2 and AlphaFold2. We find that LOF and GOF variants display distinct functional mechanisms in sodium and DNA binding, and that NS-associated missense variants identified in fetuses with ultrasound-detected anomalies and familiar cases are more likely to be pathogenic."}
{"id": "2511.12843", "pdf": "https://arxiv.org/pdf/2511.12843", "abs": "https://arxiv.org/abs/2511.12843", "authors": ["Zhang Junye", "Zheng Hongyu", "Cheng Jingran", "Zhang Shengli"], "title": "Treatment of phenol wastewater by electro-Fenton oxidative degradation based on efficient iron-based-gas diffusion-photocatalysis", "categories": ["q-bio.BM"], "comment": null, "summary": "This study introduces a novel iron-based gas diffusion electrode-photocatalytic system aimed at enhancing the degradation of phenolic compounds in wastewater. Phenolic compounds are toxic environmental pollutants with significant resistance to biodegradation. The traditional methods for treating phenol wastewater, including biological treatments and adsorption techniques, often fall short in achieving complete mineralization. Our approach utilizes a dual-chamber electrochemical setup integrating stainless steel felt-2-EAQ gas diffusion electrodes with TiO2 photocatalysis. This combination significantly boosts hydroxyl radical production, critical for effective pollutant breakdown. Experimentally, the system achieved up to 92% degradation efficiency for phenol at an optimized operating current of 10 mA/cm^2 in 3 hours, surpassing traditional methods. Additionally, energy consumption was reduced by 40% compared to conventional electro-Fenton systems. The stability tests indicated that the electrodes maintain over 80% of their initial activity after five cycles of use. These findings suggest that our system offers a more sustainable and efficient solution for treating phenolic wastewater by enhancing both degradation rates and energy efficiency."}
{"id": "2511.13550", "pdf": "https://arxiv.org/pdf/2511.13550", "abs": "https://arxiv.org/abs/2511.13550", "authors": ["Irene Cazzaniga", "Toni Giorgino"], "title": "MDIntrinsicDimension: Dimensionality-Based Analysis of Collective Motions in Macromolecules from Molecular Dynamics Trajectories", "categories": ["q-bio.BM", "physics.comp-ph"], "comment": null, "summary": "Molecular dynamics (MD) simulations provide atomistic insights into the structure, dynamics, and function of biomolecules by generating time-resolved, high-dimensional trajectories. Analyzing such data benefits from estimating the minimal number of variables required to describe the explored conformational manifold, known as the intrinsic dimension (ID). We present MDIntrinsicDimension, an open-source Python package that estimates ID directly from MD trajectories by combining rotation- and translation-invariant molecular projections (e.g., backbone dihedrals and inter-residue distances) with state-of-the-art estimators. The package provides three complementary analysis modes: whole-molecule ID; sliding windows along the sequence; and per-secondary-structure elements. It computes both overall ID (a single summary value) and instantaneous, time-resolved ID that can reveal transitions and heterogeneity over time. We illustrate the approach on fast folding-unfolding trajectories from the DESRES dataset, demonstrating that ID complements conventional geometric descriptors by highlighting spatially localized flexibility and differences across structural segments."}
{"id": "2511.13583", "pdf": "https://arxiv.org/pdf/2511.13583", "abs": "https://arxiv.org/abs/2511.13583", "authors": ["Azam Shirali", "Vitalii Stebliankin", "Jimeng Shi", "Prem Chapagain", "Giri Narasimhan"], "title": "Evaluating and Scoring Ebolavirus Protein-protein Docking Models Using PIsToN", "categories": ["q-bio.BM"], "comment": null, "summary": "Protein-protein docking is crucial for understanding how proteins interact. Numerous docking tools have been developed to discover possible conformations of two interacting proteins. However, the reliability and success of these docking tools rely on their scoring function. Accurate and efficient scoring functions are necessary to distinguish between native and non-native docking models to ensure the accuracy of a docking tool. Like in other fields where deep learning methods have been successfully utilized, these methods have also introduced innovative scoring functions. An outstanding tool for scoring and differentiating native-like docking models from non-native or incorrect conformations is called Protein binding Interfaces with Transformer Networks (PIsToN). PIsToN significantly outperforms state-of-the-art scoring functions. Using models of complexes obtained from binding the Ebola Virus Protein VP40 to the host cell's Sec24c protein as an example, we show how to evaluate docking models using PIsToN."}
{"id": "2511.11758", "pdf": "https://arxiv.org/pdf/2511.11758", "abs": "https://arxiv.org/abs/2511.11758", "authors": ["Michael Sun", "Weize Yuan", "Gang Liu", "Wojciech Matusik", "Marinka Zitnik"], "title": "Protein Structure Tokenization via Geometric Byte Pair Encoding", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/."}
{"id": "2511.12205", "pdf": "https://arxiv.org/pdf/2511.12205", "abs": "https://arxiv.org/abs/2511.12205", "authors": ["Akmuhammet Ashyralyyev", "Zülal Bingöl", "Begüm Filiz Öz", "Salem Malikic", "Uzi Vishkin", "S. Cenk Sahinalp", "Can Alkan"], "title": "LCPan: efficient variation graph construction using Locally Consistent Parsing", "categories": ["q-bio.GN"], "comment": null, "summary": "Efficient and consistent string processing is critical in the exponentially growing genomic data era. Locally Consistent Parsing (LCP) addresses this need by partitioning an input genome string into short, exactly matching substrings (e.g., \"cores\"), ensuring consistency across partitions. Labeling the cores of an input string consistently not only provides a compact representation of the input but also enables the reapplication of LCP to refine the cores over multiple iterations, providing a progressively longer and more informative set of substrings for downstream analyses.\n  We present the first iterative implementation of LCP with Lcptools and demonstrate its effectiveness in identifying cores with minimal collisions. Experimental results show that the number of cores at the i^th iteration is O(n/c^i) for c ~ 2.34, while the average length and the average distance between consecutive cores are O(c^i). Compared to the popular sketching techniques, LCP produces significantly fewer cores, enabling a more compact representation and faster analyses. To demonstrate the advantages of LCP in genomic string processing in terms of computation and memory efficiency, we also introduce LCPan, an efficient variation graph constructor. We show that LCPan generates variation graphs >10x faster than vg, while using >13x less memory."}
{"id": "2511.12805", "pdf": "https://arxiv.org/pdf/2511.12805", "abs": "https://arxiv.org/abs/2511.12805", "authors": ["Noriaki Sato", "Marco Scutari", "Shuichi Kawano", "Rui Yamaguchi", "Seiya Imoto"], "title": "Practical Causal Evaluation Metrics for Biological Networks", "categories": ["q-bio.MN", "cs.LG"], "comment": "15 pages, 1 figure", "summary": "Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric."}
{"id": "2511.11609", "pdf": "https://arxiv.org/pdf/2511.11609", "abs": "https://arxiv.org/abs/2511.11609", "authors": ["Gautier-Edouard Filardo", "Thibaut Heckmann"], "title": "A Stochastic Quantum Neural Network Model for Ai", "categories": ["q-bio.NC", "math.QA", "quant-ph"], "comment": null, "summary": "Artificial intelligence (AI) has drawn significant inspiration from neuroscience to develop artificial neural network (ANN) models. However, these models remain constrained by the Von Neumann architecture and struggle to capture the complexity of the biological brain. Quantum computing, with its foundational principles of superposition, entanglement, and unitary evolution, offers a promising alternative approach to modeling neural dynamics. This paper explores the possibility of a neuro-quantum model of the brain by introducing a stochastic quantum approach that incorporates random fluctuations of neuronal processing within a quantum framework. We propose a mathematical formalization of stochastic quantum neural networks (QNNS), where qubits evolve according to stochastic differential equations inspired by biological neuronal processes. We also discuss challenges related to decoherence, qubit stability, and implications for AI and computational neuroscience."}
{"id": "2511.12183", "pdf": "https://arxiv.org/pdf/2511.12183", "abs": "https://arxiv.org/abs/2511.12183", "authors": ["John Herrick"], "title": "DNA Replication Timing, Genome Stability and Non-adaptive Radiation", "categories": ["q-bio.PE"], "comment": null, "summary": "A correlation between karyotype diversity and species richness was first observed in mammals in 1980, and subsequently confirmed after controlling for phylogenetic signal. The correlation was attributed to submicroscopic factors, presumably operating at the level of the genome. At the same time, an unexpected association between mutation rates and substitution rates has been observed in all eukaryotes so far examined. One hypothesis to explain the latter observation proposed that neutral mutation (dS) and non-neutral (dN) substitution rates in gene codons co-vary according to genomic position, or location in the genome. Later, it was found that mutation and substitution rates in eukaryotes increase with DNA replication timing during the Synthetic phase, or S phase, of the cell cycle. In 1991, Motoo Kimura proposed a molecular theory of non-adaptive radiation (NAR). Accordingly, genetic drift plays a significant role in speciation, albeit in parallel to and in conjunction with natural selection. The following will examine the contribution of DNA replication timing and DNA repair factors to the relationship between species richness and karyotype diversity, and by extension, to the large variation in species richness across the mammalian Tree of Life."}
{"id": "2511.12931", "pdf": "https://arxiv.org/pdf/2511.12931", "abs": "https://arxiv.org/abs/2511.12931", "authors": ["Zain Shabeeb", "Daniel Saeedi", "Darin Tsui", "Vida Jamali", "Amirali Aghazadeh"], "title": "cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold", "categories": ["eess.IV", "q-bio.BM"], "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) enables the atomic-resolution visualization of biomolecules; however, modern direct detectors generate data volumes that far exceed the available storage and transfer bandwidth, thereby constraining practical throughput. We introduce cryoSENSE, the computational realization of a hardware-software co-designed framework for compressive cryo-EM sensing and acquisition. We show that cryo-EM images of proteins lie on low-dimensional manifolds that can be independently represented using sparse priors in predefined bases and generative priors captured by a denoising diffusion model. cryoSENSE leverages these low-dimensional manifolds to enable faithful image reconstruction from spatial and Fourier-domain undersampled measurements while preserving downstream structural resolution. In experiments, cryoSENSE increases acquisition throughput by up to 2.5$\\times$ while retaining the original 3D resolution, offering controllable trade-offs between the number of masked measurements and the level of downsampling. Sparse priors favor faithful reconstruction from Fourier-domain measurements and moderate compression, whereas generative diffusion priors achieve accurate recovery from pixel-domain measurements and more severe undersampling. Project website: https://cryosense.github.io."}
{"id": "2511.11913", "pdf": "https://arxiv.org/pdf/2511.11913", "abs": "https://arxiv.org/abs/2511.11913", "authors": ["Saeed Mohammadzadeh", "Yao-Chang Tsan", "Aaron Renberg", "Hiba Kobeissi", "Adam Helms", "Emma Lejeune"], "title": "SarcGraph for High-Throughput Regional Analysis of Sarcomere Organization and Contractile Function in 2D Cardiac Muscle Bundles", "categories": ["q-bio.QM"], "comment": "10 pages, 1 figure", "summary": "Timelapse images of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) provide rich information on cell structure and contractile function. However, it is challenging to reproducibly generate tissue samples and conduct scalable experiments with these cells. The two-dimensional cardiac muscle bundle (2DMB) platform helps address these limitations by standardizing tissue geometry, resulting in physiologic, uniaxial contractions of discrete tissues on an elastomeric substrate with stiffness similar to the heart. 2DMBs are highly conducive to sarcomere imaging using fluorescent reporters, but, due to their larger and more physiologic sarcomere displacements and velocities, prior sarcomere-tracking pipelines have been unreliable. Here, we present adaptations to SarcGraph, an open-source Python package for sarcomere detection and tracking, that enable automated analysis of high-frame-rate 2DMB recordings. Key modifications to the pipeline include: 1) switching to a frame-by-frame sarcomere detection approach and automating tissue segmentation with spatial partitioning, 2) performing Gaussian Process Regression for signal denoising, and 3) incorporating an automatic contractile phase detection pipeline. These enhancements enable the extraction of structural organization and functional contractility metrics for both the whole 2DMB tissue and distinct tissue regions, both in a fully automated manner. We complement this software release with a dataset of 130 example movies of baseline and drug-treated samples disseminated through the Harvard Dataverse. By providing open-source tools and datasets, we aim to enable high-throughput analysis of engineered cardiac tissues and advance collective progress within the hiPSC-CM research community."}
{"id": "2511.11717", "pdf": "https://arxiv.org/pdf/2511.11717", "abs": "https://arxiv.org/abs/2511.11717", "authors": ["Xiang Xiang Wang", "Sean Cottrell", "Guo-Wei Wei"], "title": "Multiscale Grassmann Manifolds for Single-Cell Data Analysis", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data."}
{"id": "2511.12715", "pdf": "https://arxiv.org/pdf/2511.12715", "abs": "https://arxiv.org/abs/2511.12715", "authors": ["Sushrut Thorat", "Adrien Doerig", "Alexander Kroner", "Carmen Amme", "Tim C. Kietzmann"], "title": "Predicting upcoming visual features during eye movements yields scene representations aligned with human visual cortex", "categories": ["q-bio.NC", "cs.CV"], "comment": "28 pages, 12 figures", "summary": "Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience."}
{"id": "2511.12223", "pdf": "https://arxiv.org/pdf/2511.12223", "abs": "https://arxiv.org/abs/2511.12223", "authors": ["Anshul Bagaria"], "title": "AMR-MoEGA: Antimicrobial Resistance Prediction using Mixture of Experts and Genetic Algorithms", "categories": ["q-bio.PE"], "comment": "21 pages, 24 figures", "summary": "Antimicrobial resistance (AMR) poses a mounting global health crisis, requiring rapid and reliable prediction frameworks that capture its complex evolutionary dynamics. Traditional antimicrobial susceptibility testing (AST), while accurate, remains laborious and time-consuming, limiting its clinical scalability. Existing computational approaches, primarily reliant on single nucleotide polymorphism (SNP)-based analysis, fail to account for evolutionary drivers such as horizontal gene transfer (HGT) and genome-level interactions.\n  This study introduces a novel Evolutionary Mixture of Experts (Evo-MoE) framework that integrates genomic sequence analysis, machine learning, and evolutionary algorithms to model and predict AMR evolution. A Mixture of Experts model, trained on labeled genomic data for multiple antibiotics, serves as the predictive core, estimating the likelihood of resistance for each genome. This model is embedded as a fitness function within a Genetic Algorithm designed to simulate AMR development across generations. Each genome, encoded as an individual in the population, undergoes mutation, crossover, and selection guided by predicted resistance probabilities.\n  The resulting evolutionary trajectories reveal dynamic pathways of resistance acquisition, offering mechanistic insights into genomic evolution under selective antibiotic pressure. Sensitivity analysis of mutation rates and selection pressures demonstrates the model's robustness and biological plausibility. Validation against curated AMR databases and literature evidence further substantiates the framework's predictive fidelity.\n  This integrative approach bridges genomic prediction and evolutionary simulation, offering a powerful tool for understanding and anticipating AMR dynamics, and potentially guiding rational antibiotic design and policy interventions."}
{"id": "2511.12463", "pdf": "https://arxiv.org/pdf/2511.12463", "abs": "https://arxiv.org/abs/2511.12463", "authors": ["Adham M. Alkhadrawi", "Kyungsu Kim", "Arif M. Rahman"], "title": "Explainable deep learning framework for cancer therapeutic target prioritization leveraging PPI centrality and node embeddings", "categories": ["q-bio.QM"], "comment": null, "summary": "We developed an explainable deep learning framework integrating protein-protein interaction (PPI) network centrality metrics with node embeddings for cancer therapeutic target prioritization. A high-confidence PPI network was constructed from STRING database interactions, computing six centrality metrics: degree, strength, betweenness, closeness, eigenvector centrality, and clustering coefficient. Node2Vec embeddings captured latent network topology. Combined features trained XGBoost and neural network classifiers using DepMap CRISPR essentiality scores as ground truth. Model interpretability was assessed through GradientSHAP analysis quantifying feature contributions. We developed a novel blended scoring approach combining model probability predictions with SHAP attribution magnitudes for enhanced gene prioritization. Our framework achieved state-of-the-art performance with AUROC of 0.930 and AUPRC of 0.656 for identifying the top 10\\% most essential genes. GradientSHAP analysis revealed centrality measures contributed significantly to predictions, with degree centrality showing strongest correlation ($ρ$ = -0.357) with gene essentiality. The blended scoring approach created robust gene prioritization rankings, successfully identifying known essential genes including ribosomal proteins (RPS27A, RPS17, RPS6) and oncogenes (MYC). This study presents a human-based, combinatorial \\textit{in silico} framework successfully integrating network biology with explainable AI for therapeutic target discovery. The framework provides mechanistic transparency through feature attribution analysis while maintaining state-of-the-art predictive performance. Its reproducible design and reliance on human molecular datasets demonstrate a reduction-to-practice example of next-generation, animal-free modeling for cancer therapeutic target discovery and prioritization."}
{"id": "2511.12797", "pdf": "https://arxiv.org/pdf/2511.12797", "abs": "https://arxiv.org/abs/2511.12797", "authors": ["Nathan Breslow", "Aayush Mishra", "Mahler Revsine", "Michael C. Schatz", "Anqi Liu", "Daniel Khashabi"], "title": "Genomic Next-Token Predictors are In-Context Learners", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning."}
{"id": "2511.13668", "pdf": "https://arxiv.org/pdf/2511.13668", "abs": "https://arxiv.org/abs/2511.13668", "authors": ["Pranjal Balar", "Sundeep Kapila"], "title": "Integrative Model for Interoception and Exteroception: predictive coding, points of modulation, and testable predictions", "categories": ["q-bio.NC"], "comment": null, "summary": "Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks."}
{"id": "2511.12536", "pdf": "https://arxiv.org/pdf/2511.12536", "abs": "https://arxiv.org/abs/2511.12536", "authors": ["Niket Thakkar"], "title": "A mark and recapture perspective on vaccination touchpoints", "categories": ["q-bio.PE"], "comment": null, "summary": "This paper considers large-scale vaccination campaigns, a major platform for vaccine access in a lot of the world, as a recapture estimate of the target population marked by routine immunization. Framing the campaign as a measurement, we learn about its properties, including the campaign's coverage of the target population and some implied sampling properties of post-campaign coverage surveys (PCCSs), the current gold-standard in implementation quality measurement.\n  We develop this idea in the context of the 2023 measles campaign in Kano State, Nigeria, where we have detailed implementation data collected by vaccination teams involved in that effort. Looking specifically at the teams' tally sheets, the daily records of who they vaccinated, we find significant discrepancies between the recapture estimates and those from the corresponding PCCS. Exploring a variety of bias models applied to both the tally sheets and the PCCS helps clarify how anecdotal issues from the field relate to this discrepancy.\n  Overall, we find that the tally sheets, despite being an unorthodox population sample, provide a tractable perspective on implementation and measurement, one that's in principle available nearly instantly and at high resolution for any vaccination touchpoint."}
{"id": "2511.12990", "pdf": "https://arxiv.org/pdf/2511.12990", "abs": "https://arxiv.org/abs/2511.12990", "authors": ["Sixtus Dakurah"], "title": "Brain Networks Flow-Topology via Variance Minimization in the Wasserstein Space", "categories": ["q-bio.QM"], "comment": null, "summary": "This work introduces a novel framework for testing topological variability in weighted networks by combining Hodge decomposition with Wasserstein variance minimization. Traditional approaches that analyze raw edge weights are susceptible to noise driven perturbations, limiting their ability to detect meaningful structural differences between network populations. Network signals are decomposed into various components using combinatorial Hodge theory, then topological disparity is quantified via the 2-Wasserstein distance between persistence diagrams. The test statistic measures variance reduction when comparing within group to between group dispersions in the Wasserstein space. Simulations demonstrate that the proposed method suppresses small random perturbations while maintaining sensitivity to genuine topological differences, particularly when applied to Hodge decomposed flows rather than raw edge weights. The framework is applied to functional brain networks from the Multimodal Treatment of ADHD dataset, comparing cannabis users and non-users"}
{"id": "2511.13705", "pdf": "https://arxiv.org/pdf/2511.13705", "abs": "https://arxiv.org/abs/2511.13705", "authors": ["Alaa Mezghiche"], "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering", "categories": ["cs.LG", "q-bio.GN"], "comment": "16 pages", "summary": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype."}
{"id": "2511.13501", "pdf": "https://arxiv.org/pdf/2511.13501", "abs": "https://arxiv.org/abs/2511.13501", "authors": ["Solmaz Golmohammadi", "Mina Zarei", "Jacopo Grilli"], "title": "Modeling Spatial Synchronization of Predator-Prey Oscillations via the XY Model under Demographic Stochasticity and Migration", "categories": ["q-bio.PE"], "comment": null, "summary": "We investigate stochastic predator-prey dynamics and their spatial phase synchronization using the Rosenzweig-MacArthur model coupled across multiple patches. Combining stochastic simulations based on the Gillespie algorithm with analytical methods inspired by the XY model, we uncover fundamental mechanisms through which demographic noise and dispersal shape synchronization and phase transitions. This study offers a theoretical foundation for understanding and managing large-scale ecological synchrony and ecosystem resilience."}
{"id": "2511.13141", "pdf": "https://arxiv.org/pdf/2511.13141", "abs": "https://arxiv.org/abs/2511.13141", "authors": ["Yangfan Liu", "Xiong Xiong", "Yong Liao", "Mingli Qin", "Zhen Huang", "Shilin Zhu", "Lilin Yin", "Yuhua Fu", "Haohao Zhang", "Jingya Xu", "Dong Yin", "Xin Huang", "Yuan Quan", "Xuan Li", "Tengfei Jiang", "Wanneng Yang", "Xiaohui Yuan", "Laurent Frantz", "Xinyun Li", "Xiaolei Liu", "Shuhong Zhao"], "title": "Bridging the genotype-phenotype gap with generative artificial intelligence", "categories": ["q-bio.QM"], "comment": null, "summary": "The genotype-phenotype gap is a persistent barrier to complex trait genetic dissection, worsened by the explosive growth of genomic data (1.5 billion variants identified in the UK Biobank WGS study) alongside persistently scarce and subjective human-defined phenotypes. Digital phenotyping offers a potential solution, yet existing tools fail to balance scalable non-manual phenotype generation and biological interpretability of these quantitative traits. Here we report AIPheno, the first generative AI-driven \"phenotype sequencer\" that bridges this gap. It enables high-throughput, unsupervised extraction of digital phenotypes from imaging data and unlocks their biological meaning via generative network analysis. AIPheno transforms imaging modalities into a rich source of quantitative traits, dramatically enhancing cross-species genetic discovery, including novel loci such as CCBE1 (humans), KITLG-TMTC3 (domestic pigeons), and SOD2-IGF2R (swine). Critically, its generative module decodes AI-derived phenotypes by synthesizing variant-specific images to yield actionable biological insights. For example, it clarifies how the OCA2-HERC2 locus pleiotropically links pigmentation to retinal vascular traits via vascular visibility modulation. Integrating scalable non-manual phenotyping, enhanced genetic discovery power, and generative mechanistic decoding, AIPheno establishes a transformative closed-loop paradigm. This work addresses the longstanding genotype-phenotype imbalance, redefines digital phenotype utility, and accelerates translation of genetic associations into actionable understanding with profound implications for human health and agriculture."}
{"id": "2511.12975", "pdf": "https://arxiv.org/pdf/2511.12975", "abs": "https://arxiv.org/abs/2511.12975", "authors": ["Alexandru Hening", "Nguyen T. Hieu", "Dang H. Nguyen", "Nhu Nguyen"], "title": "Dynamics of stochastic microorganism flocculation models", "categories": ["math.PR", "q-bio.PE"], "comment": "22 pages", "summary": "In this paper we study the dynamics of stochastic microorganism flocculation models. Given the strong influence of environmental and seasonal fluctuations that are present in these models, we propose a stochastic model that includes multiple layers of stochasticity, from small Brownian fluctuations, to possibly large changes due to environmental `shifts'. We are able to give a full classification of the asymptotic behavior of these models. New techniques had to be developed to prove the persistence and extinction of the process as the system is not in Kolmogorov form and, as a result, the analysis is significantly more involved."}
{"id": "2511.13295", "pdf": "https://arxiv.org/pdf/2511.13295", "abs": "https://arxiv.org/abs/2511.13295", "authors": ["Chaowang Lan", "Jingxin Wu", "Yulong Yuan", "Chuxun Liu", "Huangyi Kang", "Caihua Liu"], "title": "Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines."}
{"id": "2511.11769", "pdf": "https://arxiv.org/pdf/2511.11769", "abs": "https://arxiv.org/abs/2511.11769", "authors": ["Xiangru Wang", "Zekun Jiang", "Heng Yang", "Cheng Tan", "Xingying Lan", "Chunming Xu", "Tianhang Zhou"], "title": "Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules", "categories": ["physics.chem-ph", "cs.LG", "q-bio.QM", "stat.ME"], "comment": null, "summary": "Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms."}
{"id": "2511.12459", "pdf": "https://arxiv.org/pdf/2511.12459", "abs": "https://arxiv.org/abs/2511.12459", "authors": ["Marco Pollanen"], "title": "The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias", "categories": ["stat.ME", "cs.CY", "math.PR", "q-bio.QM", "stat.AP"], "comment": "24 pages, 1 figure", "summary": "For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.\n  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \\(10^{-35}\\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \\emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.\n  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement."}
{"id": "2511.13124", "pdf": "https://arxiv.org/pdf/2511.13124", "abs": "https://arxiv.org/abs/2511.13124", "authors": ["Changxi Chi", "Yufei Huang", "Jun Xia", "Jiangbin Zheng", "Yunfan Liu", "Zelin Zang", "Stan Z. Li"], "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance."}
{"id": "2511.13611", "pdf": "https://arxiv.org/pdf/2511.13611", "abs": "https://arxiv.org/abs/2511.13611", "authors": ["Torec T. Luik", "Joost de Folter", "Rodrigo Rosas-Bertolini", "Eric A. J. Reits", "Ron A. Hoebe", "Przemek M. Krawczyk"], "title": "BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance", "categories": ["cs.SE", "q-bio.QM"], "comment": "16 pages, 2 figures, 25 pages supplemental information; for software, see https://github.com/Cellular-Imaging-Amsterdam-UMC/NL-BIOMERO", "summary": "We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing."}
