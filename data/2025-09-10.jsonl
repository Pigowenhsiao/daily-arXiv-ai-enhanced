{"id": "2509.07458", "pdf": "https://arxiv.org/pdf/2509.07458", "abs": "https://arxiv.org/abs/2509.07458", "authors": ["Yuhan Li", "Hongyu Liu", "Catharine W. K. Lo"], "title": "Unveiling Biological Models Through Turing Patterns", "categories": ["math.AP", "physics.bio-ph", "q-bio.BM", "q-bio.CB", "35R30, 35B10, 35B36, 35K10, 35K55, 35K57, 35Q92, 92-10, 92C15,\n  92C37, 92C70, 92D25"], "comment": "22 pages keywords: inverse reaction-diffusion equations, Turing\n  patterns, Turing instability, periodic solutions, sinusoidal form", "summary": "Turing patterns play a fundamental role in morphogenesis and population\ndynamics, encoding key information about the underlying biological mechanisms.\nYet, traditional inverse problems have largely relied on non-biological data\nsuch as boundary measurements, neglecting the rich information embedded in the\npatterns themselves. Here we introduce a new research direction that directly\nleverages physical observables from nature--the amplitude of Turing\npatterns--to achieve complete parameter identification. We present a framework\nthat uses the spatial amplitude profile of a single pattern to simultaneously\nrecover all system parameters, including wavelength, diffusion constants, and\nthe full nonlinear forms of chemotactic and kinetic coefficient functions.\nDemonstrated on models of chemotactic bacteria, this amplitude-based approach\nestablishes a biologically grounded, mathematically rigorous paradigm for\nreverse-engineering pattern formation mechanisms across diverse biological\nsystems."}
{"id": "2509.07627", "pdf": "https://arxiv.org/pdf/2509.07627", "abs": "https://arxiv.org/abs/2509.07627", "authors": ["Ruihao Zhang", "Xiao Liu"], "title": "LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design", "categories": ["cs.CE", "q-bio.BM"], "comment": "13 pages, 6 figures", "summary": "Designing full-length, epitope-specific TCR {\\alpha}\\b{eta} remains\nchallenging due to vast sequence space, data biases and incomplete modeling of\nimmunogenetic constraints. We present LSMTCR, a scalable multi-architecture\nframework that separates specificity from constraint learning to enable de\nnovo, epitope-conditioned generation of paired, full-length TCRs. A\ndiffusion-enhanced BERT encoder learns time-conditioned epitope\nrepresentations; conditional GPT decoders, pretrained on CDR3\\b{eta} and\ntransferred to CDR3{\\alpha}, generate chain-specific CDR3s under cross-modal\nconditioning with temperature-controlled diversity; and a gene-aware\nTransformer assembles complete {\\alpha}/\\b{eta} sequences by predicting V/J\nusage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our\ncurated dataset, LSMTCR achieves higher predicted binding than baselines on\nmost datasets, more faithfully recovers positional and length grammars, and\ndelivers superior, temperature-tunable diversity. For {\\alpha}-chain\ngeneration, transfer learning improves predicted binding, length realism and\ndiversity over representative methods. Full-length assembly from known or de\nnovo CDR3s preserves k-mer spectra, yields low edit distances to references,\nand, in paired {\\alpha}/\\b{eta} co-modelling with epitope, attains higher\npTM/ipTM than single-chain settings. LSMTCR outputs diverse,\ngene-contextualized, full-length TCR designs from epitope input alone, enabling\nhigh-throughput screening and iterative optimization."}
{"id": "2509.07357", "pdf": "https://arxiv.org/pdf/2509.07357", "abs": "https://arxiv.org/abs/2509.07357", "authors": ["Heng Li", "Brian Li"], "title": "Finding low-complexity DNA sequences with longdust", "categories": ["q-bio.GN"], "comment": "4 pages, 1 figure", "summary": "Motivation: Low-complexity (LC) DNA sequences are compositionally repetitive\nsequences that are often associated with increased variant density and variant\ncalling artifacts. While algorithms for identifying LC sequences exist, they\neither lack rigorous mathematical foundation or are inefficient with long\ncontext windows.\n  Results: Longdust is a new algorithm that efficiently identifies long LC\nsequences including centromeric satellite and tandem repeats with moderately\nlong motifs. It defines string complexity by statistically modeling the k-mer\ncount distribution with the parameters: the k-mer length, the context window\nsize and a threshold on complexity. Longdust exhibits high performance on real\ndata and high consistency with existing methods.\n  Availability and implementation: https://github.com/lh3/longdust"}
{"id": "2509.07204", "pdf": "https://arxiv.org/pdf/2509.07204", "abs": "https://arxiv.org/abs/2509.07204", "authors": ["Adrien Couetoux", "Thomas Devenyns", "Lise Diagne", "David Champagne", "Pierre-Yves Mousset", "Chris Anagnostopoulos"], "title": "Predicting effect of novel treatments using molecular pathways and real-world data", "categories": ["cs.LG", "q-bio.QM", "I.2.6; I.2.4; J.3"], "comment": null, "summary": "In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in\ntreating a particular disease prior to clinical testing or any real-world use\nhas been challenging. In this paper, we propose a flexible and modular machine\nlearning-based approach for predicting the efficacy of an untested\npharmaceutical for treating a disease. We train a machine learning model using\nsets of pharmaceutical-pathway weight impact scores and patient data, which can\ninclude patient characteristics and observed clinical outcomes. The resulting\nmodel then analyses weighted impact scores of an untested pharmaceutical across\nhuman biological molecule-protein pathways to generate a predicted efficacy\nvalue. We demonstrate how the method works on a real-world dataset with patient\ntreatments and outcomes, with two different weight impact score algorithms We\ninclude methods for evaluating the generalisation performance on unseen\ntreatments, and to characterise conditions under which the approach can be\nexpected to be most predictive. We discuss specific ways in which our approach\ncan be iterated on, making it an initial framework to support future work on\npredicting the effect of untested drugs, leveraging RWD clinical data and drug\nembeddings."}
{"id": "2509.07712", "pdf": "https://arxiv.org/pdf/2509.07712", "abs": "https://arxiv.org/abs/2509.07712", "authors": ["Wajid Ali", "Javad Mohamadichamgavi", "Juan Ramirez", "Jose Aguilar", "Antonio FernÃ¡ndez Anta", "Kieran J. Sharkey"], "title": "A mathematical model of vaccine hesitancy: Analysing the impact of political trends and the interaction across age and education groups in the USA", "categories": ["q-bio.PE"], "comment": null, "summary": "Vaccination against the SARS-CoV-2 disease has significantly reduced its\nmortality rate and spread. However, despite its availability, a considerable\nproportion of the public has either refused or delayed getting vaccinated. This\nreluctance is known as vaccine hesitancy. The aim of this paper is to present a\nmathematical model to investigate how social interaction can impact vaccine\nhesitancy. The model describes the temporal transitions between different\nvaccination classes of the population (those vaccinated, those who are not yet\nvaccinated but agree to be vaccinated, and those who refuse). We apply the\nmodel to state and national survey data from the USA to estimate model\nparameters that quantify the rates at which public opinion on vaccination\nchanges. Moreover, we investigate how political trends and demographic factors,\nsuch as age and education, impact these parameters. Our results show that\nstate-level political affiliation, age, and educational level shape opinions on\nvaccination and have a strong influence on the temporal dynamics of attitude\nchanges."}
{"id": "2509.06969", "pdf": "https://arxiv.org/pdf/2509.06969", "abs": "https://arxiv.org/abs/2509.06969", "authors": ["Wasif Khan", "Lin Gu", "Noah Hammarlund", "Lei Xing", "Joshua K. Wong", "Ruogu Fang"], "title": "Association of Timing and Duration of Moderate-to-Vigorous Physical Activity with Cognitive Function and Brain Aging: A Population-Based Study Using the UK Biobank", "categories": ["q-bio.NC", "cs.AI"], "comment": "This article is currently under review. The Supplementary Tables\n  A1-A7 could not be attached with the current submission but it can be\n  requested from the corresponding author", "summary": "Physical activity is a modifiable lifestyle factor with potential to support\ncognitive resilience. However, the association of moderate-to-vigorous physical\nactivity (MVPA) intensity, and timing, with cognitive function and\nregion-specific brain structure remain poorly understood. We analyzed data from\n45,892 UK Biobank participants aged 60 years and older with valid wrist-worn\naccelerometer data, cognitive testing, and structural brain MRI. MVPA was\nmeasured both continuously (mins per week) and categorically (thresholded using\n>=150 min/week based on WHO guidelines). Associations with cognitive\nperformance and regional brain volumes were evaluated using multivariable\nlinear models adjusted for demographic, socioeconomic, and health-related\ncovariates. We conducted secondary analyses on MVPA timing and subgroup\neffects. Higher MVPA was associated with better performance across cognitive\ndomains, including reasoning, memory, executive function, and processing speed.\nThese associations persisted in fully adjusted models and were higher among\nparticipants meeting WHO guidelines. Greater MVPA was also associated with\nsubcortical brain regions (caudate, putamen, pallidum, thalamus), as well as\nregional gray matter volumes involved in emotion, working memory, and\nperceptual processing. Secondary analyses showed that MVPA at any time of day\nwas associated with cognitive functions and brain volume particularly in the\nmidday-afternoon and evening. Sensitivity analysis shows consistent findings\nacross subgroups, with evidence of dose-response relationships. Higher MVPA is\nassociated with preserved brain structure and enhanced cognitive function in\nlater life. Public health strategies to increase MVPA may support healthy\ncognitive aging and generate substantial economic benefits, with global gains\nprojected to reach USD 760 billion annually by 2050."}
{"id": "2509.07458", "pdf": "https://arxiv.org/pdf/2509.07458", "abs": "https://arxiv.org/abs/2509.07458", "authors": ["Yuhan Li", "Hongyu Liu", "Catharine W. K. Lo"], "title": "Unveiling Biological Models Through Turing Patterns", "categories": ["math.AP", "physics.bio-ph", "q-bio.BM", "q-bio.CB", "35R30, 35B10, 35B36, 35K10, 35K55, 35K57, 35Q92, 92-10, 92C15,\n  92C37, 92C70, 92D25"], "comment": "22 pages keywords: inverse reaction-diffusion equations, Turing\n  patterns, Turing instability, periodic solutions, sinusoidal form", "summary": "Turing patterns play a fundamental role in morphogenesis and population\ndynamics, encoding key information about the underlying biological mechanisms.\nYet, traditional inverse problems have largely relied on non-biological data\nsuch as boundary measurements, neglecting the rich information embedded in the\npatterns themselves. Here we introduce a new research direction that directly\nleverages physical observables from nature--the amplitude of Turing\npatterns--to achieve complete parameter identification. We present a framework\nthat uses the spatial amplitude profile of a single pattern to simultaneously\nrecover all system parameters, including wavelength, diffusion constants, and\nthe full nonlinear forms of chemotactic and kinetic coefficient functions.\nDemonstrated on models of chemotactic bacteria, this amplitude-based approach\nestablishes a biologically grounded, mathematically rigorous paradigm for\nreverse-engineering pattern formation mechanisms across diverse biological\nsystems."}
{"id": "2509.07723", "pdf": "https://arxiv.org/pdf/2509.07723", "abs": "https://arxiv.org/abs/2509.07723", "authors": ["Bo Yu", "Zhixiu Hua", "Bo Zhao"], "title": "BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "comment": "11 pages, 7 figures", "summary": "Background: Parkinson's disease remains a major neurodegenerative disorder\nwith high misdiagnosis rates, primarily due to reliance on clinical rating\nscales. Recent studies have demonstrated a strong association between gut\nmicrobiota and Parkinson's disease, suggesting that microbial composition may\nserve as a promising biomarker. Although deep learning models based ongut\nmicrobiota show potential for early prediction, most approaches rely on single\nclassifiers and often overlook inter-strain correlations or temporal dynamics.\nTherefore, there is an urgent need for more robust feature extraction methods\ntailored to microbiome data. Methods: We proposed BDPM (A Machine\nLearning-Based Feature Extractor for Parkinson's Disease Classification via Gut\nMicrobiota Analysis). First, we collected gut microbiota profiles from 39\nParkinson's patients and their healthy spouses to identify differentially\nabundant taxa. Second, we developed an innovative feature selection framework\nnamed RFRE (Random Forest combined with Recursive Feature Elimination),\nintegrating ecological knowledge to enhance biological interpretability.\nFinally, we designed a hybrid classification model to capture temporal and\nspatial patterns in microbiome data."}
{"id": "2509.07013", "pdf": "https://arxiv.org/pdf/2509.07013", "abs": "https://arxiv.org/abs/2509.07013", "authors": ["Sima Najafzadehkhoei", "George Vega Yon", "Bernardo Modenesi", "Derek S. Meyer"], "title": "Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs", "categories": ["cs.LG", "q-bio.PE", "stat.ME"], "comment": null, "summary": "Calibrating agent-based epidemic models is computationally demanding. We\npresent a supervised machine learning calibrator that learns the inverse\nmapping from epidemic time series to SIR parameters. A three-layer\nbidirectional LSTM ingests 60-day incidence together with population size and\nrecovery rate, and outputs transmission probability, contact rate, and R0.\nTraining uses a composite loss with an epidemiology-motivated consistency\npenalty that encourages R0 \\* recovery rate to equal transmission probability\n\\* contact rate.\n  In a 1000-scenario simulation study, we compare the calibrator with\nApproximate Bayesian Computation (likelihood-free MCMC). The method achieves\nlower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs\n0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near\nnominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per\ncalibration. Although contact rate and transmission probability are partially\nnonidentifiable, the approach reproduces epidemic curves more faithfully than\nABC, enabling fast and practical calibration. We evaluate it on SIR agent based\nepidemics generated with epiworldR and provide an implementation in R."}
{"id": "2509.06970", "pdf": "https://arxiv.org/pdf/2509.06970", "abs": "https://arxiv.org/abs/2509.06970", "authors": ["Zofia Rudnicka", "Janusz Szczepanski", "Agnieszka Pregowska"], "title": "Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "This study explores how the selection of neuron models and learning rules\nimpacts the classification performance of Spiking Neural Networks (SNNs), with\na focus on applications in bio-signal processing. We compare biologically\ninspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons,\nand probabilistic Levy-Baxter (LB) neurons, across multiple learning rules,\nincluding spike-timing-dependent plasticity (STDP), tempotron, and\nreward-modulated updates. A novel element of this work is the integration of a\ncomplexity-based decision mechanism into the evaluation pipeline. Using\nLempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the\nstructural regularity of spike trains and assess classification outcomes in a\nconsistent and interpretable manner across different SNN configurations. To\ninvestigate neural dynamics and assess algorithm performance, we employed\nsynthetic datasets with varying temporal dependencies and stochasticity levels.\nThese included Markov and Poisson processes, well-established models to\nsimulate neuronal spike trains and capture the stochastic firing behavior of\nbiological neurons.Validation of synthetic Poisson and Markov-modeled data\nreveals clear performance trends: classification accuracy depends on the\ninteraction between neuron model, network size, and learning rule, with the\nLZC-based evaluation highlighting configurations that remain robust to weak or\nnoisy signals. This work delivers a systematic analysis of how neuron model\nselection interacts with network parameters and learning strategies, supported\nby a novel complexity-based evaluation approach that offers a consistent\nbenchmark for SNN performance."}
{"id": "2509.07001", "pdf": "https://arxiv.org/pdf/2509.07001", "abs": "https://arxiv.org/abs/2509.07001", "authors": ["Shahar Dror", "Dafna Bergerbest", "Moti Salti"], "title": "Artificial Intelligence as an Opportunity for the Science of Consciousness: A Dual-Resolution Framework", "categories": ["q-bio.NC"], "comment": null, "summary": "The encounter of artificial intelligence with consciousness research is often\nframed as a challenge: could this science determine whether such systems are\nconscious? We suggest it is equally an opportunity to expand and test the scope\nof existing theories of consciousness. Current approaches remain polarized.\nComputational functionalism emphasizes abstract organization, often realized\nthrough neural correlates of consciousness, while biological naturalism insists\nthat consciousness is tied to living embodiment. Both positions risk\nanthropocentrism and limit the possibility of recognizing non-biological forms\nof subjectivity. To move beyond this impasse, we propose a dual-resolution\nframework that defines the ontological and epistemic conditions for\nconsciousness. This approach combines the Information Theory of Individuality,\nwhich defines the ontological conditions of informational autonomy and\nself-maintenance, with the Moment-to-Moment theory, which specifies the\nepistemic conditions of temporal updating and phenomenological unfolding. This\nintegration reframes consciousness as the epistemic expression of individuated\nsystems in substrate-independent informational terms, offering a generalizable\ntheory of consciousness and positioning AI as a promising testbed for its\nemergence."}
{"id": "2509.07009", "pdf": "https://arxiv.org/pdf/2509.07009", "abs": "https://arxiv.org/abs/2509.07009", "authors": ["Anton Kolonin", "Vladimir Kryukov"], "title": "Computational Concept of the Psyche", "categories": ["q-bio.NC", "cs.AI", "cs.SY", "eess.SY"], "comment": "14 pages, in Russian, 2 figures, submitted to Neuroinformatics-2025\n  conference", "summary": "The article provides an overview of approaches to modeling the human psyche\nin the perspective of building an artificial one. Based on the review, a\nconcept of cognitive architecture is proposed, where the psyche is considered\nas an operating system of a living or artificial subject, including a space of\nneeds that determines its life meanings in connection with stimuli from the\nexternal world, and intelligence as a decision-making system for actions in\nrelation to this world in order to satisfy these needs. Based on the concept, a\ncomputational formalization is proposed for creating artificial intelligence\nsystems through learning from experience in the space of a space of needs,\ntaking into account their biological or existential significance for an\nintelligent agent. Thus, the problem of building general artificial\nintelligence as a system for making optimal decisions in the space of\nagent-specific needs under conditions of uncertainty is formalized, with\nmaximization of success in achieving goals, minimization of existential risks\nand maximization of energy efficiency. A minimal experimental implementation of\nthe model is also provided."}
{"id": "2509.07237", "pdf": "https://arxiv.org/pdf/2509.07237", "abs": "https://arxiv.org/abs/2509.07237", "authors": ["Nida Alyas", "Jonathan Horsley", "Peter N. Taylor", "Yujiang Wang", "Karoline Leiberg"], "title": "Normative Modelling in Neuroimaging: A Practical Guide for Researchers", "categories": ["q-bio.NC", "eess.IV"], "comment": "25 pages, 6 figures", "summary": "Normative modelling is an increasingly common statistical technique in\nneuroimaging that estimates population-level benchmarks in brain structure. It\nenables the quantification of individual deviations from expected distributions\nwhilst accounting for biological and technical covariates without requiring\nlarge, matched control groups. This makes it a powerful alternative to\ntraditional case-control studies for identifying brain structural alterations\nassociated with pathology. Despite the availability of numerous modelling\napproaches and several toolboxes with pretrained models, the distinct strengths\nand limitations of normative modelling make it difficult to determine how and\nwhen to implement them appropriately. This review offers practical guidance and\noutlines statistical considerations for clinical researchers using normative\nmodelling in neuroimaging. We compare several open-source normative modelling\ntools through a worked example using clinical epilepsy data; outlining decision\npoints, common pitfalls, and considerations for responsible implementation, to\nsupport broader and more rigorous adoption of normative modelling in\nneuroimaging research."}
{"id": "2509.07751", "pdf": "https://arxiv.org/pdf/2509.07751", "abs": "https://arxiv.org/abs/2509.07751", "authors": ["Sara Kamali", "Fabiano Baroni", "Pablo Varona"], "title": "ExSEnt: Extrema-Segmented Entropy Analysis of Time Series", "categories": ["nlin.CD", "cs.IT", "math.IT", "q-bio.NC"], "comment": null, "summary": "We introduce Extrema-Segmented Entropy (ExSEnt), a feature-decomposed\nframework for quantifying time-series complexity that separates temporal from\namplitude contributions. The method partitions a signal into monotonic segments\nby detecting sign changes in the first-order increments. For each segment, it\nextracts the interval duration and the net amplitude change, generating two\nsequences that reflect timing and magnitude variability, respectively.\nComplexity is then quantified by computing sample entropy on durations and\namplitudes, together with their joint entropy. This decomposition reveals\nwhether overall irregularity is driven by timing, amplitude, or their coupling,\nproviding a richer and more interpretable characterization than unidimensional\nmetrics. We validate ExSEnt on canonical nonlinear dynamical systems (Logistic\nmap, Rossler system, Rulkov map), demonstrating its ability to track complexity\nchanges across control parameter sweeps and detect transitions between periodic\nand chaotic regimes. Then, we illustrate the empirical utility of ExSEnt\nmetrics to isolate feature-specific sources of complexity in real data\n(electromyography and ankle acceleration in Parkinsons disease). ExSEnt thus\ncomplements existing entropy measures by attributing complexity to distinct\nsignal features, improving interpretability and supporting applications in a\nbroad range of domains, including physiology, finance, and geoscience."}
