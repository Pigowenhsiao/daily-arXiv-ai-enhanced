{"id": "2509.08831", "pdf": "https://arxiv.org/pdf/2509.08831", "abs": "https://arxiv.org/abs/2509.08831", "authors": ["Doai Ngo", "Mingxuan Sun", "Zhengji Zhang", "Ashwin G Ramayya", "Mark Schnitzer", "Zhe Zhao"], "title": "Path to Intelligence: Measuring Similarity between Human Brain and Large Language Model Beyond Language Task", "categories": ["q-bio.NC"], "comment": null, "summary": "Large language models (LLMs) have demonstrated human-like abilities in\nlanguage-based tasks. While language is a defining feature of human\nintelligence, it emerges from more fundamental neurophysical processes rather\nthan constituting the basis of intelligence itself. In this work, we study the\nsimilarity between LLM internal states and human brain activity in a\nsensory-motor task rooted in anticipatory and visuospatial behavior. These\nabilities are essential for cognitive performance that constitute human\nintelligence. We translate the sensory-motor task into natural language in\norder to replicate the process for LLMs. We extract hidden states from\npre-trained LLMs at key time steps and compare them to human intracranial EEG\nsignals. Our results reveal that LLM-derived reactions can be linearly mapped\nonto human neural activity. These findings suggest that LLMs, with a simple\nnatural language translation to make them understand temporal-relevant tasks,\ncan approximate human neurophysical behavior in experiments involving sensory\nstimulants. In all, our contribution is two-fold: (1) We demonstrate similarity\nbetween LLM and human brain activity beyond language-based tasks. (2) We\ndemonstrate that with such similarity, LLMs could help us understand human\nbrains by enabling us to study topics in neuroscience that are otherwise\nchallenging to tackle."}
{"id": "2509.09213", "pdf": "https://arxiv.org/pdf/2509.09213", "abs": "https://arxiv.org/abs/2509.09213", "authors": ["Alireza Irandoost", "Amirreza Bahramani", "Roya Mohajeri", "Faezeh Shahdost-Fard", "Ali Ghazizadeh", "Mehdi Fardmanesh"], "title": "A novel cost-effective fabrication of a flexible neural probe for brain signal recording", "categories": ["q-bio.NC"], "comment": null, "summary": "This study introduces a novel, flexible, and implantable neural probe using a\ncost-effective microfabrication process based on a thin polyimide film.\nPolyimide film, known as Kapton, serves as a flexible substrate for\nmicroelectrodes, conductive tracks, and contact pads of the probe, which are\nmade from a thin film of gold (Au). SU-8 is used to cover the corresponding\ntracks for electrical isolation and to increase the stiffness of the probe for\nbetter implantation. To evaluate the performance of the fabricated probe,\nelectrochemical impedance spectroscopy (EIS) and artificial neural signal\nrecording have been used to characterize its properties. The microelectrode\ndimensions have been carefully chosen to provide low impedance characteristics,\nwhich are necessary for acquiring local field potential (LFP) signals. The in\nvivo LFP data have been obtained from a male zebra finch presented with\nauditory stimuli. By properly filtering the extracellular recordings and\nanalyzing the data, the obtained results have been validated by comparing them\nwith the signals acquired with a commercial neural electrode. Due to the use of\nKapton, SU-8, and Au materials with non-toxic and adaptable properties in the\nbody environment, the fabricated neural probe is considered a promising\nbiocompatible implantable neural probe that may pave the way for the\nfabrication of other neural implantable devices with commercial aims."}
{"id": "2509.09152", "pdf": "https://arxiv.org/pdf/2509.09152", "abs": "https://arxiv.org/abs/2509.09152", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io"}
{"id": "2509.09235", "pdf": "https://arxiv.org/pdf/2509.09235", "abs": "https://arxiv.org/abs/2509.09235", "authors": ["Sarah C. Irvine", "Christian Lucas", "Diana Kr√ºger", "Bianca Guedert", "Julian Moosmann", "Berit Zeller-Plumhoff"], "title": "Virtual staining for 3D X-ray histology of bone implants", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.comp-ph", "q-bio.QM"], "comment": null, "summary": "Three-dimensional X-ray histology techniques offer a non-invasive alternative\nto conventional 2D histology, enabling volumetric imaging of biological tissues\nwithout the need for physical sectioning or chemical staining. However, the\ninherent greyscale image contrast of X-ray tomography limits its biochemical\nspecificity compared to traditional histological stains. Within digital\npathology, deep learning-based virtual staining has demonstrated utility in\nsimulating stained appearances from label-free optical images. In this study,\nwe extend virtual staining to the X-ray domain by applying cross-modality image\ntranslation to generate artificially stained slices from\nsynchrotron-radiation-based micro-CT scans. Using over 50 co-registered image\npairs of micro-CT and toluidine blue-stained histology from bone-implant\nsamples, we trained a modified CycleGAN network tailored for limited paired\ndata. Whole slide histology images were downsampled to match the voxel size of\nthe CT data, with on-the-fly data augmentation for patch-based training. The\nmodel incorporates pixelwise supervision and greyscale consistency terms,\nproducing histologically realistic colour outputs while preserving\nhigh-resolution structural detail. Our method outperformed Pix2Pix and standard\nCycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the\nmodel can be applied to full CT volumes to generate virtually stained 3D\ndatasets, enhancing interpretability without additional sample preparation.\nWhile features such as new bone formation were able to be reproduced, some\nvariability in the depiction of implant degradation layers highlights the need\nfor further training data and refinement. This work introduces virtual staining\nto 3D X-ray imaging and offers a scalable route for chemically informative,\nlabel-free tissue characterisation in biomedical research."}
{"id": "2509.09480", "pdf": "https://arxiv.org/pdf/2509.09480", "abs": "https://arxiv.org/abs/2509.09480", "authors": ["Matan Shmunik", "Michael Assaf"], "title": "Large deviations in non-Markovian stochastic epidemics", "categories": ["q-bio.PE", "cond-mat.stat-mech"], "comment": "6 pages, 4 figures + Supplemental Information file", "summary": "We develop a framework for non-Markovian SIR and SIS models beyond mean\nfield, utilizing the continuous-time random walk formalism. Using a gamma\ndistribution for the infection and recovery inter-event times as a test case,\nwe derive asymptotical late-time master equations with effective memory kernels\nand obtain analytical predictions for the final outbreak size distribution in\nthe SIR model, and quasistationary distribution and disease lifetime in the SIS\nmodel. We show that increasing memory can greatly widen the outbreak size\ndistribution and reduce the disease lifetime. We also show that rescaled\nMarkovian models fail to capture fluctuations in the non-Markovian case.\nOverall, our findings, confirmed against numerical simulations, demonstrate\nthat memory strongly shapes epidemic dynamics and paves the way for extending\nsuch analyses to structured populations."}
