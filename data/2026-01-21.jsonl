{"id": "2601.13693", "pdf": "https://arxiv.org/pdf/2601.13693", "abs": "https://arxiv.org/abs/2601.13693", "authors": ["Shengjie Xu", "Xianbin Ye", "Mengran Zhu", "Xiaonan Zhang", "Shanzhuo Zhang", "Xiaomin Fang"], "title": "End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. Despite its critical role, reverse screening remains challenging because accurately capturing interactions between a small molecule and structurally diverse proteins is inherently complex, and conventional step-wise workflows often propagate errors across decoupled steps such as target structure modeling, pocket identification, docking, and scoring. Here, we present an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model akin to AlphaFold3, which simultaneously models the folding of proteins from a protein library and the docking of small-molecule ligands within a unified framework. We validate this approach on a diverse and representative set of approximately one hundred small molecules. Compared with conventional reverse docking, our method improves screening accuracy and demonstrates enhanced structural fidelity, binding-site precision, and target prioritization. By systematically linking small molecules to their protein targets, this framework establishes a scalable and straightforward platform for dissecting molecular mechanisms, exploring off-target interactions, and supporting rational drug discovery."}
{"id": "2601.12381", "pdf": "https://arxiv.org/pdf/2601.12381", "abs": "https://arxiv.org/abs/2601.12381", "authors": ["Esra Busra Isik", "Yusuf Hakan Usta", "Haozhe Liu", "Maryam Riazi", "William Roach", "Hongpeng Zhou", "Magnus Rattray", "Sokratia Georgaka"], "title": "Multimodal Spatial Omics: From Data Acquisition to Computational Integration", "categories": ["q-bio.QM", "q-bio.BM", "q-bio.GN"], "comment": null, "summary": "Recent developments in spatial omics technologies have enabled the generation of high dimensional molecular data, such as transcriptomes, proteomes, and epigenomes, within their spatial tissue context, either through coprofiling on the same slice or through serial tissue sections. These datasets, which are often complemented by images, have given rise to multimodal frameworks that capture both the cellular and architectural complexity of tissues across multiple molecular layers. Integration in such multimodal data poses significant computational challenges due to differences in scale, resolution, and data modality. In this review, we present a comprehensive overview of computational methods developed to integrate multimodal spatial omics and imaging datasets. We highlight key algorithmic principles underlying these methods, ranging from probabilistic to the latest deep learning approaches."}
{"id": "2601.13564", "pdf": "https://arxiv.org/pdf/2601.13564", "abs": "https://arxiv.org/abs/2601.13564", "authors": ["Yanheng Li", "Zhichen Pu", "Lijiang Yang", "Zehao Zhou", "Yi Qin Gao"], "title": "Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": "Total 43 pages: 32 pages Main Text + 11 pages SI", "summary": "Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design."}
{"id": "2601.12805", "pdf": "https://arxiv.org/pdf/2601.12805", "abs": "https://arxiv.org/abs/2601.12805", "authors": ["Xiaohan Huang", "Meng Xiao", "Chuan Qin", "Qingqing Long", "Jinmiao Chen", "Yuanchun Zhou", "Hengshu Zhu"], "title": "SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding", "categories": ["q-bio.GN", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation."}
{"id": "2601.11941", "pdf": "https://arxiv.org/pdf/2601.11941", "abs": "https://arxiv.org/abs/2601.11941", "authors": ["Tatsuaki Tsuruyama"], "title": "Fluctuation Theorems from a Continuous-Time Markov Model of Information-Thermodynamic Capacity in Biochemical Signal Cascades", "categories": ["q-bio.MN"], "comment": null, "summary": "Biochemical signaling cascades transmit intracellular information while dissipating energy under nonequilibrium conditions. We model a cascade as a code string and apply information-entropy ideas to quantify an optimal transmission rate. A time-normalized entropy functional is maximized to define a capacity-like quantity governed by a conserved multiplier. To place the theory on a rigorous stochastic-thermodynamic footing, we formulate stepwise signaling as a continuous-time Markov jump process with forward and reverse competing rates. The embedded jump chain yields well-defined transition probabilities that justify time-scale-based expressions. Under local detailed balance, the log ratio of forward and reverse rates can be interpreted as entropy production per event, enabling a trajectory-level derivation of detailed and integral fluctuation theorems. We further connect the information-theoretic capacity to the mean dissipation rate and outline finite-time fluctuation structure via the scaled cumulant generating function (SCGF) and Gallavotti--Cohen symmetry, including a worked example using MAPK/ERK timescales."}
{"id": "2601.11861", "pdf": "https://arxiv.org/pdf/2601.11861", "abs": "https://arxiv.org/abs/2601.11861", "authors": ["Hailu Zhou", "Fei Jiang", "Zhigang Jiang"], "title": "Comparative efficacy and safety of pharmacological interventions for the treatment of long COVID in adults: a systematic review and network meta-analysis", "categories": ["q-bio.OT"], "comment": "68 pages, 7 figures, 1 table", "summary": "Coronavirus disease 2019 (COVID-19), caused by SARS-CoV-2, represents a major global pandemic of the 21st century, with long-term effects termed long COVID. This systematic review and network meta-analysis (NMA) evaluated pharmacological interventions for adults with long COVID, incorporating randomized controlled trials and adjusted observational studies. Primary outcomes included all-cause mortality, hospitalization, ICU admission, and mechanical ventilation; secondary outcomes covered symptom recovery across five categories, with safety assessed via adverse events. Results from random-effects models showed that saline nasal irrigation (SMD=21.10, 95% CI [16.91, 25.30]), nitrilotriacetic acid trisodium (SMD=7.40 [5.79, 9.01]), tetra sodium pyrophosphate (SMD=3.69 [2.61, 4.77]), and sodium gluconate (SMD=3.01 [1.92, 4.09]) significantly improved anosmia versus control. For thrombosis, rivaroxaban reduced arterial (OR=0.33 [0.01, 8.19]) and venous thrombotic events (OR=0.12 [0.01, 0.97]), while therapeutic-dose anticoagulants lowered thrombotic risks but increased major bleeding events (OR=1.86 [1.19, 2.89]) compared to prophylactic dosing. This NMA provides comparative evidence to guide treatment strategies for long COVID, highlighting the need for further research as new evidence emerges."}
{"id": "2601.11668", "pdf": "https://arxiv.org/pdf/2601.11668", "abs": "https://arxiv.org/abs/2601.11668", "authors": ["Houda Yaqine", "Christiane Fuchs"], "title": "Integrating Household Dynamics in Stochastic Epidemic Modeling: An SDE Approach to the SIR Framework", "categories": ["q-bio.PE"], "comment": null, "summary": "Understanding infectious disease spread remains a critical public health challenge, particularly given the interplay between household dynamics and community transmission patterns. Traditional epidemiological models often oversimplify these dynamics by treating populations as homogeneous, failing to capture crucial household-level interactions that can significantly impact disease spread. This paper introduces a new stochastic differential equation model extending the SIR framework by capturing the randomness in disease spread and incorporating household structure and heterogeneous mixing patterns. The model divides the population into groups based on age and household size, includes subpopulation-targeted lockdown parameters and constructs detailed contact matrices accounting for both public and within-household interactions.\n  Through the approximation of Markov jump processes by branching processes near the disease free equilibrium, we derive the basic reproduction number of our model and conduct global sensitivity analysis using Sobol indices to identify influential factors. Our simulations reveal that incorporating household structure leads to substantially different predictions compared to traditional models, particularly in epidemic timing and peak intensity. The stochastic framework captures important variations in outbreak trajectories overlooked by deterministic approaches, especially during early and peak phases. This work contributes to both mathematical epidemiology and practical public health planning by providing a sophisticated mathematical understanding of how population structure and randomness influence disease dynamics, offering insights for intervention strategies where household transmission plays a significant role."}
{"id": "2601.11833", "pdf": "https://arxiv.org/pdf/2601.11833", "abs": "https://arxiv.org/abs/2601.11833", "authors": ["Anthony Hur"], "title": "Karhunen-Loève Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen--Loève Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\\times$240$\\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance."}
{"id": "2601.11653", "pdf": "https://arxiv.org/pdf/2601.11653", "abs": "https://arxiv.org/abs/2601.11653", "authors": ["Fouad Bousetouane"], "title": "AI Agents Need Memory Control Over More Context", "categories": ["q-bio.NC", "cs.LG", "cs.MA"], "comment": "32 pages, 7 figures", "summary": "AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents."}
{"id": "2601.12381", "pdf": "https://arxiv.org/pdf/2601.12381", "abs": "https://arxiv.org/abs/2601.12381", "authors": ["Esra Busra Isik", "Yusuf Hakan Usta", "Haozhe Liu", "Maryam Riazi", "William Roach", "Hongpeng Zhou", "Magnus Rattray", "Sokratia Georgaka"], "title": "Multimodal Spatial Omics: From Data Acquisition to Computational Integration", "categories": ["q-bio.QM", "q-bio.BM", "q-bio.GN"], "comment": null, "summary": "Recent developments in spatial omics technologies have enabled the generation of high dimensional molecular data, such as transcriptomes, proteomes, and epigenomes, within their spatial tissue context, either through coprofiling on the same slice or through serial tissue sections. These datasets, which are often complemented by images, have given rise to multimodal frameworks that capture both the cellular and architectural complexity of tissues across multiple molecular layers. Integration in such multimodal data poses significant computational challenges due to differences in scale, resolution, and data modality. In this review, we present a comprehensive overview of computational methods developed to integrate multimodal spatial omics and imaging datasets. We highlight key algorithmic principles underlying these methods, ranging from probabilistic to the latest deep learning approaches."}
{"id": "2601.12854", "pdf": "https://arxiv.org/pdf/2601.12854", "abs": "https://arxiv.org/abs/2601.12854", "authors": ["Xiangting Li", "Tom Chou"], "title": "A generalized work theorem for stopped stochastic chemical reaction networks", "categories": ["cond-mat.stat-mech", "q-bio.MN"], "comment": "12 pp, 4 figures", "summary": "We establish a generalized work theorem for stochastic chemical reaction networks (CRNs). By using a compensated Poisson jump process, we identify a martingale structure in a generalized entropy defined relative to an auxiliary backward process and extend nonequilibrium work relations to processes stopped at bounded arbitrary times. Our results apply to discrete, mesoscopic chemical reaction networks and remain valid for singular initial conditions and state-dependent termination events. We show how martingale properties emerge directly from the structure of reaction propensities without assuming detailed balance. Stochastic simulations of a simple chemical kinetic proofreading network are used to explore the dependence of the exponentiated entropy production on initial conditions and model parameters, validating our new work theorem relationships. Our results provide new quantitative tools for analyzing biological circuits ranging from metabolic to gene regulation pathways."}
{"id": "2601.13504", "pdf": "https://arxiv.org/pdf/2601.13504", "abs": "https://arxiv.org/abs/2601.13504", "authors": ["Brandon Dunbar", "Paramahansa Pramanik", "Haley Kate Robinson"], "title": "Modeling Age-Adjusted Mortality in the United States", "categories": ["q-bio.OT"], "comment": "29 pages, 5 figures, 1 table", "summary": "This research explores how total mortality figures relate to age-standardized death rates within the United States, using the complete historical record of national mortality statistics. Through a detailed investigation of both all-cause and cause-specific mortality trends, the study evaluates the impact of demographic standardization on interpreting mortality data across different time periods and geographic regions. Results indicate a robust and persistent association between crude death totals and age-adjusted rates. However, the findings also demonstrate that without adjusting for age, comparisons over time or across locations may misrepresent underlying epidemiological shifts, largely due to evolving population age structures. The study underscores the critical role of age adjustment as a methodological tool for generating accurate, interpretable, and comparable measures of public health outcomes."}
{"id": "2601.12005", "pdf": "https://arxiv.org/pdf/2601.12005", "abs": "https://arxiv.org/abs/2601.12005", "authors": ["Susanna Manrubia", "Luis F. Seoane", "José A. Cuesta"], "title": "The challenge of scale in molecular adaptation: Local searches in astronomical genotype networks", "categories": ["q-bio.PE"], "comment": "21 pages, 3 figures", "summary": "The exploration of vast genotype spaces poses fundamental challenges for evolving populations. As the number of genotypes encoding viable phenotypes grows exponentially with genome length, populations can only explore a tiny fraction of these immense spaces, a fact consistently supported by empirical and theoretical evidence. Paradoxically, local, mutation-driven searches near abundant sequences allow populations to generate phenotypic improvements and functional innovations despite this immense search space. In this contribution, we integrate insights from viral evolution with theoretical expectations derived from genotype-phenotype maps to re-examine how high-dimensional sequence spaces shape evolutionary dynamics. In resolving the paradox, abundant phenotypes play a crucial role because their combinatorial weight biases evolutionary trajectories. We discuss how this bias, together with limited accessibility of fitness peaks, modifies traditional metaphors -- such as fitness landscapes -- and challenges standard notions of evolutionary optimality. Our results underscore that adaptation is predominantly local yet remarkably efficient, providing a unifying perspective on the coexistence of robustness, innovation, and constrained exploration in molecular evolution."}
{"id": "2601.12101", "pdf": "https://arxiv.org/pdf/2601.12101", "abs": "https://arxiv.org/abs/2601.12101", "authors": ["Pierre Gaspard"], "title": "Chargaff's second parity rule and the kinetics of DNA replication", "categories": ["q-bio.QM"], "comment": null, "summary": "A model of DNA replication is investigated, which is based on the biochemical kinetics of DNA polymerases, copying a DNA strand into its complement, except for rare point-like mutations due to nucleotide substitutions. Numerical simulations of many successive replications starting from an initial DNA sequence show that the fractions of mono- and oligonucleotides converge toward compliance with Chargaff's second parity rule. The theory of this multireplication process reveals that the approximate equalities between the fractions of complementary nucleotides are the consequence of (1) the dominant role of complementarity in the DNA replication kinetic process and (2) the smallness of the polymerase error probability. These two features lead to a robust mechanism underlying Chargaff's second parity rule."}
{"id": "2601.12053", "pdf": "https://arxiv.org/pdf/2601.12053", "abs": "https://arxiv.org/abs/2601.12053", "authors": ["Maël Donoso"], "title": "A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions."}
{"id": "2601.11592", "pdf": "https://arxiv.org/pdf/2601.11592", "abs": "https://arxiv.org/abs/2601.11592", "authors": ["Arturo Tozzi"], "title": "Low-Dimensional Interaction Spaces Impose Geometric Constraints On Collective Organization", "categories": ["physics.soc-ph", "physics.bio-ph", "q-bio.OT"], "comment": "9 pages, 0 figures", "summary": "Collective organization in physical, biophysical, and biological systems often emerges from many weak, local interactions, yet the resulting global structures display striking regularities and apparent limits in diversity. Existing theoretical approaches typically emphasize specific mechanisms, detailed dynamics, or energetic optimization, making it difficult to identify constraints that are independent of microscopic realization. Here we develop a general theoretical framework showing that, when effective interactions among system components compress into a low-dimensional interaction space, global organization is governed by geometric constraints rather than detailed dynamics. We formalize interaction spaces as metric manifolds derived from coarse-grained effective couplings and show that low interaction dimensionality imposes upper bounds on the number, separability, and robustness of distinct collective organizations. These results yield impossibility statements: many conceivable macroscopic organizations are excluded a priori, even when locally compatible interactions exist. The framework applies across equilibrium and nonequilibrium systems without assuming specific symmetries or conservation laws. By shifting the explanatory focus from generative mechanisms to structural constraints, this work establishes a general, geometry-based perspective on collective organization."}
{"id": "2601.12712", "pdf": "https://arxiv.org/pdf/2601.12712", "abs": "https://arxiv.org/abs/2601.12712", "authors": ["Tom Kimpson", "Domenic P. J. Germano", "Jennifer A. Flegg", "Mark B. Flegg"], "title": "Multiscale Modelling of Birth-Death Processes", "categories": ["q-bio.PE"], "comment": "25 pages, 5 figures. Submitted to the Journal of Mathematical Biology", "summary": "Many biological systems exhibit multiscale dynamics, where some species occur in high copy numbers while others remain rare. This heterogeneity necessitates hybrid modelling approaches: deterministic models are computationally efficient but inaccurate for low-count species, while fully stochastic simulations are accurate but prohibitively expensive. Hybrid methods like the Jump-Switch-Flow (JSF) algorithm address this by simulating low-count species stochastically and high-count species deterministically. However, selecting regime-switching thresholds to control errors for specific observables remains an open challenge. We develop a principled framework for threshold selection targeting extinction probability. We formalise JSF as a piecewise-deterministic Markov process and derive backward equations for extinction under exact and hybrid dynamics. Near extinction boundaries, complex nonlinear dynamics reduce to tractable time-inhomogeneous linear birth-death processes. This structure yields a rigorous error decomposition based on early and late excursions. Isolating the dominant error term motivates a fast, actionable heuristic. We demonstrate via Monte Carlo studies on a stochastic Lotka-Volterra model that our heuristic reliably upper-bounds empirical errors in extinction probability. This enables users to select the smallest threshold that satisfies a target error tolerance. This work paves the way for principled, efficient multiscale modelling and simulation in stochastic biological systems."}
{"id": "2601.12381", "pdf": "https://arxiv.org/pdf/2601.12381", "abs": "https://arxiv.org/abs/2601.12381", "authors": ["Esra Busra Isik", "Yusuf Hakan Usta", "Haozhe Liu", "Maryam Riazi", "William Roach", "Hongpeng Zhou", "Magnus Rattray", "Sokratia Georgaka"], "title": "Multimodal Spatial Omics: From Data Acquisition to Computational Integration", "categories": ["q-bio.QM", "q-bio.BM", "q-bio.GN"], "comment": null, "summary": "Recent developments in spatial omics technologies have enabled the generation of high dimensional molecular data, such as transcriptomes, proteomes, and epigenomes, within their spatial tissue context, either through coprofiling on the same slice or through serial tissue sections. These datasets, which are often complemented by images, have given rise to multimodal frameworks that capture both the cellular and architectural complexity of tissues across multiple molecular layers. Integration in such multimodal data poses significant computational challenges due to differences in scale, resolution, and data modality. In this review, we present a comprehensive overview of computational methods developed to integrate multimodal spatial omics and imaging datasets. We highlight key algorithmic principles underlying these methods, ranging from probabilistic to the latest deep learning approaches."}
{"id": "2601.12054", "pdf": "https://arxiv.org/pdf/2601.12054", "abs": "https://arxiv.org/abs/2601.12054", "authors": ["Guanghui Li", "Xingfei Hou", "Zhenxiang Zhao"], "title": "Automated Place Preference Paradigm for Optogenetic Stimulation of the Pedunculopontine Nucleus Reveals Motor Arrest-Linked Preference Behavior", "categories": ["q-bio.NC"], "comment": null, "summary": "Understanding how the brain integrates motor suppression with motivational processes remains a fundamental question in neuroscience. The rostral Pedunculopontine nucleus, a brainstem structure involved in motor control, has been shown to induce transient motor arrest upon optogenetic or electrical stimulation. However, our current understanding of its potential role in linking motor suppression with motivational or reinforcement-related processes is still insufficient. To further explore the effects induced by PPN stimulations and infer the potential mechanism underlying its role involved in both motor and emotional regulation, we developed a fully automated, low-cost system combining real-time animal tracking with closed-loop optogenetic stimulation, using the OpenMV Cam H7 Plus and embedded neural network models. The system autonomously detects the rat's position and triggers optical stimulation upon entry into a predefined region of interest, enabling unbiased, unsupervised behavioral assays. Optogenetic activation of CaMKIIa-expressing neurons in the rostral PPN reliably induced transient motor arrest. When stimulation was consistently paired with a specific location in a conditioned place preference task. When motor arrest was spatially paired with a defined region of interest, rats developed a robust place preference after limited training. These results suggest that rostral PPN activation can couple motor inhibition with reinforcement-related behavioral circuitry. Together, our work provides both a technical framework for scalable closed-loop neuroscience experiments and preliminary evidence that the rostral PPN may participate in coordinating motor suppression with motivational processes."}
{"id": "2601.13985", "pdf": "https://arxiv.org/pdf/2601.13985", "abs": "https://arxiv.org/abs/2601.13985", "authors": ["Andrea Mazzolini", "Mattia Corigliano", "Rossana Droghetti", "Matteo Osella", "Marco Cosentino-Lagomarsino"], "title": "Component systems: do null models explain everything?", "categories": ["cond-mat.stat-mech", "q-bio.OT"], "comment": null, "summary": "Component systems - ensembles of realizations built from a shared repertoire of modular parts - are ubiquitous in biological, ecological, technological, and socio-cultural domains. From genomes to texts, cities, and software, these systems exhibit statistical regularities that often meet the \"bona fide\" requirements of laws in the physical sciences. Here, we argue that the generality and simplicity of those laws are often due to basic combinatorial or sampling constraints, raising the question of whether such patterns are actually revealing system-specific mechanisms and how we might move beyond them. To this end, we first present a unifying mathematical framework, which allows us to compare modular systems in different fields and highlights the common \"null\" trends as well as the system-specific uniqueness, which, arguably, are signatures of the underlying generative dynamics. Next, we can exploit the framework with statistical mechanics and modern machine-learning tools for a twofold objective. (i) Explaining why the general regularities emerge, highlighting the constraints between them and the general principles at their origins, and (ii) \"subtracting\" them from data, which will isolate the informative features for inferring hidden system-specific generative processes, mechanistic and causal aspects."}
{"id": "2601.12717", "pdf": "https://arxiv.org/pdf/2601.12717", "abs": "https://arxiv.org/abs/2601.12717", "authors": ["Rukmani Ramachandran", "Akshit Goyal"], "title": "Energy flow controls the stability of multitrophic ecosystems with stratified nonreciprocity", "categories": ["q-bio.PE", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": "5 pages, 3 figures, plus supplementary information, for a total of 31 pages", "summary": "Complex systems with nonreciprocal interactions are often stratified into layers. Ecosystems are a prime example, where species at one trophic level grow by consuming those at another. Yet the dynamical consequences of such stratified nonreciprocity -- where the correlation between growth and consumption differs across trophic levels -- remain unexplored. Here, using an ecological model with three trophic levels, we reveal an emergent asymmetry: nonreciprocal interactions between consumers and predators (top and middle level) destabilize ecosystems far more readily than nonreciprocity between consumers and resources (middle and bottom level). We analytically derive the phase diagram for the model and show that its stability boundary is controlled by energy flow across trophic levels. Because energy flows upward -- from resources to predators -- diversity is progressively lower at higher trophic levels, which we show explains the asymmetry. Lowering energy flow efficiency flips the asymmetry toward resources and remarkably expands the stable region of the phase diagram, suggesting that the famous \"10% energy transfer\" seen in natural ecosystems might promote stability. More broadly, our findings show that the location of nonreciprocity within a complex network, not merely its magnitude, determines stability."}
{"id": "2601.12455", "pdf": "https://arxiv.org/pdf/2601.12455", "abs": "https://arxiv.org/abs/2601.12455", "authors": ["Kyle Adams", "Julia Bruner", "Salma Ameziane", "Ashley Brown", "Mohammed Gbadamosi", "Helen Moore"], "title": "Identifying Therapeutic Targets for Triple-Negative Breast Cancer using a Novel Mathematical Model of the Tumor Microenvironment", "categories": ["q-bio.QM"], "comment": "39 pages, 7 figures", "summary": "Triple-negative breast cancer (TNBC) is an aggressive disease with high mortality and limited treatment options, due to its lack of receptors that have targeted therapies available. The tumor microenvironment (TME) plays a critical role in TNBC progression and therapeutic resistance. In this work, we developed a novel mathematical model to describe key cellular interactions within the TNBC TME, informed by current literature and expert input. Our model consists of a system of ordinary differential equations representing five interacting cell populations: M2 macrophages, cancer-associated fibroblasts, TNBC tumor cells, cytotoxic T lymphocytes, and regulatory T cells. We performed global sensitivity analysis to determine which model parameters most strongly influence tumor burden over a clinically-relevant treatment timeframe. The pathways associated with the most-influential parameters correspond to biological mechanisms that are consistent with known and emerging therapeutic strategies in TNBC, including stromal-mediated tumor support. These results highlight key regulatory interactions within the TNBC TME and provide a quantitative framework for hypothesis generation and future investigation of combination treatment strategies."}
{"id": "2601.12258", "pdf": "https://arxiv.org/pdf/2601.12258", "abs": "https://arxiv.org/abs/2601.12258", "authors": ["Matteo Dunnhofer", "Maren Wehrheim", "Hamidreza Ramezanpour", "Sabine Muzellec", "Kohitij Kar"], "title": "Modeling Dynamic Computations in the Primate Ventral Visual Stream", "categories": ["q-bio.NC"], "comment": "4 figures, invited review submitted to Current Opinion in Neurobiology", "summary": "A major goal of computational neuroscience has been to explain how the primate ventral visual stream (VVS) transforms visual input into temporally evolving neural representations that support robust visual perception. Historically, most modeling efforts have assumed static conditions: monkeys fixate a dot, images are briefly flashed, and neural responses are analyzed through time-averaged metrics. Feedforward deep networks trained on static object recognition tasks outperform prior work in approximating these static snapshot-driven VVS responses. However, mounting neurophysiological evidence demonstrates that VVS responses are rich dynamical signals shaped not only by the retinal input but also by intrinsic circuit dynamics, recurrent interactions, and widespread top-down modulation. Moreover, real-world vision is inherently dynamic: objects move, the observer moves, and the eyes actively sample the environment. Here, we review recent progress in modeling dynamic responses in the macaque ventral stream across three domains: (1) intrinsic dynamics elicited by static images, (2) dynamics evoked by dynamic visual stimuli, and (3) dynamics generated by active sensing during eye movements. We argue that accurately modeling VVS dynamics will require representational, circuit-level, and behavioral perspectives, including multi-area recurrence, structured E/I interactions, and temporal objectives that better reflect natural behavior. We outline some key missing ingredients and propose a roadmap toward dynamic, multi-timescale models of the primate VVS."}
{"id": "2601.13010", "pdf": "https://arxiv.org/pdf/2601.13010", "abs": "https://arxiv.org/abs/2601.13010", "authors": ["Iain G. Johnston"], "title": "Extracting useful information about reversible evolutionary processes from irreversible evolutionary accumulation models", "categories": ["q-bio.PE", "stat.ME"], "comment": null, "summary": "Evolutionary accumulation models (EvAMs) are an emerging class of machine learning methods designed to infer the evolutionary pathways by which features are acquired. Applications include cancer evolution (accumulation of mutations), anti-microbial resistance (accumulation of drug resistances), genome evolution (organelle gene transfers), and more diverse themes in biology and beyond. Following these themes, many EvAMs assume that features are gained irreversibly -- no loss of features can occur. Reversible approaches do exist but are often computationally (much) more demanding and statistically less stable. Our goal here is to explore whether useful information about evolutionary dynamics which are in reality reversible can be obtained from modelling approaches with an assumption of irreversibility. We identify, and use simulation studies to quantify, errors involved in neglecting reversible dynamics, and show the situations in which approximate results from tractable models can be informative and reliable. In particular, EvAM inferences about the relative orderings of acquisitions, and the core dynamic structure of evolutionary pathways, are robust to reversibility in many cases, while estimations of uncertainty and feature interactions are more error-prone."}
{"id": "2601.12801", "pdf": "https://arxiv.org/pdf/2601.12801", "abs": "https://arxiv.org/abs/2601.12801", "authors": ["Pranali Roy Chowdhury", "Tian Xu Wang", "Abbey MacDonald", "Keith B. Tierney", "Hao Wang"], "title": "Pollutant-induced changes in fish pigmentation and spatial patterns", "categories": ["q-bio.QM", "math.AP", "physics.bio-ph"], "comment": "25", "summary": "Pigmentation abnormalities, ranging from hypo- to hyperpigmentation, can serve as biomarkers of developmental disruption in fish exposed to environmental contaminants. However, the mechanistic pathways underlying these alterations remain poorly understood. Studies have shown that pattern formation in fish development requires specific pigment cell interactions. Motivated by experimental observations of pigmentation alterations following contaminant exposure, we investigate how pollutants influence pigment cell self-organization using a continuum reaction-diffusion-advection framework. The model incorporates nonlocal Morse-type kernels to describe short- and long-range interactions among melanophores and xanthophores. Our results show that perturbations to the strengths of adhesion or repulsion can drive transitions between stripes, spots, and mixed patterns, reproducing phenotypes characteristic of fish pigmentation mutants. In particular, homotypic interactions are sensitive to contamination, leading to pronounced changes in melanophore density and resulting pigmentation patterns. Time-dependent simulations indicate that pigment changes from early short-term contaminant exposure may be recoverable, whereas prolonged exposure can lead to sustained pigment loss. In a growing fish, contaminant-induced changes in cell-cell interactions directly influence stripe formation rate, stripe number, and pigmentation levels. Overall, our study provides insight into the mechanistic link between experimentally observed pigmentation alterations and the changes in spatial patterns of adult fish."}
{"id": "2601.12424", "pdf": "https://arxiv.org/pdf/2601.12424", "abs": "https://arxiv.org/abs/2601.12424", "authors": ["William Dorrell", "James C. R. Whittington"], "title": "If Grid Cells are the Answer, What is the Question? A Review of Normative Grid Cell Theory", "categories": ["q-bio.NC"], "comment": "18 pages, 6 figures", "summary": "For 20 years the beautiful structure in the grid cell code has presented an attractive puzzle: what computation do these representations subserve, and why does it manifest so curiously in neurons. The first question quickly attracted an answer: grid cells subserve path-integration, the ability to keep track of one's position as you move about the world. Subsequent work has only solidified this link: bottom-up mechanistic models that perform path-integration match the measured neural responses, while experimental perturbations that selectively disrupt grid cell activity impair performance on path-integration dependent tasks. A more controversial area of work has been top-down normative modelling: why has the brain chosen to compute like this? Floods of ink have been spilt attempting to build a precise link between the population's objective and the measured implementation. The holy grail is a normative link with broad predictive power which generalises to other neural systems. We review this literature and argue that, despite some controversies, the literature largely agrees that grid cells can be explained as a (1) biologically plausible (2) high fidelity, non-linearly decodable code for position that (3) subserves path-integration. As a rare area of neuroscience with mature theoretical and experimental work, this story holds lessons for normative theories of neural computations, and on the risks and rewards of integrating task-optimised neural networks into such theorising."}
{"id": "2601.13349", "pdf": "https://arxiv.org/pdf/2601.13349", "abs": "https://arxiv.org/abs/2601.13349", "authors": ["Leonardo Viotti", "Luis Diego Herrera", "Garo Batmanian", "Franck Berthe", "Rachael Kramp"], "title": "Conservation priorities to prevent the next pandemic", "categories": ["q-bio.PE", "econ.GN"], "comment": null, "summary": "Diseases originating from wildlife pose a significant threat to global health, causing human and economic losses each year. The transmission of disease from animals to humans occurs at the interface between humans, livestock, and wildlife reservoirs, influenced by abiotic factors and ecological mechanisms. Although evidence suggests that intact ecosystems can reduce transmission, disease prevention has largely been neglected in conservation efforts and remains underfunded compared to mitigation. A major constraint is the lack of reliable, spatially explicit information to guide efforts effectively. Given the increasing rate of new disease emergence, accelerated by climate change and biodiversity loss, identifying priority areas for mitigating the risk of disease transmission is more crucial than ever. We present new high-resolution (1 km) maps of priority areas for targeted ecological countermeasures aimed at reducing the likelihood of zoonotic spillover, along with a methodology adaptable to local contexts. Our study compiles data on well-documented risk factors, protection status, forest restoration potential, and opportunity cost of the land to map areas with high potential for cost-effective interventions. We identify low-cost priority areas across 50 countries, including 277,000 km2 where environmental restoration could mitigate the risk of zoonotic spillover and 198,000 km2 where preventing deforestation could do the same, 95% of which are not currently under protection. The resulting layers, covering tropical regions globally, are freely available alongside an interactive no-code platform that allows users to adjust parameters and identify priority areas at multiple scales. Ecological countermeasures can be a cost-effective strategy for reducing the emergence of new pathogens; however, our study highlights the extent to which current conservation efforts fall short of this goal."}
{"id": "2601.13211", "pdf": "https://arxiv.org/pdf/2601.13211", "abs": "https://arxiv.org/abs/2601.13211", "authors": ["James N. Cobley"], "title": "A tropical geometry for bounded biochemical state spaces", "categories": ["q-bio.QM"], "comment": "15 pages, 2 figures", "summary": "Many biochemical measurements define state spaces that are bounded, absorbing, and physically irreversible, yet are routinely analysed using linear and Euclidean frameworks that assume global invertibility, symmetry, and translation invariance. This mismatch can irretrievably obscure biological structure, independent of data quality, scale, or preprocessing. This work formalises the structure of bounded biochemical state spaces using cysteine redox regulation as a representative example and identify the minimal algebraic properties required for categorically correct representation. Hard boundaries, absorbing states, and irreversible ensemble dynamics render linear algebra incompatible with these objects. This work demonstrates that tropical algebra provides a natural realisation of the required properties by replacing additive linear structure with order-based, piecewise-linear operations that encode dominance, saturation, and path dependence without contradiction. By making non-invertibility and absorption explicit rather than implicit, this framework resolves a fundamental algebraic mismatch and establishes a principled foundation for the representation and analysis of bounded biochemical data."}
{"id": "2601.12577", "pdf": "https://arxiv.org/pdf/2601.12577", "abs": "https://arxiv.org/abs/2601.12577", "authors": ["Nathan J. Wispinski", "Scott A. Stone", "Anthony Singhal", "Patrick M. Pilarski", "Craig S. Chapman"], "title": "Primate-like perceptual decision making emerges through deep recurrent reinforcement learning", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions."}
{"id": "2601.13442", "pdf": "https://arxiv.org/pdf/2601.13442", "abs": "https://arxiv.org/abs/2601.13442", "authors": ["Edward H. Hagen"], "title": "Menopause averted a midlife energetic crisis with help from older children and parents: A simulation study", "categories": ["q-bio.PE"], "comment": null, "summary": "The grandmother hypothesis is the most influential account of the evolution of menopause in humans, but other theories warrant investigation. Here I use simulations to investigate two theories that ground the evolution of menopause in biparental care. Kaplan et al. (2010) proposed a \"two-sex\" learning and skill-based account, termed the Embodied Capital Model (ECM), in which the high energetic burden of caring for multiple, slow-developing offspring was met with biparental investment. Menopause evolved because the physiological costs of pregnancy and childbirth increased with age yet productivity also increased with age, and the benefits of transferring resources to adult children and their offspring eventually outweigh the benefits of continued reproduction. Kuhle (2007) proposed the \"father absent\" hypothesis in which the higher mortality rate of husbands would often have left wives without the resources to raise young children, selecting for early reproductive cessation in monogamous couples. Simulations of hunter-gatherer energy consumption and production across the lifespan, taking account of age- and sex-specific survivorship, interbirth intervals, and varying rates of strength and foraging skill acquisition typical of contemporary foragers, reveal a pronounced midlife energy deficit that could be averted by ceasing reproduction midlife and receiving energy transfers from both younger couples (e.g., brideservice) and from older parents (the grandmother hypothesis). Menopause emerges as an integral and strictly necessary component of the unique human pattern of relatively short interbirth intervals and a long period of juvenile dependency, supporting and extending the ECM."}
{"id": "2601.13370", "pdf": "https://arxiv.org/pdf/2601.13370", "abs": "https://arxiv.org/abs/2601.13370", "authors": ["Logan Thrasher Collins"], "title": "A First Step for Expansion X-Ray Microscopy: Achieving Contrast in Expanded Tissues Sufficient to Reveal Cell Bodies", "categories": ["q-bio.QM", "q-bio.NC"], "comment": null, "summary": "Existing methods in nanoscale connectomics are at present too slow to map entire mammalian brains. As an emerging approach, expansion microscopy (ExM) has enormous promise, yet it still suffers from throughput limitations. Mapping the human brain and even mapping nonhuman primate brains therefore remain distant goals. While ExM increases effective resolution linearly, it enlarges tissue volume cubically, which dramatically increases imaging time. As a rapid tomographic technique, X-ray microscopy has potential for drastically speeding up large-volume connectomics. But to the best of my knowledge, no group has so far imaged cellular features within expanded tissue using X-ray microscopy. I herein present an early-stage report featuring the first demonstration of X-ray microscopy reconstruction of cell bodies within expanded tissue. This was achieved by combining a modified enzymatic Unclearing technique with a metallic gold stain and imaging using a laboratory X-ray microscope. I emphasize that a great deal of work remains to develop \"expansion X-ray microscopy\" (ExXRM) to the point where it can be useful for connectomics since the current iteration of ExXRM only resolves cell bodies and not neurites due to extensive off-target staining. Additionally, the current method must be modified to accommodate for the challenges of synchrotron X-ray microscopy, a vastly speedier approach than laboratory X-ray microscopy. Nonetheless, achieving X-ray contrast in expanded tissues represents a significant first step towards realizing ExXRM as a connectomics imaging modality."}
{"id": "2601.12837", "pdf": "https://arxiv.org/pdf/2601.12837", "abs": "https://arxiv.org/abs/2601.12837", "authors": ["Ricard Solé", "Luis F Seoane", "Jordi Pla-Mauri", "Michael Timothy Bennett", "Michael E. Hochberg", "Michael Levin"], "title": "Cognition spaces: natural, artificial, and hybrid", "categories": ["q-bio.NC", "cs.AI", "cs.HC", "cs.NE"], "comment": null, "summary": "Cognitive processes are realized across an extraordinary range of natural, artificial, and hybrid systems, yet there is no unified framework for comparing their forms, limits, and unrealized possibilities. Here, we propose a cognition space approach that replaces narrow, substrate-dependent definitions with a comparative representation based on organizational and informational dimensions. Within this framework, cognition is treated as a graded capacity to sense, process, and act upon information, allowing systems as diverse as cells, brains, artificial agents, and human-AI collectives to be analyzed within a common conceptual landscape. We introduce and examine three cognition spaces -- basal aneural, neural, and human-AI hybrid -- and show that their occupation is highly uneven, with clusters of realized systems separated by large unoccupied regions. We argue that these voids are not accidental but reflect evolutionary contingencies, physical constraints, and design limitations. By focusing on the structure of cognition spaces rather than on categorical definitions, this approach clarifies the diversity of existing cognitive systems and highlights hybrid cognition as a promising frontier for exploring novel forms of complexity beyond those produced by biological evolution."}
{"id": "2601.13714", "pdf": "https://arxiv.org/pdf/2601.13714", "abs": "https://arxiv.org/abs/2601.13714", "authors": ["Yuna Lim", "Gerardo Chowell", "Eunok Jung"], "title": "Cost-Effectiveness of Adult Hepatitis A Vaccination Strategies in Korea Under an Aging Susceptibility Profile", "categories": ["q-bio.PE", "math.OC"], "comment": "17 pages except reference section, 4 figures, 3 tables", "summary": "Hepatitis A severity increases sharply with age, while Korea is experiencing a cohort shift in which low seroprevalence adult cohorts are aging into older, higher fatality age groups. This demographic and immunological transition creates an urgent policy question regarding how adult vaccination should be prioritized under resource constraints. We evaluated three adult vaccination scenarios targeting low seroprevalence age groups (S1) 20 to 39 years, (S2) 40 to 59 years, and (S3) 20 to 59 years. Using an age structured dynamic transmission model calibrated to Korean data, we derived dynamically feasible vaccination allocation trajectories under realistic capacity constraints using an optimal control framework and linked these trajectories to long term transmission model simulations. We conducted DALY based cost effectiveness analyses over a lifetime horizon from both healthcare system and societal perspectives, and characterized uncertainty using probabilistic sensitivity analysis (PSA) and cost effectiveness acceptability curves (CEACs). Robustness was examined using one way sensitivity analyses. In the base case, S2 consistently yields the most favorable and robust cost effectiveness profile under both perspectives, with the lowest ICER. S3 achieved the largest reduction in DALYs but requires substantially higher incremental costs, resulting in a higher ICER than S2. S1 produces the smallest DALY reduction and is the least efficient strategy. PSA and CEACs confirm that S2 remains the preferred option across most willingness to pay ranges. S2 offers the most balanced and robustly cost effective strategy in Korea, capturing substantial mortality reduction while limiting additional program costs. S3 may be justified when higher budgets or willingness to pay thresholds are acceptable, but S2 provides the clearest value for money under epidemiological and economic conditions."}
{"id": "2601.13407", "pdf": "https://arxiv.org/pdf/2601.13407", "abs": "https://arxiv.org/abs/2601.13407", "authors": ["Mengman Wei", "Stanislav Listopad", "Qian Peng"], "title": "A Joint Survival Modeling and Therapy Knowledge Graph Framework to Characterize Opioid Use Disorder Trajectories", "categories": ["q-bio.QM"], "comment": null, "summary": "Motivation: Opioid use disorder (OUD) often arises after prescription opioid exposure and follows transitions among onset, remission, and relapse. Linked EHR-survey resources such as the All of Us Research Program enable stage-specific risk modeling and connection to intervention options. Results: We built a multi-stage framework to model time-to-onset, time-to-remission, and time-to-relapse after remission using All of Us EHR and survey data. For each participant we derived longitudinal predictors from clinical conditions and survey concepts, including recent (1/3/12-month) event counts, cumulative exposures, and time since last event. We fit regularized Cox models for each transition and aggregated selection frequencies and hazard ratios to identify a compact set of high-confidence predictors. Pain, mental health, and polysubstance use contributed across stages: chronic pain syndromes, tobacco/nicotine dependence, anxiety and depressive disorders, and cannabis dependence prominently predicted onset and relapse, whereas tobacco dependence during remission and other remission-coded conditions were strongly associated with transition to remission. To support therapeutic prioritization, we constructed a therapy knowledge graph integrating genetic targets, biological pathways, and published evidence to map identified risk factors to candidate treatments in recent OUD studies and clinical guidelines."}
{"id": "2601.13170", "pdf": "https://arxiv.org/pdf/2601.13170", "abs": "https://arxiv.org/abs/2601.13170", "authors": ["David Lipshutz", "Robert J. Lipshutz"], "title": "Global stability of a Hebbian/anti-Hebbian network for principal subspace learning", "categories": ["q-bio.NC", "cs.NE", "math.DS"], "comment": "27 pages, 6 figures", "summary": "Biological neural networks self-organize according to local synaptic modifications to produce stable computations. How modifications at the synaptic level give rise to such computations at the network level remains an open question. Pehlevan et al. [Neur. Comp. 27 (2015), 1461--1495] proposed a model of a self-organizing neural network with Hebbian and anti-Hebbian synaptic updates that implements an algorithm for principal subspace analysis; however, global stability of the nonlinear synaptic dynamics has not been established. Here, for the case that the feedforward and recurrent weights evolve at the same timescale, we prove global stability of the continuum limit of the synaptic dynamics and show that the dynamics evolve in two phases. In the first phase, the synaptic weights converge to an invariant manifold where the `neural filters' are orthonormal. In the second phase, the synaptic dynamics follow the gradient flow of a non-convex potential function whose minima correspond to neural filters that span the principal subspace of the input data."}
{"id": "2601.13730", "pdf": "https://arxiv.org/pdf/2601.13730", "abs": "https://arxiv.org/abs/2601.13730", "authors": ["Makoto Ueki", "Robin N. Thompson", "Murad Banaji"], "title": "Outbreak dynamics and population vulnerability in stochastic epidemic models on networks", "categories": ["q-bio.PE"], "comment": null, "summary": "During infectious disease epidemics, pathogen transmission occurs in host populations made up of interacting subpopulations. Using stochastic simulation and analytical approximations, we examine how outbreak sizes in networked populations depend on network architecture, subpopulation sizes and the strength of coupling between subpopulations. We find, as expected, that mean outbreak sizes are frequently lower in networked populations than in homogeneous populations with the same basic reproduction number. However, after an outbreak ends, a networked population is often vulnerable to further outbreaks, and the ending of an outbreak may not imply herd immunity in any sense. Another key finding is that a relatively small amount of randomly distributed prior immunity can be more protective in a networked population than a homogeneous population, a phenomenon which can be reproduced analytically in certain cases. We also find that in networked populations, randomly distributed prior immunity is often more protective than infection-acquired immunity; but this conclusion can be reversed in populations with highly variable susceptibility. All of these conclusions have implications for designing outbreak control strategies that aim to reduce pathogen transmission during epidemics."}
{"id": "2601.13926", "pdf": "https://arxiv.org/pdf/2601.13926", "abs": "https://arxiv.org/abs/2601.13926", "authors": ["Peter Golenderov", "Yaroslav Matushenko", "Anastasia Tushina", "Michal Barodkin"], "title": "SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions", "categories": ["q-bio.QM", "cs.LG", "eess.SP"], "comment": null, "summary": "Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems."}
{"id": "2601.13182", "pdf": "https://arxiv.org/pdf/2601.13182", "abs": "https://arxiv.org/abs/2601.13182", "authors": ["Alexander D Shaw"], "title": "Polyphonic Intelligence: Constraint-Based Emergence, Pluralistic Inference, and Non-Dominating Integration", "categories": ["q-bio.NC"], "comment": null, "summary": "Across neuroscience, artificial intelligence, and related fields, dominant models of intelligence typically privilege convergence: uncertainty is reduced, competing explanations are eliminated, and behaviour is governed by the optimisation of a single objective or policy. While this framing has proved powerful in many settings, it sits uneasily with biological and adaptive systems that maintain redundancy, ambiguity, and parallel explanatory processes over extended timescales. Here we propose an alternative perspective, termed polyphonic intelligence, in which coherent behaviour and meaning emerge from the coordination of multiple semi-independent inferential processes operating under shared constraints. Rather than resolving plurality through dominance or collapse, polyphonic systems sustain multiple explanatory trajectories and integrate them through soft alignment, compatibility relations, and bounded influence. We develop this perspective conceptually and formally, introducing a variational framework in which multiple coordinated approximations are maintained without winner-takes-all selection. This formulation makes explicit how plurality can remain stable, tractable, and productive, and clarifies how polyphonic inference differs from ensemble methods, mixture models, and Bayesian model averaging. Through proof-of-principle examples, we demonstrate that non-dominating, pluralistic inference can be implemented in simple computational systems without requiring centralised control or global convergence. We conclude by discussing implications for neuroscience, psychiatry, and artificial intelligence, and by arguing that intelligence may be more fruitfully understood as coordination without command rather than as the elimination of uncertainty."}
{"id": "2601.13947", "pdf": "https://arxiv.org/pdf/2601.13947", "abs": "https://arxiv.org/abs/2601.13947", "authors": ["Mattia Mattei", "David Soriano-Paños", "Alex Arenas"], "title": "Nonlinear competition avoidance favors coexistence in microbial populations", "categories": ["q-bio.PE", "physics.bio-ph"], "comment": null, "summary": "Bacteria regulate their motility through a variety of mechanisms, including quorum sensing (QS) and other density-dependent responses mediated by diffusible signals. While nonlinear density-dependent motility is well known in active-matter theory to generate nonequilibrium spatial patterns, its consequences for the coexistence of growing, interacting species remain less explored. Here we develop a minimal spatially structured model for two strongly competing species in which local demographic interactions are coupled to an escape response: each species increases its motility nonlinearly (sigmoidal) with the local abundance of its competitor. We show that this sigmoidal motility regulation promotes optimal spatial self-organization and can sustain long term coexistence via segregation, even in parameter regimes that yield competitive exclusion in well-mixed Lotka-Volterra dynamics. On two-dimensional lattices, the interplay between demographic competition and density-dependent motility generates a range of emergent patterns, including regimes in which the weaker competitor counterintuitively has higher total abundance. Overall, our results identify nonlinear, competitor-induced motility as a fundamental mechanism capable of sustaining coexistence in competing microbial populations."}
{"id": "2601.11691", "pdf": "https://arxiv.org/pdf/2601.11691", "abs": "https://arxiv.org/abs/2601.11691", "authors": ["Jan-Philipp Redlich", "Friedrich Feuerhake", "Stefan Nikolin", "Nadine Sarah Schaadt", "Sarah Teuber-Hanselmann", "Joachim Weis", "Sabine Luttmann", "Andrea Eberle", "Christoph Buck", "Timm Intemann", "Pascal Birnstill", "Klaus Kraywinkel", "Jonas Ort", "Peter Boor", "André Homeyer"], "title": "Explainable histomorphology-based survival prediction of glioblastoma, IDH-wildtype", "categories": ["eess.IV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Glioblastoma, IDH-wildtype (GBM-IDHwt) is the most common malignant brain tumor. Histomorphology is a crucial component of the integrated diagnosis of GBM-IDHwt. Artificial intelligence (AI) methods have shown promise to extract additional prognostic information from histological whole-slide images (WSI) of hematoxylin and eosin-stained glioblastoma tissue. Here, we present an explainable AI-based method to support systematic interpretation of histomorphological features associated with survival. It combines an explainable multiple instance learning (MIL) architecture with a sparse autoencoder (SAE) to relate human-interpretable visual patterns of tissue to survival. The MIL architecture directly identifies prognosis-relevant image tiles and the SAE maps these tiles post-hoc to visual patterns. The MIL method was trained and evaluated using a new real-world dataset that comprised 720 GBM-IDHwt cases from three hospitals and four cancer registries in Germany. The SAE was trained using 1878 WSIs of glioblastoma from five independent public data collections. Despite the many factors influencing survival time, our method showed some ability to discriminate between patients living less than 180 days or more than 360 days solely based on histomorphology (AUC: 0.67; 95% CI: 0.63-0.72). Cox proportional hazards regression confirmed a significant difference in survival time between the predicted groups after adjustment for established prognostic factors (hazard ratio: 1.47; 95% CI: 1.26-1.72). Our method identified multiple interpretable visual patterns associated with survival. Three neuropathologists separately found that 21 of the 24 most strongly associated patterns could be clearly attributed to seven histomorphological categories. Necrosis and hemorrhage appeared to be associated with shorter survival while highly cellular tumor areas were associated with longer survival."}
{"id": "2601.13297", "pdf": "https://arxiv.org/pdf/2601.13297", "abs": "https://arxiv.org/abs/2601.13297", "authors": ["Xuan Yang", "Chuanji Gao", "Cheng Xiao", "Nicholas Riccardi", "Rutvik H. Desai"], "title": "Multifaceted neural representation of words in naturalistic language", "categories": ["q-bio.NC"], "comment": "65 pages, 7 figures", "summary": "Understanding how the brain represents the multifaceted properties of words in context is essential for explaining the neural architecture of human language. Here, we combine large-scale psycholinguistic modeling with naturalistic fMRI to uncover the latent structure of word properties and their neural representations during narrative comprehension. By analyzing 106 psycholinguistic variables across 13,850 English words, we identified eight interpretable latent dimensions spanning lexical usage, word form, phonology orthography mapping, sublexical regularity, and semantic organization. These factors robustly predicted behavioral performance across lexical decision, naming, recognition, and semantic judgment tasks, demonstrating their cognitive relevance. Parcel-based and multivariate fMRI analyses of narrative listening revealed that these latent dimensions are encoded in overlapping yet functionally differentiated cortical systems. Multidimensional scaling and hierarchical clustering analyses further identified four interacting subsystems supporting sensorimotor grounding, controlled semantic retrieval, resolution of lexical competition, and contextual episodic integration. Together, these findings provide a unified neurocognitive framework linking fundamental lexical psycholinguistic dimensions to distributed cortical systems engaged during naturalistic language comprehension."}
{"id": "2601.14213", "pdf": "https://arxiv.org/pdf/2601.14213", "abs": "https://arxiv.org/abs/2601.14213", "authors": ["Emily Reynebeau", "Cristina Takacs-Vesbach", "Davorka Gulisija", "Mitchell Newberry"], "title": "Rare species advantage in Antarctic Lakes", "categories": ["q-bio.PE"], "comment": null, "summary": "The maintenance of diversity in complex ecological communities despite unpredictable dynamics and competitive exclusion is thought to require continual influx of new species or competitive advantages that accrue as species become rare. We examine isolated planktonic microbial communities under permanent ice cover in Antarctic lakes, recording prokaryotic abundance across 9 communities, 11 years, 30~m of depth, and thousands of species in the McMurdo LTER. We quantify rare species advantage by modeling community dynamics under frequency-dependent selection. We find persistent diversity and pervasive negative frequency dependence with limited immigration and turnover. While ecology and evolutionary sciences have long debated whether diversity is maintained selectively, we measure selection over a $10^4$-fold range of abundance in naturally coevolving communities and implicate rare species advantage."}
{"id": "2601.11878", "pdf": "https://arxiv.org/pdf/2601.11878", "abs": "https://arxiv.org/abs/2601.11878", "authors": ["Xi Peng"], "title": "Accelerated MR Elastography Using Learned Neural Network Representation", "categories": ["eess.SP", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches."}
{"id": "2601.13866", "pdf": "https://arxiv.org/pdf/2601.13866", "abs": "https://arxiv.org/abs/2601.13866", "authors": ["Zhengdi Zhang", "Hao Zhang", "Wenjun Xia"], "title": "Audio Outperforms Text for Visual Decoding", "categories": ["q-bio.NC"], "comment": null, "summary": "Decoding visual semantic representations from human brain activity is a significant challenge. While recent zero-shot decoding approaches have improved performance by leveraging aligned image-text datasets, they overlook a fundamental aspect of human cognition: semantic understanding is inherently anchored in the auditory modality of speech, not text. To address this, our study introduces the first comparative framework for evaluating auditory versus textual semantic modalities in zero-shot visual neural decoding. We propose a novel brain-visual-auditory multimodal alignment model that directly utilizes auditory representations to encapsulate semantics, serving as a substitute for traditional textual descriptors. Our experimental results demonstrate that the auditory modality not only surpasses the textual modality in decoding accuracy but also achieves higher computational efficiency. These findings indicate that auditory semantic representations are more closely aligned with neural activity patterns during visual processing. This work reveals the critical and previously underestimated role of auditory semantics in decoding visual cognition and provides new insights for developing brain-computer interfaces that are more congruent with natural human cognitive mechanisms."}
{"id": "2601.11636", "pdf": "https://arxiv.org/pdf/2601.11636", "abs": "https://arxiv.org/abs/2601.11636", "authors": ["Gaurav Saini", "Bappa Ghosh", "Sunita Chand"], "title": "Qualitative analysis and numerical investigations of time-fractional Zika virus model arising in population dynamics", "categories": ["math.DS", "math.NA", "q-bio.PE"], "comment": null, "summary": "Epidemic models play a crucial role in population dynamics, offering valuable insights into disease transmission while aiding in epidemic prediction and control. In this paper, we analyze the mathematical model of the time-fractional Zika virus transmission for human and mosquito populations. The fractional derivative is considered in the Caputo sense of order $α\\in(0,1).$ We begin by conducting a qualitative analysis using the stability theory of differential equations. The existence and uniqueness of the solution are established, and the model's stability is examined through Hyers-Ulam stability analysis. Furthermore, an efficient difference scheme utilizing the standard L1 technique is developed to simulate the model and analyze the solution's behavior under key parameters. The resulting nonlinear algebraic system is solved using the Newton-Raphson method. Finally, illustrative examples are presented to validate the theoretical findings. Graphical results indicate that the fractional model provides deeper insights and a better understanding of disease dynamics. These findings aid in controlling the virus through contact precautions and recommended therapies while also helping to predict its future spread."}
{"id": "2601.12219", "pdf": "https://arxiv.org/pdf/2601.12219", "abs": "https://arxiv.org/abs/2601.12219", "authors": ["Yiming Ren", "Junjie Wee", "Xi Chen", "Grace Qian", "Guo-Wei Wei"], "title": "Persistent Sheaf Laplacian Analysis of Protein Stability and Solubility Changes upon Mutation", "categories": ["math.SP", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Genetic mutations frequently disrupt protein structure, stability, and solubility, acting as primary drivers for a wide spectrum of diseases. Despite the critical importance of these molecular alterations, existing computational models often lack interpretability, and fail to integrate essential physicochemical interaction. To overcome these limitations, we propose SheafLapNet, a unified predictive framework grounded in the mathematical theory of Topological Deep Learning (TDL) and Persistent Sheaf Laplacian (PSL). Unlike standard Topological Data Analysis (TDA) tools such as persistent homology, which are often insensitive to heterogeneous information, PSL explicitly encodes specific physical and chemical information such as partial charges directly into the topological analysis. SheafLapNet synergizes these sheaf-theoretic invariants with advanced protein transformer features and auxiliary physical descriptors to capture intrinsic molecular interactions in a multiscale and mechanistic manner. To validate our framework, we employ rigorous benchmarks for both regression and classification tasks. For stability prediction, we utilize the comprehensive S2648 and S350 datasets. For solubility prediction, we employ the PON-Sol2 dataset, which provides annotations for increased, decreased, or neutral solubility changes. By integrating these multi-perspective features, SheafLapNet achieves state-of-the-art performance across these diverse benchmarks, demonstrating that sheaf-theoretic modeling significantly enhances both interpretability and generalizability in predicting mutation-induced structural and functional changes."}
{"id": "2601.14077", "pdf": "https://arxiv.org/pdf/2601.14077", "abs": "https://arxiv.org/abs/2601.14077", "authors": ["Lars C. Reining", "Thabo Matthies", "Luisa Haussner", "Rabea Turon", "Thomas S. A. Wallis"], "title": "MooneyMaker: A Python package to create ambiguous two-tone images", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research."}
{"id": "2601.12637", "pdf": "https://arxiv.org/pdf/2601.12637", "abs": "https://arxiv.org/abs/2601.12637", "authors": ["Long D. Nguyen", "Kelin Xia", "Binh P. Nguyen"], "title": "Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning."}
{"id": "2601.11614", "pdf": "https://arxiv.org/pdf/2601.11614", "abs": "https://arxiv.org/abs/2601.11614", "authors": ["Jason Qiu"], "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": "19 pages, 10 figures", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice."}
{"id": "2601.12928", "pdf": "https://arxiv.org/pdf/2601.12928", "abs": "https://arxiv.org/abs/2601.12928", "authors": ["Yaima Paz Soto", "Silena Herold Garcia", "Ximo Gual-Arnau", "Antoni Jaume-i-Capó", "Manuel González-Hidalgo"], "title": "An efficient heuristic for geometric analysis of cell deformations", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs."}
{"id": "2601.13370", "pdf": "https://arxiv.org/pdf/2601.13370", "abs": "https://arxiv.org/abs/2601.13370", "authors": ["Logan Thrasher Collins"], "title": "A First Step for Expansion X-Ray Microscopy: Achieving Contrast in Expanded Tissues Sufficient to Reveal Cell Bodies", "categories": ["q-bio.QM", "q-bio.NC"], "comment": null, "summary": "Existing methods in nanoscale connectomics are at present too slow to map entire mammalian brains. As an emerging approach, expansion microscopy (ExM) has enormous promise, yet it still suffers from throughput limitations. Mapping the human brain and even mapping nonhuman primate brains therefore remain distant goals. While ExM increases effective resolution linearly, it enlarges tissue volume cubically, which dramatically increases imaging time. As a rapid tomographic technique, X-ray microscopy has potential for drastically speeding up large-volume connectomics. But to the best of my knowledge, no group has so far imaged cellular features within expanded tissue using X-ray microscopy. I herein present an early-stage report featuring the first demonstration of X-ray microscopy reconstruction of cell bodies within expanded tissue. This was achieved by combining a modified enzymatic Unclearing technique with a metallic gold stain and imaging using a laboratory X-ray microscope. I emphasize that a great deal of work remains to develop \"expansion X-ray microscopy\" (ExXRM) to the point where it can be useful for connectomics since the current iteration of ExXRM only resolves cell bodies and not neurites due to extensive off-target staining. Additionally, the current method must be modified to accommodate for the challenges of synchrotron X-ray microscopy, a vastly speedier approach than laboratory X-ray microscopy. Nonetheless, achieving X-ray contrast in expanded tissues represents a significant first step towards realizing ExXRM as a connectomics imaging modality."}
{"id": "2601.14183", "pdf": "https://arxiv.org/pdf/2601.14183", "abs": "https://arxiv.org/abs/2601.14183", "authors": ["Francesco Mottes", "Qian-Ze Zhu", "Michael P. Brenner"], "title": "Gradient-based optimization of exact stochastic kinetic models", "categories": ["physics.comp-ph", "cond-mat.stat-mech", "q-bio.QM"], "comment": "9 pages, 4 figures, Supplementary Information", "summary": "Stochastic kinetic models describe systems across biology, chemistry, and physics where discrete events and small populations render deterministic approximations inadequate. Parameter inference and inverse design in these systems require optimizing over trajectories generated by the Stochastic Simulation Algorithm, but the discrete reaction events involved are inherently non-differentiable. We present an approach based on straight-through Gumbel-Softmax estimation that maintains exact stochastic simulations in the forward pass while approximating gradients through a continuous relaxation applied only in the backward pass. We demonstrate robust performance on parameter inference in stochastic gene expression, accurately recovering kinetic rates of telegraph promoter models from both moment statistics and full steady-state distributions across diverse and challenging parameter regimes. We further demonstrate the method's applicability to inverse design problems in stochastic thermodynamics, characterizing Pareto-optimal trade-offs between non-equilibrium currents and entropy production. The ability to efficiently differentiate through exact stochastic simulations provides a foundation for systematic inference and rational design across the many domains governed by continuous-time Markov dynamics."}
{"id": "2601.13962", "pdf": "https://arxiv.org/pdf/2601.13962", "abs": "https://arxiv.org/abs/2601.13962", "authors": ["Eike Osmers", "Dorothea Kolossa"], "title": "Optimal Calibration of the endpoint-corrected Hilbert Transform", "categories": ["eess.SP", "eess.SY", "q-bio.NC", "stat.ME"], "comment": null, "summary": "Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT."}
{"id": "2601.14205", "pdf": "https://arxiv.org/pdf/2601.14205", "abs": "https://arxiv.org/abs/2601.14205", "authors": ["J. Staforelli-Vivanco", "V. Salamanca-Levi", "R. Jofré-Cerda", "M. Rondanelli-Reyes", "I. Lamas"], "title": "Three-Dimensional Volumetric Reconstruction of Native Chilean Pollen via Lens-Free Digital In-line Holographic Microscopy", "categories": ["physics.optics", "q-bio.QM"], "comment": "5 pages, pre-print article", "summary": "This study presents a robust methodology for the 3D volumetric reconstruction of native Chileanpollen grains, specifically Gevuina avellana (hazel),Conium maculatum (hemloc) and Anthemis cotula (chamomile). Using a lens-free Digital In-line Holographic Microscopy (DLHM) system, we capture complex interference patterns that are numerically reconstructed using the Kirchhoff-Helmholtz transform. Our results demonstrate that this label-free approach provides high-fidelity morphological characterization and nanometric precision in biophysical parameter extraction, offering a scalable alternative for automated melissopalynology and environmental monitoring."}
