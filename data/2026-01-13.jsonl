{"id": "2601.06354", "pdf": "https://arxiv.org/pdf/2601.06354", "abs": "https://arxiv.org/abs/2601.06354", "authors": ["Luis F. Gordillo", "Priscilla E. Greenwood"], "title": "Designing a Resilient Allee-Ornstein-Uhlenbeck model", "categories": ["q-bio.PE", "math.OC"], "comment": null, "summary": "In stochastic population dynamics, stochastic wandering can produce transition to an absorbing state. In particular, under Allee effects, low densities amplify the possibility of population collapse. We investigate this in an Allee-Ornstein-Uhlenbeck (Allee-OU) model, that couples a bistable Allee growth equation, with demographic noise, and environmental fluctuations modeled as an Ornstein-Uhlenbeck process. This process replaces the bifurcation parameter of the deterministic Allee effect equation. In the model, small noise may induce escape from the safe basin around the positive equilibrium toward extinction. We construct a stochastic control, altering the process to have a stationary distribution. We enable tractable control design, approximating the process by one with a stationary distribution. Two controlled models are developed, one acting directly on population size and another also modulating the environment. A threshold-based implementation minimizes the frequency of interventions while maximizing safe time. Simulations demonstrate that the control stabilizes fluctuations around the equilibrium."}
{"id": "2601.07403", "pdf": "https://arxiv.org/pdf/2601.07403", "abs": "https://arxiv.org/abs/2601.07403", "authors": ["Burcu Gürbüz", "Aytül Gökçe", "Joseph Páez Chávez", "Thomas Götz"], "title": "Modeling and analysis of a novel two-strain dengue epidemics model considering secondary infections with increased mortality", "categories": ["math.DS", "q-bio.PE"], "comment": null, "summary": "In this study, we develop and analyze a deterministic two-strain host--vector model for dengue transmission that incorporates key immuno-epidemiological mechanisms, including temporary cross-immunity, antibody-dependent enhancement (ADE), disease-induced mortality during secondary infections, and explicit vector co-infection. The human population is divided into compartments for primary and secondary infections, while the mosquito population includes single- and co-infected classes. ADE is modeled through distinct primary ($α$) and secondary ($σ$) transmission rates. Using the next-generation matrix method, we derive the basic reproduction number $R_0$ and establish the local stability of the disease-free equilibrium for $R_0 < 1$. Analytical results show that one-strain endemic equilibria lose stability under ADE conditions ($σ> α$), allowing invasion by a heterologous strain. Employing center-manifold theory and numerical continuation (COCO), we demonstrate the occurrence of backward bifurcation, bistability between disease-free and endemic states, and Hopf-induced oscillations. Numerical simulations confirm transitions among disease-free, endemic, and periodic regimes as key parameters vary. The model highlights how ADE, waning cross-immunity, and vector co-infection interact to generate complex dengue dynamics and provides insights useful for designing effective control and vaccination strategies in dengue-endemic regions."}
{"id": "2601.06360", "pdf": "https://arxiv.org/pdf/2601.06360", "abs": "https://arxiv.org/abs/2601.06360", "authors": ["Mara Pleasure", "Ekaterina Redekop", "Dhakshina Ilango", "Zichen Wang", "Vedrana Ivezic", "Kimberly Flores", "Israa Laklouk", "Jitin Makker", "Gregory Fishbein", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "title": "Computational Mapping of Reactive Stroma in Prostate Cancer Yields Interpretable, Prognostic Biomarkers", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Current histopathological grading of prostate cancer relies primarily on glandular architecture, largely overlooking the tumor microenvironment. Here, we present PROTAS, a deep learning framework that quantifies reactive stroma (RS) in routine hematoxylin and eosin (H&E) slides and links stromal morphology to underlying biology. PROTAS-defined RS is characterized by nuclear enlargement, collagen disorganization, and transcriptomic enrichment of contractile pathways. PROTAS detects RS robustly in the external Prostate, Lung, Colorectal, and Ovarian (PLCO) dataset and, using domain-adversarial training, generalizes to diagnostic biopsies. In head-to-head comparisons, PROTAS outperforms pathologists for RS detection, and spatial RS features predict biochemical recurrence independently of established prognostic variables (c-index 0.80). By capturing subtle stromal phenotypes associated with tumor progression, PROTAS provides an interpretable, scalable biomarker to refine risk stratification."}
{"id": "2601.06257", "pdf": "https://arxiv.org/pdf/2601.06257", "abs": "https://arxiv.org/abs/2601.06257", "authors": ["Sobhana Jahan", "Saydul Akbar Murad", "Nick Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Gamma2Patterns: Deep Cognitive Attention Region Identification and Gamma-Alpha Pattern Analysis", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep cognitive attention is characterized by heightened gamma oscillations and coordinated visual behavior. Despite the physiological importance of these mechanisms, computational studies rarely synthesize these modalities or identify the neural regions most responsible for sustained focus. To address this gap, this work introduces Gamma2Patterns, a multimodal framework that characterizes deep cognitive attention by leveraging complementary Gamma and Alpha band EEG activity alongside Eye-tracking measurements. Using the SEED-IV dataset [1], we extract spectral power, burst-based temporal dynamics, and fixation-saccade-pupil signals across 62 channels or electrodes to analyze how neural activation differs between high-focus (Gamma-dominant) and low-focus (Alpha-dominant) states. Our findings reveal that frontopolar, temporal, anterior frontal, and parieto-occipital regions exhibit the strongest Gamma power and burst rates, indicating their dominant role in deep attentional engagement, while Eye-tracking signals confirm complementary contributions from frontal, frontopolar, and frontotemporal regions. Furthermore, we show that Gamma power and burst duration provide more discriminative markers of deep focus than Alpha power alone, demonstrating their value for attention decoding. Collectively, these results establish a multimodal, evidence-based map of cortical regions and oscillatory signatures underlying deep focus, providing a neurophysiological foundation for future brain-inspired attention mechanisms in AI systems."}
{"id": "2601.07826", "pdf": "https://arxiv.org/pdf/2601.07826", "abs": "https://arxiv.org/abs/2601.07826", "authors": ["Ninghui Hao", "Xinxing Yang", "Boshen Yan", "Dong Li", "Junzhou Huang", "Xintao Wu", "Emily S. Ruiz", "Arlene Ruiz de Luzuriaga", "Chen Zhao", "Guihong Wan"], "title": "Histopathology-centered Computational Evolution of Spatial Omics: Integration, Mapping, and Foundation Models", "categories": ["q-bio.GN"], "comment": "30 pages, 5 figures", "summary": "Spatial omics (SO) technologies enable spatially resolved molecular profiling, while hematoxylin and eosin (H&E) imaging remains the gold standard for morphological assessment in clinical pathology. Recent computational advances increasingly place H&E images at the center of SO analysis, bridging morphology with transcriptomic, proteomic, and other spatial molecular modalities, and pushing resolution toward the single-cell level. In this survey, we systematically review the computational evolution of SO from a histopathology-centered perspective and organize existing methods into three paradigms: integration, which jointly models paired multimodal data; mapping, which infers molecular profiles from H&E images; and foundation models, which learn generalizable representations from large-scale spatial datasets. We analyze how the role of H&E images evolves across these paradigms from spatial context to predictive anchor and ultimately to representation backbone in response to practical constraints such as limited paired data and increasing resolution demands. We further summarize actionable modeling directions enabled by current architectures and delineate persistent gaps driven by data, biology, and technology that are unlikely to be resolved by model design alone. Together, this survey provides a histopathology-centered roadmap for developing and applying computational frameworks in SO."}
{"id": "2601.07691", "pdf": "https://arxiv.org/pdf/2601.07691", "abs": "https://arxiv.org/abs/2601.07691", "authors": ["Jonathan D. Socha", "Seyed F. Maroufi", "Dipankar Biswas", "Richard Um", "Aruna S. Rao", "Mark G. Luciano"], "title": "A Framework for Feature Discovery in Intracranial Pressure Monitoring Data Using Neural Network Attention", "categories": ["q-bio.QM", "cs.LG"], "comment": "12 pages, 18 figures", "summary": "We present a novel framework for analyzing intracranial pressure monitoring data by applying interpretability principles. Intracranial pressure monitoring data was collected from 60 patients at Johns Hopkins. The data was segmented into individual cardiac cycles. A convolutional neural network was trained to classify each cardiac cycle into one of seven body positions. Neural network attention was extracted and was used to identify regions of interest in the waveform. Further directions for exploration are identified. This framework provides an extensible method to further understand the physiological and clinical underpinnings of the intracranial pressure waveform, which could lead to better diagnostic capabilities for intracranial pressure monitoring."}
{"id": "2601.06531", "pdf": "https://arxiv.org/pdf/2601.06531", "abs": "https://arxiv.org/abs/2601.06531", "authors": ["Da-Zheng Feng", "Hao-Xuan Du"], "title": "Learning Principles for Overcoming Non-ideal Factors in Brain", "categories": ["q-bio.NC"], "comment": null, "summary": "The human brain's computational prowess emerges not despite but because of its inherent \"non-ideal factors\"-noise, heterogeneity, structural irregularities, decentralized plasticity, systemic errors, and chaotic dynamics-challenging classical neuroscience's idealized models. These traits, long dismissed as flaws, are evolutionary adaptations that endow the brain with robustness, creativity, and adaptability. Classical frameworks falter under the brain's complexity: simulating 86 billion neurons and 100 trillion synapses is intractable, stochastic neurotransmitter release confounds signal interpretation, and the absence of global idealized models invalidates deterministic learning frameworks. Technological gaps further obscure whole-brain dynamics, revealing a disconnect between biological reality and computational abstraction."}
{"id": "2601.06381", "pdf": "https://arxiv.org/pdf/2601.06381", "abs": "https://arxiv.org/abs/2601.06381", "authors": ["Thomas Vaitses Fontanari", "Mariana Recamonde-Mendoza"], "title": "Hierarchical Pooling and Explainability in Graph Neural Networks for Tumor and Tissue-of-Origin Classification Using RNA-seq Data", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "This study explores the use of graph neural networks (GNNs) with hierarchical pooling and multiple convolution layers for cancer classification based on RNA-seq data. We combine gene expression data from The Cancer Genome Atlas (TCGA) with a precomputed STRING protein-protein interaction network to classify tissue origin and distinguish between normal and tumor samples. The model employs Chebyshev graph convolutions (K=2) and weighted pooling layers, aggregating gene clusters into 'supernodes' across multiple coarsening levels. This approach enables dimensionality reduction while preserving meaningful interactions. Saliency methods were applied to interpret the model by identifying key genes and biological processes relevant to cancer. Our findings reveal that increasing the number of convolution and pooling layers did not enhance classification performance. The highest F1-macro score (0.978) was achieved with a single pooling layer. However, adding more layers resulted in over-smoothing and performance degradation. However, the model proved highly interpretable through gradient methods, identifying known cancer-related genes and highlighting enriched biological processes, and its hierarchical structure can be used to develop new explainable architectures. Overall, while deeper GNN architectures did not improve performance, the hierarchical pooling structure provided valuable insights into tumor biology, making GNNs a promising tool for cancer biomarker discovery and interpretation"}
{"id": "2601.06214", "pdf": "https://arxiv.org/pdf/2601.06214", "abs": "https://arxiv.org/abs/2601.06214", "authors": ["Fang Wu", "Stan Z. Li"], "title": "Dynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein-protein interaction (PPI) represents a central challenge within the biology field, and accurately predicting the consequences of mutations in this context is crucial for drug design and protein engineering. Deep learning (DL) has shown promise in forecasting the effects of such mutations, but is hindered by two primary constraints. First, the structures of mutant proteins are often elusive to acquire. Secondly, PPI takes place dynamically, which is rarely integrated into the DL architecture design. To address these obstacles, we present a novel framework named Refine-PPI with two key enhancements. First, we introduce a structure refinement module trained by a mask mutation modeling (MMM) task on available wild-type structures, which is then transferred to produce the inaccessible mutant structures. Second, we employ a new kind of geometric network, called the probability density cloud network (PDC-Net), to capture 3D dynamic variations and encode the atomic uncertainty associated with PPI. Comprehensive experiments on SKEMPI.v2 substantiate the superiority of Refine-PPI over all existing tools for predicting free energy change. These findings underscore the effectiveness of our hallucination strategy and the PDC module in addressing the absence of mutant protein structure and modeling geometric uncertainty."}
{"id": "2601.07215", "pdf": "https://arxiv.org/pdf/2601.07215", "abs": "https://arxiv.org/abs/2601.07215", "authors": ["Gabriel A. Silva"], "title": "Neuronal Spike Trains as Functional-Analytic Distributions: Representation, Analysis, and Significance", "categories": ["q-bio.NC", "math.FA"], "comment": "18 pages", "summary": "The action potential constitutes the digital component of the signaling dynamics of neurons. But the biophysical nature of the full time course of the action potential associated with changes in membrane potential is fundamentally and mathematically distinct from its representation as a discrete set of events that encode when action potentials triggered in a collection spike trains. In this paper, we rigorously explore from first principles the transition and modeling from the standard biophysical picture of a single action potential to its representation as a spike in a spike train. In particular, we adopt a functional-analytic framework, using Schwartz distribution theory to represent spike trains as generalized Dirac delta functions acting on smooth test functions. We then show how and why this representation transcends a purely descriptive formalism to support deep downstream analysis and modeling of spike train neural dynamics in a mathematically consistent way."}
{"id": "2601.07546", "pdf": "https://arxiv.org/pdf/2601.07546", "abs": "https://arxiv.org/abs/2601.07546", "authors": ["Shiv Pratap Singh Rathore", "Navin Kashyap"], "title": "Estimators for Substitution Rates in Genomes from Read Data", "categories": ["cs.IT", "q-bio.GN"], "comment": null, "summary": "We study the problem of estimating the mutation rate between two sequences from noisy sequencing reads. Existing alignment-free methods typically assume direct access to the full sequences. We extend these methods to the sequencing framework, where only noisy reads from the sequences are observed. We use a simple model in which both mutations and sequencing errors are substitutions. We propose multiple estimators, provide theoretical guarantees for one of them, and evaluate the others through simulations."}
{"id": "2601.07261", "pdf": "https://arxiv.org/pdf/2601.07261", "abs": "https://arxiv.org/abs/2601.07261", "authors": ["Haomin Wu", "Zhiwei Nie", "Hongyu Zhang", "Zhixiang Ren"], "title": "Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering.However, existing deep learning-based enzyme-substrate interaction (ESI) predictors often exhibit performance degradation on sequence-divergent, out-of-distribution (OOD) cases, limiting robustness under biologically relevant perturbations.We propose O$^2$DENet, a lightweight, plug-and-play module that enhances OOD generalization via biologically and chemically informed perturbation augmentation and invariant representation learning.O$^2$DENet introduces enzyme-substrate perturbations and enforces consistency between original and augmented enzyme-substrate-pair representations to encourage invariance to distributional shifts.When integrated with representative ESI models, O$^2$DENet consistently improves predictive performance for both $k_{cat}$ and $K_m$ across stringent sequence-identity-based OOD benchmarks, achieving state-of-the-art results among the evaluated methods in terms of accuracy and robustness metrics.Overall, O$^2$DENet provides a general and effective strategy to enhance the stability and deployability of data-driven enzyme kinetics predictors for real-world enzyme engineering applications."}
{"id": "2601.07591", "pdf": "https://arxiv.org/pdf/2601.07591", "abs": "https://arxiv.org/abs/2601.07591", "authors": ["Johanna M. M. Bayer", "Augustijn A. A. de Boer", "Barbora Rehak-Bucova", "Charlotte J. Fraza", "Tobias Banaschewski", "Gareth J. Barker", "Arun L. W. Bokde", "Ruediger Bruehl", "Sylvane Desrivieres", "Herta Flor", "Hugh Garavan", "Penny Gowland", "Antoine Grigis", "Andreas Heinz", "Herve Lemaitre", "Jean-Luc Martinot", "Marie-Laure Paillere Martinot", "Eric Artigues", "Frauke Nees", "Dimitri Papadopoulos Orfanos", "Tomas Paus", "Luise Poustka", "Michael N. Smolka", "Nathalie Holz", "Nilakshi Vaidya", "Henrik Walter", "Robert Whelan", "Paul Wirsching", "Gunter Schumann", "Alzheimers Disease Neuroimaging Initiative", "Nina Kraguljac", "Christian F. Beckmann", "Andre F. Marquand"], "title": "Charting the velocity of brain growth and development", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Brain charts have emerged as a highly useful approach for understanding brain development and aging on the basis of brain imaging and have shown substantial utility in describing typical and atypical brain development with respect to a given reference model. However, all existing models are fundamentally cross-sectional and cannot capture change over time at the individual level. We address this using velocity centiles, which directly map change over time and can be overlaid onto cross-sectionally derived population centiles. We demonstrate this by modelling rates of change for 24062 scans from 10795 healthy individuals with up to 8 longitudinal measurements across the lifespan. We provide a method to detect individual deviations from a stable trajectory, generalising the notion of thrive lines, which are used in pediatric medicine to declare failure to thrive. Using this approach, we predict transition from mild cognitive impairment to dementia more accurately than by using either time point alone, replicated across two datasets. Last, by taking into account multiple time points, we improve the sensitivity of velocity models for predicting the future trajectory of brain change. This highlights the value of predicting change over time and makes a fundamental step towards precision medicine."}
{"id": "2601.07591", "pdf": "https://arxiv.org/pdf/2601.07591", "abs": "https://arxiv.org/abs/2601.07591", "authors": ["Johanna M. M. Bayer", "Augustijn A. A. de Boer", "Barbora Rehak-Bucova", "Charlotte J. Fraza", "Tobias Banaschewski", "Gareth J. Barker", "Arun L. W. Bokde", "Ruediger Bruehl", "Sylvane Desrivieres", "Herta Flor", "Hugh Garavan", "Penny Gowland", "Antoine Grigis", "Andreas Heinz", "Herve Lemaitre", "Jean-Luc Martinot", "Marie-Laure Paillere Martinot", "Eric Artigues", "Frauke Nees", "Dimitri Papadopoulos Orfanos", "Tomas Paus", "Luise Poustka", "Michael N. Smolka", "Nathalie Holz", "Nilakshi Vaidya", "Henrik Walter", "Robert Whelan", "Paul Wirsching", "Gunter Schumann", "Alzheimers Disease Neuroimaging Initiative", "Nina Kraguljac", "Christian F. Beckmann", "Andre F. Marquand"], "title": "Charting the velocity of brain growth and development", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Brain charts have emerged as a highly useful approach for understanding brain development and aging on the basis of brain imaging and have shown substantial utility in describing typical and atypical brain development with respect to a given reference model. However, all existing models are fundamentally cross-sectional and cannot capture change over time at the individual level. We address this using velocity centiles, which directly map change over time and can be overlaid onto cross-sectionally derived population centiles. We demonstrate this by modelling rates of change for 24062 scans from 10795 healthy individuals with up to 8 longitudinal measurements across the lifespan. We provide a method to detect individual deviations from a stable trajectory, generalising the notion of thrive lines, which are used in pediatric medicine to declare failure to thrive. Using this approach, we predict transition from mild cognitive impairment to dementia more accurately than by using either time point alone, replicated across two datasets. Last, by taking into account multiple time points, we improve the sensitivity of velocity models for predicting the future trajectory of brain change. This highlights the value of predicting change over time and makes a fundamental step towards precision medicine."}
{"id": "2601.06134", "pdf": "https://arxiv.org/pdf/2601.06134", "abs": "https://arxiv.org/abs/2601.06134", "authors": ["Jiquan Wang", "Sha Zhao", "Yangxuan Zhou", "Yiming Kang", "Shijian Li", "Gang Pan"], "title": "DeeperBrain: A Neuro-Grounded EEG Foundation Model Towards Universal BCI", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "comment": "Preprint.Under Review", "summary": "Electroencephalography (EEG) foundation models hold significant promise for universal Brain-Computer Interfaces (BCIs). However, existing approaches often rely on end-to-end fine-tuning and exhibit limited efficacy under frozen-probing protocols, lacking the intrinsic universality required for broad generalization. This limitation stems from adapting general-purpose sequence architectures that overlook the biophysical and dynamical principles of neural activity. To bridge this gap, we propose DeeperBrain, a neuro-grounded foundation model integrating domain-specific inductive biases into its model design and learning objectives. Architecturally, DeeperBrain incorporates a volume conduction-aware channel encoding to model spatial mixing via 3D geometry, and a neurodynamics-aware temporal encoding capturing slow adaptations using oscillatory and exponential bases. For pretraining, we introduce a dual-objective strategy combining Masked EEG Reconstruction (MER) for local fidelity and Neurodynamics Statistics Prediction (NSP). NSP enforces alignment with macroscopic brain states by predicting interpretable order parameters, including spectral power, functional connectivity, cross-frequency coupling, and dynamic complexity. Extensive experiments demonstrate that DeeperBrain achieves state-of-the-art or highly competitive performance under end-to-end fine-tuning. Crucially, it maintains superior efficacy under a rigorous frozen-probing protocol, verifying that embedding neuroscientific first principles endows learned representations with the intrinsic universality essential for universal BCI. The code will be publicly available."}
{"id": "2601.06991", "pdf": "https://arxiv.org/pdf/2601.06991", "abs": "https://arxiv.org/abs/2601.06991", "authors": ["Triet M. Tran", "Seyed Majid Razavi", "Dee H. Wu", "Sina Khanmohammadi"], "title": "Continuous Energy Landscape Model for Analyzing Brain State Transitions", "categories": ["eess.SP", "q-bio.NC"], "comment": null, "summary": "Energy landscape models characterize neural dynamics by assigning energy values to each brain state that reflect their stability or probability of occurrence. The conventional energy landscape models rely on binary brain state representation, where each region is considered either active or inactive based on some signal threshold. However, this binarization leads to significant information loss and an exponential increase in the number of possible brain states, making the calculation of energy values infeasible for large numbers of brain regions. To overcome these limitations, we propose a novel continuous energy landscape framework that employs Graph Neural Networks (GNNs) to learn a continuous precision matrix directly from functional MRI (fMRI) signals, preserving the full range of signal values during energy landscape computation. We validated our approach using both synthetic data and real-world fMRI datasets from brain tumor patients. Our results on synthetic data generated from a switching linear dynamical system (SLDS) and a Kuramoto model show that the continuous energy model achieved higher likelihood and more accurate recovery of basin geometry, state occupancy, and transition dynamics than conventional binary energy landscape models. In addition, results from the fMRI dataset indicate a 0.27 increase in AUC for predicting working memory and executive function, along with a 0.35 improvement in explained variance (R2) for predicting reaction time. These findings highlight the advantages of utilizing the full signal values in energy landscape models for capturing neuronal dynamics, with strong implications for diagnosing and monitoring neurological disorders."}
{"id": "2601.07830", "pdf": "https://arxiv.org/pdf/2601.07830", "abs": "https://arxiv.org/abs/2601.07830", "authors": ["Valentina Njaradi", "Rodrigo Carrasco-Davis", "Peter E. Latham", "Andrew Saxe"], "title": "Optimal Learning Rate Schedule for Balancing Effort and Performance", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "comment": null, "summary": "Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent's current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework."}
