{"id": "2510.09678", "pdf": "https://arxiv.org/pdf/2510.09678", "abs": "https://arxiv.org/abs/2510.09678", "authors": ["Jannatul Ferdous", "G. Matthew Fricke", "Judy L. Cannon", "Melanie E. Moses"], "title": "Bigger is Faster in the Adaptive Immune Response", "categories": ["physics.bio-ph", "q-bio.PE"], "comment": null, "summary": "Zoonotic pathogens represent a growing global risk, yet the speed of adaptive\nimmune activation across mammalian species remains poorly understood. Despite\norders-of-magnitude differences in size and metabolic rate, we show that the\ntime to initiate adaptive immunity is remarkably consistent across species. To\nunderstand this invariance, we analyse empirical data showing how the numbers\nand sizes of lymph nodes scale with body mass, finding that larger animals have\nboth more and larger lymph nodes. Using scaling theory and our mathematical\nmodel, we show that larger lymph nodes enable faster search times, conferring\nan advantage to larger animals that otherwise face slower biological times.\nThis enables mammals to maintain, or even accelerate, the time to initiate the\nadaptive immune response as body size increases. We validate our analysis in\nsimulations and compare it to empirical data."}
{"id": "2510.09828", "pdf": "https://arxiv.org/pdf/2510.09828", "abs": "https://arxiv.org/abs/2510.09828", "authors": ["Kesler O'Connor", "Julia M. Jess", "Devlin Costello", "Manuel E. Lladser"], "title": "Observer-Based Source Localization in Tree Infection Networks via Laplace Transforms", "categories": ["stat.ME", "cs.SI", "math.PR", "q-bio.PE", "92D30, 46N30, 62P10, 62-08, 60-08, 91D30, 92-04", "G.2.3; G.3"], "comment": "25 pages, 9 figures, 1 table", "summary": "We address the problem of localizing the source of infection in an\nundirected, tree-structured network under a susceptible-infected outbreak\nmodel. The infection propagates with independent random time increments (i.e.,\nedge-delays) between neighboring nodes, while only the infection times of a\nsubset of nodes can be observed. We show that a reduced set of observers may be\nsufficient, in the statistical sense, to localize the source and characterize\nits identifiability via the joint Laplace transform of the observers' infection\ntimes. Using the explicit form of these transforms in terms of the edge-delay\nprobability distributions, we propose scale-invariant least-squares estimators\nof the source. We evaluate their performance on synthetic trees and on a river\nnetwork, demonstrating accurate localization under diverse edge-delay models.\nTo conclude, we highlight overlooked technical challenges for observer-based\nsource localization on networks with cycles, where standard spanning-tree\nreductions may be ill-posed."}
{"id": "2510.10259", "pdf": "https://arxiv.org/pdf/2510.10259", "abs": "https://arxiv.org/abs/2510.10259", "authors": ["Yishen Jiang", "Xin Wang", "Wenqiang Zhu", "Ming Wei", "Longzhao Liu", "Shaoting Tang", "Hongwei Zheng"], "title": "Nonlinear Public Goods Game in Dynamical Environments", "categories": ["nlin.AO", "math.DS", "q-bio.PE"], "comment": null, "summary": "The evolutionary mechanisms of cooperative behavior represent a fundamental\ntopic in complex systems and evolutionary dynamics. Although recent advances\nhave introduced real-world stochasticity in nonlinear public goods game (PGG),\nsuch stochasticity remains static, neglecting its origin in the external\nenvironment as well as the coevolution of system stochasticity and cooperative\nbehavior driven by environmental dynamics. In this work, we introduce a dynamic\nenvironment feedback mechanism into the stochastic nonlinear PGG framework,\nestablishing a coevolutionary model that couples environmental states and\nindividual cooperative strategies. Our results demonstrate that the interplay\namong environment feedback, nonlinear effects, and stochasticity can drive the\nsystem toward a wide variety of steady-state structures, including full\ndefection, full cooperation, stable coexistence, and periodic limit cycles.\nFurther analysis reveals that asymmetric nonlinear parameters and environment\nfeedback rates exert significant regulatory effects on cooperation levels and\nsystem dynamics. This study not only enriches the theoretical framework of\nevolutionary game theory, but also provides a foundation for the management of\necological systems and the design of cooperative mechanisms in society."}
{"id": "2510.09702", "pdf": "https://arxiv.org/pdf/2510.09702", "abs": "https://arxiv.org/abs/2510.09702", "authors": ["Sara Behnamian", "Fatemeh Fogh"], "title": "Interval-Censored Survival Analysis of Grapevine Phenology: Thermal Controls on Flowering and Fruit Ripening", "categories": ["q-bio.QM", "stat.AP", "stat.CO"], "comment": null, "summary": "European grapevine (\\textit{Vitis vinifera} L.) is a climate-sensitive\nperennial whose flowering and ripening govern yield and quality. Phenological\nrecords from monitoring programs are typically collected at irregular\nintervals, so true transition dates are interval-censored, and many site-years\nare right-censored. We develop a reproducible workflow that treats phenology as\na time-to-event outcome: Status \\& Intensity observations from the USA-NPN are\nconverted to interval bounds, linked to NASA POWER daily weather, and analyzed\nwith parametric accelerated failure time (AFT) models (Weibull and\nlog-logistic). To avoid outcome-dependent bias from aggregating weather up to\nthe event date, antecedent conditions are summarized in fixed pre-season\nwindows and standardized; quality-control filters ensure adequate within-window\ndata coverage. Applied to flowering and ripening of \\textit{V.~vinifera}, the\nframework yields interpretable time-ratio effects and publication-ready tables\nand figures. Warmer pre-season conditions are associated with earlier ripening,\nwhereas flowering responses are modest and uncertain in these data;\nprecipitation plays, at most, a secondary role. The approach demonstrates how\ninterval-censored survival models with exogenous weather windows can extract\nrobust climate signals from citizen-science phenology while preserving\nobservation uncertainty, and it generalizes readily to other species and\nnetworks."}
{"id": "2510.09816", "pdf": "https://arxiv.org/pdf/2510.09816", "abs": "https://arxiv.org/abs/2510.09816", "authors": ["Bin Wang", "W. Jeffrey Johnston", "Stefano Fusi"], "title": "A mathematical theory for understanding when abstract representations emerge in neural networks", "categories": ["q-bio.NC", "math.OC", "physics.bio-ph", "stat.ML"], "comment": "18 pages, 8 figures", "summary": "Recent experiments reveal that task-relevant variables are often encoded in\napproximately orthogonal subspaces of the neural activity space. These\ndisentangled low-dimensional representations are observed in multiple brain\nareas and across different species, and are typically the result of a process\nof abstraction that supports simple forms of out-of-distribution\ngeneralization. The mechanisms by which such geometries emerge remain poorly\nunderstood, and the mechanisms that have been investigated are typically\nunsupervised (e.g., based on variational auto-encoders). Here, we show\nmathematically that abstract representations of latent variables are guaranteed\nto appear in the last hidden layer of feedforward nonlinear networks when they\nare trained on tasks that depend directly on these latent variables. These\nabstract representations reflect the structure of the desired outputs or the\nsemantics of the input stimuli. To investigate the neural representations that\nemerge in these networks, we develop an analytical framework that maps the\noptimization over the network weights into a mean-field problem over the\ndistribution of neural preactivations. Applying this framework to a\nfinite-width ReLU network, we find that its hidden layer exhibits an abstract\nrepresentation at all global minima of the task objective. We further extend\nthese analyses to two broad families of activation functions and deep\nfeedforward architectures, demonstrating that abstract representations\nnaturally arise in all these scenarios. Together, these results provide an\nexplanation for the widely observed abstract representations in both the brain\nand artificial neural networks, as well as a mathematically tractable toolkit\nfor understanding the emergence of different kinds of representations in\ntask-optimized, feature-learning network models."}
{"id": "2510.09668", "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making."}
{"id": "2510.11670", "pdf": "https://arxiv.org/pdf/2510.11670", "abs": "https://arxiv.org/abs/2510.11670", "authors": ["Ludovica Maga", "Mathias Peirlinck", "Lise Noël"], "title": "Understanding the interplay of collagen and myocyte adaptation in cardiac volume overload: a multi-constituent growth and remodeling framework", "categories": ["physics.med-ph", "cs.NA", "math.NA", "q-bio.TO"], "comment": null, "summary": "Hearts subjected to volume overload (VO) are prone to detrimental anatomical\nand functional changes in response to elevated mechanical stretches, ultimately\nleading to heart failure. Experimental findings increasingly emphasize that\norgan-scale changes following VO cannot be explained by myocyte growth alone,\nas traditionally proposed in the literature. Collagen degradation, in\nparticular, has been associated with left ventricular adaptation in both acute\nand chronic stages of VO. These hypotheses remain to be substantiated by\ncomprehensive mechanistic evidence, and the contribution of each constituent to\nmyocardial growth and remodeling (G&R) processes is yet to be quantified. In\nthis work, we establish a hybrid G&R framework in which we integrate a\nmixture-based constitutive model with the kinematic growth formulation. This\nmulti-constituent model enables us to mechanistically assess the relative\ncontributions of collagen and myocyte changes to alterations in tissue\nproperties, ventricular dimensions, and growth phenotype. Our numerical results\nconfirm that collagen dynamics control the passive mechanical response of the\nmyocardium, whereas myocytes predominantly impact the extent and the phenotype\nof eccentric hypertrophy. Importantly, collagen degradation exacerbates myocyte\nhypertrophy, demonstrating a synergistic interplay that accelerates left\nventricular progression toward diastolic dysfunction. This work constitutes an\nimportant step towards an integrated characterization of the early compensatory\nstages of VO-induced cardiac G&R."}
{"id": "2510.09716", "pdf": "https://arxiv.org/pdf/2510.09716", "abs": "https://arxiv.org/abs/2510.09716", "authors": ["Hansol Hong", "Sangwon Lee", "Jang-Ho Ha", "Sung-June Chu", "So-Hee An", "Woo-Hyun Paek", "Gyuhwa Chung", "Kyoung Tai No"], "title": "MS2toImg: A Framework for Direct Bioactivity Prediction from Raw LC-MS/MS Data", "categories": ["q-bio.QM"], "comment": "35 pages, 5 figures, 2 tables", "summary": "Untargeted metabolomics using LC-MS/MS offers the potential to\ncomprehensively profile the chemical diversity of biological samples. However,\nthe process is fundamentally limited by the \"identification bottleneck,\" where\nonly a small fraction of detected features can be annotated using existing\nspectral libraries, leaving the majority of data uncharacterized and unused. In\naddition, the inherently low reproducibility of LC-MS/MS instruments introduces\nalignment errors between runs, making feature alignment across large datasets\nboth error-prone and challenging. To overcome these constraints, we developed a\ndeep learning method that eliminates the requirement for metabolite\nidentification and reduces the influence of alignment inaccuracies. Here, we\npropose MS2toImg, a method that converts raw LC-MS/MS data into a\ntwo-dimensional images representing the global fragmentation pattern of each\nsample. These images are then used as direct input for a convolutional neural\nnetwork (CNN), enabling end-to-end prediction of biological activity without\nexplicit feature engineering or alignment. Our approach was validated using\nwild soybean samples and multiple bioactivity assays (e.g., DPPH, elastase\ninhibition). The MS2toImg-CNN model outperformed conventional machine learning\nbaselines (e.g., Random Forest, PCA), demonstrating robust classification\naccuracy across diverse tasks. By transforming raw spectral data into images,\nour framework is inherently less sensitive to alignment errors caused by low\ninstrument reproducibility, as it leverages the overall fragmentation landscape\nrather than relying on precise feature matching. This identification-free,\nimage-based approach enables more robust and scalable bioactivity prediction\nfrom untargeted metabolomics data, offering a new paradigm for high-throughput\nfunctional screening in complex biological systems."}
{"id": "2510.09951", "pdf": "https://arxiv.org/pdf/2510.09951", "abs": "https://arxiv.org/abs/2510.09951", "authors": ["Xiao-Xiong Lin", "Yuk Hoi Yiu", "Christian Leibold"], "title": "Egocentric Visual Navigation through Hippocampal Sequences", "categories": ["q-bio.NC", "cs.LG"], "comment": "20 pages, 21 figures. This is a conference submission", "summary": "Sequential activation of place-tuned neurons in an animal during navigation\nis typically interpreted as reflecting the sequence of input from adjacent\npositions along the trajectory. More recent theories about such place cells\nsuggest sequences arise from abstract cognitive objectives like planning. Here,\nwe propose a mechanistic and parsimonious interpretation to complement these\nideas: hippocampal sequences arise from intrinsic recurrent circuitry that\npropagates activity without readily available input, acting as a temporal\nmemory buffer for extremely sparse inputs.We implement a minimal sequence\ngenerator inspired by neurobiology and pair it with an actor-critic learner for\negocentric visual navigation. Our agent reliably solves a continuous maze\nwithout explicit geometric cues, with performance depending on the length of\nthe recurrent sequence. Crucially, the model outperforms LSTM cores under\nsparse input conditions (16 channels, ~2.5% activity), but not under dense\ninput, revealing a strong interaction between representational sparsity and\nmemory architecture.In contrast to LSTM agents, hidden sequence units develop\nlocalized place fields, distance-dependent spatial kernels, and task-dependent\nremapping, while inputs orthogonalize and spatial information increases across\nlayers. These phenomena align with neurobiological data and are causal to\nperformance. Together, our results show that sparse input synergizes with\nsequence-generating dynamics, providing both a mechanistic account of place\ncell sequences in the mammalian hippocampus and a simple inductive bias for\nreinforcement learning based on sparse egocentric inputs in navigation tasks."}
{"id": "2510.10020", "pdf": "https://arxiv.org/pdf/2510.10020", "abs": "https://arxiv.org/abs/2510.10020", "authors": ["Henry D. Smith", "Nathaniel L. Diamant", "Brian L. Trippe"], "title": "Calibrating Generative Models", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "comment": "Our codebase accompanying the paper is available at:\n  https://github.com/smithhenryd/cgm", "summary": "Generative models frequently suffer miscalibration, wherein class\nprobabilities and other statistics of the sampling distribution deviate from\ndesired values. We frame calibration as a constrained optimization problem and\nseek the closest model in Kullback-Leibler divergence satisfying calibration\nconstraints. To address the intractability of imposing these constraints\nexactly, we introduce two surrogate objectives for fine-tuning: (1) the relax\nloss, which replaces the constraint with a miscalibration penalty, and (2) the\nreward loss, which converts calibration into a reward fine-tuning problem. We\ndemonstrate that these approaches substantially reduce calibration error across\nhundreds of simultaneous constraints and models with up to one billion\nparameters, spanning applications in protein design, image generation, and\nlanguage modeling."}
{"id": "2510.09837", "pdf": "https://arxiv.org/pdf/2510.09837", "abs": "https://arxiv.org/abs/2510.09837", "authors": ["Bing Hu", "Jong-Hoon Park", "Helen Chen", "Young-Rae Cho", "Anita Layton"], "title": "Domain Knowledge Infused Generative Models for Drug Discovery Synthetic Data", "categories": ["q-bio.QM"], "comment": "11 pages, Chen Institute Symposium for AI Accelerated Science (AIAS\n  2025)", "summary": "The role of Artificial Intelligence (AI) is growing in every stage of drug\ndevelopment. Nevertheless, a major challenge in drug discovery AI remains: Drug\npharmacokinetic (PK) and Drug-Target Interaction (DTI) datasets collected in\ndifferent studies often exhibit limited overlap, creating data overlap\nsparsity. Thus, data curation becomes difficult, negatively impacting\ndownstream research investigations in high-throughput screening, polypharmacy,\nand drug combination. We propose xImagand-DKI, a novel\nSMILES/Protein-to-Pharmacokinetic/DTI (SP2PKDTI) diffusion model capable of\ngenerating an array of PK and DTI target properties conditioned on SMILES and\nprotein inputs that exhibit data overlap sparsity. We infuse additional\nmolecular and genomic domain knowledge from the Gene Ontology (GO) and\nmolecular fingerprints to further improve our model performance. We show that\nxImagand-DKI-generated synthetic PK data closely resemble real data univariate\nand bivariate distributions, and can adequately fill in gaps among PK and DTI\ndatasets. As such, xImagand-DKI is a promising solution for data overlap\nsparsity and may improve performance for downstream drug discovery research\ntasks. Code available at:\nhttps://github.com/GenerativeDrugDiscovery/xImagand-DKI"}
{"id": "2510.10251", "pdf": "https://arxiv.org/pdf/2510.10251", "abs": "https://arxiv.org/abs/2510.10251", "authors": ["Gualtiero Piccinini"], "title": "Neural Hardware for the Language of Thought: New Rules for an Old Game", "categories": ["q-bio.NC"], "comment": "54 pages. This report is a revised version of an unpublished essay\n  that has circulated since early 2025. Since it has been cited in print and\n  others are engaging with it in unpublished manuscripts, I am making it public\n  in this form while I work on a monograph that will draw from this material", "summary": "The Language of Thought (LOT) hypothesis posits that at least some important\ncognitive processes involve language-like representations. These\nrepresentations must be processed by appropriate hardware. Since the organ of\nbiological cognition is the nervous system, whether biological cognition relies\non a LOT depends on how neural hardware works. I distinguish between different\nversions of LOT, articulate their hardware requirements, and consider which\nversions of LOT are supported by empirical evidence. I argue that the Classical\nLOT hypothesis (Fodor 1975) is ruled out; the version of LOT that is best\nsupported by empirical evidence is the Nonclassical LOT thesis that some neural\nrepresentations mirror some of the structure of natural language and represent\nin a language-like way, yet they encode information nondigitally and are\nprocessed by ordinary (nondigital, and hence Nonclassical) neural computations\nthat rely not only on syntactic structure but many other features."}
{"id": "2510.11188", "pdf": "https://arxiv.org/pdf/2510.11188", "abs": "https://arxiv.org/abs/2510.11188", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "title": "Protein as a Second Language for LLMs", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18\n  pages, 9 figures total. Submitted to ICLR 2026 (under review)", "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels."}
{"id": "2510.11275", "pdf": "https://arxiv.org/pdf/2510.11275", "abs": "https://arxiv.org/abs/2510.11275", "authors": ["Ana Sofia Carmo", "Lourenço Abrunhosa Rodrigues", "Ana Rita Peralta", "Ana Fred", "Carla Bentes", "Hugo Plácido da Silva"], "title": "SeFEF: A Seizure Forecasting Evaluation Framework", "categories": ["q-bio.QM", "cs.LG"], "comment": "main document: 14 pages, 9 figures, 2 tables; appendix: 7 pages, 2\n  figures, 3 tables, 2 algorithms", "summary": "The lack of standardization in seizure forecasting slows progress in the\nfield and limits the clinical translation of forecasting models. In this work,\nwe introduce a Python-based framework aimed at streamlining the development,\nassessment, and documentation of individualized seizure forecasting algorithms.\n  The framework automates data labeling, cross-validation splitting, forecast\npost-processing, performance evaluation, and reporting. It supports various\nforecasting horizons and includes a model card that documents implementation\ndetails, training and evaluation settings, and performance metrics. Three\ndifferent models were implemented as a proof-of-concept. The models leveraged\nfeatures extracted from time series data and seizure periodicity. Model\nperformance was assessed using time series cross-validation and key\ndeterministic and probabilistic metrics.\n  Implementation of the three models was successful, demonstrating the\nflexibility of the framework. The results also emphasize the importance of\ncareful model interpretation due to variations in probability scaling,\ncalibration, and subject-specific differences. Although formal usability\nmetrics were not recorded, empirical observations suggest reduced development\ntime and methodological consistency, minimizing unintentional variations that\ncould affect the comparability of different approaches.\n  As a proof-of-concept, this validation is inherently limited, relying on a\nsingle-user experiment without statistical analyses or replication across\nindependent datasets. At this stage, our objective is to make the framework\npublicly available to foster community engagement, facilitate experimentation,\nand gather feedback. In the long term, we aim to contribute to the\nestablishment of a consensus on a standardized methodology for the development\nand validation of seizure forecasting algorithms in people with epilepsy."}
{"id": "2510.10286", "pdf": "https://arxiv.org/pdf/2510.10286", "abs": "https://arxiv.org/abs/2510.10286", "authors": ["Wolfgang Kurz", "Danny Baranes"], "title": "AI-Assisted Geometric Analysis of Cultured Neuronal Networks: Parallels with the Cosmic Web", "categories": ["q-bio.NC"], "comment": null, "summary": "Building on evidence of structural parallels between brain networks and the\ncosmic web [1], we apply AI-based geometric analysis to cultured neuronal\nnetworks. Isolated neurons self-organize into dendritic lattices shaped by\nreproducible wiring rules. These lattices show non-random features-frequent\ndendritic convergence, hub nodes, small-world connectivity, and large voids.\nSynaptic contacts cluster and strengthen at hubs. Strikingly, these properties\nmirror the cosmic web: dendritic branches resemble cosmic filaments and\nsynapses map to galaxies. Quantitative metrics align across systems, suggesting\nshared underlying geometric principles. We invite cross-disciplinary\ncollaboration to interrogate and extend these parallels."}
{"id": "2510.09784", "pdf": "https://arxiv.org/pdf/2510.09784", "abs": "https://arxiv.org/abs/2510.09784", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "comment": null, "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional\nspaces. In molecular science, where data collection is expensive and important\nevents are rare, compression to lower-dimensional manifolds is especially\nimportant for various downstream tasks, including generation. We combine a\ntime-lagged information bottleneck designed to characterize molecular important\nrepresentations and a diffusion model in one joint training objective. The\nresulting protocol, which we term Diffusive State Predictive Information\nBottleneck (D-SPIB), enables the balancing of representation learning and\ngeneration aims in one flexible architecture. Additionally, the model is\ncapable of combining temperature information from different molecular\nsimulation trajectories to learn a coherent and useful internal representation\nof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase\nits potential for exploring physical conditions outside the training set."}
{"id": "2510.10308", "pdf": "https://arxiv.org/pdf/2510.10308", "abs": "https://arxiv.org/abs/2510.10308", "authors": ["Yinuo Zhang", "Demao Liu", "Zhichao Liang", "Jiani Cheng", "Kexin Lou", "Jinqiao Duan", "Ting Gao", "Bin Hu", "Quanying Liu"], "title": "Artificial intelligence as a surrogate brain: Bridging neural dynamical models and data", "categories": ["q-bio.NC", "cs.NE"], "comment": "5 figures", "summary": "Recent breakthroughs in artificial intelligence (AI) are reshaping the way we\nconstruct computational counterparts of the brain, giving rise to a new class\nof ``surrogate brains''. In contrast to conventional hypothesis-driven\nbiophysical models, the AI-based surrogate brain encompasses a broad spectrum\nof data-driven approaches to solve the inverse problem, with the primary\nobjective of accurately predicting future whole-brain dynamics with historical\ndata. Here, we introduce a unified framework of constructing an AI-based\nsurrogate brain that integrates forward modeling, inverse problem solving, and\nmodel evaluation. Leveraging the expressive power of AI models and large-scale\nbrain data, surrogate brains open a new window for decoding neural systems and\nforecasting complex dynamics with high dimensionality, nonlinearity, and\nadaptability. We highlight that the learned surrogate brain serves as a\nsimulation platform for dynamical systems analysis, virtual perturbation, and\nmodel-guided neurostimulation. We envision that the AI-based surrogate brain\nwill provide a functional bridge between theoretical neuroscience and\ntranslational neuroengineering."}
{"id": "2510.09903", "pdf": "https://arxiv.org/pdf/2510.09903", "abs": "https://arxiv.org/abs/2510.09903", "authors": ["Lenny Aharon", "Keemin Lee", "Karan Sikka", "Selmaan Chettih", "Cole Hurwitz", "Liam Paninski", "Matthew R Whiteway"], "title": "An uncertainty-aware framework for data-efficient multi-view animal pose estimation", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Multi-view pose estimation is essential for quantifying animal behavior in\nscientific research, yet current methods struggle to achieve accurate tracking\nwith limited labeled data and suffer from poor uncertainty estimates. We\naddress these challenges with a comprehensive framework combining novel\ntraining and post-processing techniques, and a model distillation procedure\nthat leverages the strengths of these techniques to produce a more efficient\nand effective pose estimator. Our multi-view transformer (MVT) utilizes\npretrained backbones and enables simultaneous processing of information across\nall views, while a novel patch masking scheme learns robust cross-view\ncorrespondences without camera calibration. For calibrated setups, we\nincorporate geometric consistency through 3D augmentation and a triangulation\nloss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to\nthe nonlinear case and enhance uncertainty quantification via a variance\ninflation technique. Finally, to leverage the scaling properties of the MVT, we\ndesign a distillation procedure that exploits improved EKS predictions and\nuncertainty estimates to generate high-quality pseudo-labels, thereby reducing\ndependence on manual labels. Our framework components consistently outperform\nexisting methods across three diverse animal species (flies, mice, chickadees),\nwith each component contributing complementary benefits. The result is a\npractical, uncertainty-aware system for reliable pose estimation that enables\ndownstream behavioral analyses under real-world data constraints."}
{"id": "2510.10559", "pdf": "https://arxiv.org/pdf/2510.10559", "abs": "https://arxiv.org/abs/2510.10559", "authors": ["G. Bargigli", "L. Frassineti", "A. Lanata'", "P. Baragli", "C. Scopa", "A. Vignoli"], "title": "Evidence of Physiological Co-Modulation During Human-Animal Interaction: A Systematic Review", "categories": ["q-bio.NC"], "comment": null, "summary": "This review examines the evidence in the literature for physiological\nco-modulation during human-animal interaction. The aim of this work is to\nidentify studies that assessed co-modulation via simultaneous measurement of\nphysiological signals in both species, performing quantitative comparisons, and\nevaluate the consistency of the findings.\\\\ We searched PubMed, EM-BASE,\nScopus, Google Scholar, Animal Studies Repository, and the ''Consensus app''\ntool between June and August 2025 (last search: August 5, 2025). Risk of bias\nwas assessed using an adapted version of the ROBINS-I V2 tool. Results were\ngrouped by data analysis method, interaction context, and physiological\nparameter. Data were synthesised narratively, in structured tables and in\nbarplots. Thirty-seven studies were included, primarily focusing on dogs (n=22)\nand horses (n=15), framed primarily within the interaction contexts of\nAnimal-Assisted Therapy and Intervention (AAT and AAI) and companionship.\nCardiac and hormonal measures were most frequently assessed. Most studies (n =\n20) performed correlation analyses. Sample sizes ranged from less than 10 to\nmore than 130 dyads. Co-modulation resulted significant in 22 studies, partial\n(limited to subsets of data) in 9, and absent in 6. Time-series coupling\nmethods yielded more consistent evidence than discrete-time correlations. Many\nstudies had small samples and did not explicitly test for significant\nco-modulation. Evidence, while not conclusive, supports physiological\nco-modulation during human-animal interactions. However, studies' heterogeneity\nlimits generalizability: rather than indicating a universal phenomenon,\nfindings suggest co-modulation may emerge under specific biological and\nmethodological conditions. Future research should explicitly test its presence\nacross contexts."}
{"id": "2510.09914", "pdf": "https://arxiv.org/pdf/2510.09914", "abs": "https://arxiv.org/abs/2510.09914", "authors": ["Aditya Malusare", "Vineet Punyamoorty", "Vaneet Aggarwal"], "title": "Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Artificial Intelligence, October 2025", "summary": "Recent breakthroughs in generative modeling have demonstrated remarkable\ncapabilities in molecular generation, yet the integration of comprehensive\nbiomedical knowledge into these models has remained an untapped frontier. In\nthis study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model),\na novel framework that leverages knowledge graphs to augment diffusion-based\ngenerative models for drug discovery. By embedding structured information from\nlarge-scale knowledge graphs, K-DREAM directs molecular generation toward\ncandidates with higher biological relevance and therapeutic suitability. This\nintegration ensures that the generated molecules are aligned with specific\ntherapeutic targets, moving beyond traditional heuristic-driven approaches. In\ntargeted drug design tasks, K-DREAM generates drug candidates with improved\nbinding affinities and predicted efficacy, surpassing current state-of-the-art\ngenerative models. It also demonstrates flexibility by producing molecules\ndesigned for multiple targets, enabling applications to complex disease\nmechanisms. These results highlight the utility of knowledge-enhanced\ngenerative models in rational drug design and their relevance to practical\ntherapeutic development."}
{"id": "2510.10733", "pdf": "https://arxiv.org/pdf/2510.10733", "abs": "https://arxiv.org/abs/2510.10733", "authors": ["Eva Guttmann-Flury", "Jian Zhao", "Mohamad Sawan"], "title": "Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Electroencephalography (EEG) provides a non-invasive window into brain\nactivity, enabling Brain-Computer Interfaces (BCIs) for communication and\ncontrol. However, their performance is limited by signal fidelity issues, among\nwhich the choice of re-referencing strategy is a pervasive but often overlooked\npreprocessing bias. Addressing controversies about its necessity and optimal\nchoice, we adopted a quantified approach to evaluate four strategies - no\nre-referencing, Common Average Reference (CAR), small Laplacian, and large\nLaplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our\nknowledge, this is the first study systematically quantifying their impact on\nsingle-trial P300 classification accuracy. Our controlled pipeline isolated\nre-referencing effects for source-space reconstruction (eLORETA with Phase Lag\nIndex) and anatomically constrained classification. The large Laplacian\nresolves distributed P3b networks while maintaining P3a specificity, achieving\nthe best P300 peak classification accuracy (81.57% hybrid method; 75.97%\nmajority regions of interest). Performance follows a consistent and\nstatistically significant hierarchy: large Laplacian > CAR > no re-reference >\nsmall Laplacian, providing a foundation for unified methodological evaluation."}
{"id": "2510.10614", "pdf": "https://arxiv.org/pdf/2510.10614", "abs": "https://arxiv.org/abs/2510.10614", "authors": ["Robert G. Cowell"], "title": "A clustering algorithm for the single cell analysis of mixtures", "categories": ["stat.AP", "q-bio.QM"], "comment": "48 paegs, 5 figures", "summary": "A probabilistic clustering algorithm is proposed for the analysis of forensic\nDNA mixtures in which individual cells are isolated and short tandem repeats\nare amplified using the polymerase chain reaction to generate single cell\nelectropherograms. The task of the algorithm is to use the peak height\ninformation in the electropherograms to group the cells according to their\ncontributors. Using a recently developed experimental set of individual cell\nelectropherograms, a large set of simulations shows that the proposed\nclustering algorithm has excellent performance in correctly grouping single\ncells, and for assigning likelihood ratios for persons of interest (of known\ngenotype)."}
{"id": "2510.10770", "pdf": "https://arxiv.org/pdf/2510.10770", "abs": "https://arxiv.org/abs/2510.10770", "authors": ["Eva Guttmann-Flury", "Yanyan Wei", "Shan Zhao", "Jian Zhao", "Mohamad Sawan"], "title": "The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Electrode density optimization in electroencephalography (EEG)-based\nBrain-Computer Interfaces (BCIs) requires balancing practical usability against\nsignal fidelity, particularly for source localization. Reducing electrodes\nenhances portability but its effects on neural source reconstruction quality\nand source connectivity - treated as proxies to BCI performance - remain\nunderstudied. We address this gap through systematic evaluation of 62-, 32-,\nand 16-channel configurations using a fixed, fully automated processing\npipeline applied to the well-characterized P300 potential. This approach's\nrationale is to minimize variability and bias inherent to EEG analysis by\nleveraging the P300's stimulus-locked reproducibility and pipeline\nstandardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset\nwith rigorous artifact correction and channel validation, we demonstrate: (1)\nProgressive degradation in source reconstruction quality with sparser\nconfigurations, including obscured deep neural generators and spatiotemporal\ndistortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio\n(Re) to localization accuracy - a previously unquantified relationship to the\nbest of our knowledge; (3) While reduced configurations preserve basic P300\ntopography and may suffice for communicative BCIs, higher-density channels are\nessential for reliable deep source reconstruction. Overall, this study\nestablishes a first step towards quantitative benchmarks for electrode\nselection, with critical implications for clinical BCIs requiring anatomical\nprecision in applications like neurodegenerative disease monitoring, where\ncompromised spatial resolution could mask pathological signatures. Most\nimportantly, the sqrt(Re) scaling law may provide the first principled method\nto determine the minimal electrode density required based on acceptable error\nmargins or expected effect sizes."}
{"id": "2510.10733", "pdf": "https://arxiv.org/pdf/2510.10733", "abs": "https://arxiv.org/abs/2510.10733", "authors": ["Eva Guttmann-Flury", "Jian Zhao", "Mohamad Sawan"], "title": "Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Electroencephalography (EEG) provides a non-invasive window into brain\nactivity, enabling Brain-Computer Interfaces (BCIs) for communication and\ncontrol. However, their performance is limited by signal fidelity issues, among\nwhich the choice of re-referencing strategy is a pervasive but often overlooked\npreprocessing bias. Addressing controversies about its necessity and optimal\nchoice, we adopted a quantified approach to evaluate four strategies - no\nre-referencing, Common Average Reference (CAR), small Laplacian, and large\nLaplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our\nknowledge, this is the first study systematically quantifying their impact on\nsingle-trial P300 classification accuracy. Our controlled pipeline isolated\nre-referencing effects for source-space reconstruction (eLORETA with Phase Lag\nIndex) and anatomically constrained classification. The large Laplacian\nresolves distributed P3b networks while maintaining P3a specificity, achieving\nthe best P300 peak classification accuracy (81.57% hybrid method; 75.97%\nmajority regions of interest). Performance follows a consistent and\nstatistically significant hierarchy: large Laplacian > CAR > no re-reference >\nsmall Laplacian, providing a foundation for unified methodological evaluation."}
{"id": "2510.10791", "pdf": "https://arxiv.org/pdf/2510.10791", "abs": "https://arxiv.org/abs/2510.10791", "authors": ["Dale Zhou", "Sharon Mina Noh", "Nora C Harhen", "Nidhi V Banavar", "C. Brock Kirwan", "Michael A Yassa", "Aaron M Bornstein"], "title": "A compressed code for memory discrimination", "categories": ["q-bio.NC"], "comment": null, "summary": "The ability to discriminate similar visual stimuli is an important index of\nmemory function. This ability is widely thought to be supported by expanding\nthe dimensionality of relevant neural codes, such that neural representations\nfor similar stimuli are maximally distinct, or ``separated.'' An alternative\nhypothesis is that discrimination is supported by lossy compression of visual\ninputs, efficiently coding sensory information by discarding seemingly\nirrelevant details. A benefit of compression, relative to expansion, is that it\nallows individuals to retain fewer essential dimensions underlying stimulus\nvariation -- a process linked to higher-order visual processing -- without\nhindering discrimination. Under this hypothesis, pattern separation is\nfacilitated when more information from similar stimuli can be discarded, rather\nthan preserved. We test the compression versus expansion hypotheses by\npredicting performance on the canonical mnemonic similarity task. We train\nneural networks to compress perceptual and semantic factors of stimuli,\nmeasuring lossiness using the mathematical framework underlying compression.\nConsistent with the compression hypothesis, and not the expansion hypothesis,\ngreater lossiness predicts the ease and performance of lure discrimination,\nespecially in deeper convolutional network layers that predict higher-order\nvisual brain activity. We then confirm these predictions across two image sets,\nfour behavioral datasets, and alternative lossiness metrics. Finally, using\ntask fMRI, we identify signatures of lossy compression -- neural dimensionality\nreduction and information loss -- in higher-order visual regions V4 and IT and\nhippocampal DG/CA3 and CA1 linked to lure discrimination. These results suggest\nlossy compression supports mnemonic discrimination by discarding redundant and\noverlapping information."}
{"id": "2510.10770", "pdf": "https://arxiv.org/pdf/2510.10770", "abs": "https://arxiv.org/abs/2510.10770", "authors": ["Eva Guttmann-Flury", "Yanyan Wei", "Shan Zhao", "Jian Zhao", "Mohamad Sawan"], "title": "The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Electrode density optimization in electroencephalography (EEG)-based\nBrain-Computer Interfaces (BCIs) requires balancing practical usability against\nsignal fidelity, particularly for source localization. Reducing electrodes\nenhances portability but its effects on neural source reconstruction quality\nand source connectivity - treated as proxies to BCI performance - remain\nunderstudied. We address this gap through systematic evaluation of 62-, 32-,\nand 16-channel configurations using a fixed, fully automated processing\npipeline applied to the well-characterized P300 potential. This approach's\nrationale is to minimize variability and bias inherent to EEG analysis by\nleveraging the P300's stimulus-locked reproducibility and pipeline\nstandardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset\nwith rigorous artifact correction and channel validation, we demonstrate: (1)\nProgressive degradation in source reconstruction quality with sparser\nconfigurations, including obscured deep neural generators and spatiotemporal\ndistortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio\n(Re) to localization accuracy - a previously unquantified relationship to the\nbest of our knowledge; (3) While reduced configurations preserve basic P300\ntopography and may suffice for communicative BCIs, higher-density channels are\nessential for reliable deep source reconstruction. Overall, this study\nestablishes a first step towards quantitative benchmarks for electrode\nselection, with critical implications for clinical BCIs requiring anatomical\nprecision in applications like neurodegenerative disease monitoring, where\ncompromised spatial resolution could mask pathological signatures. Most\nimportantly, the sqrt(Re) scaling law may provide the first principled method\nto determine the minimal electrode density required based on acceptable error\nmargins or expected effect sizes."}
{"id": "2510.11503", "pdf": "https://arxiv.org/pdf/2510.11503", "abs": "https://arxiv.org/abs/2510.11503", "authors": ["Katherine M. Collins", "Cedegao E. Zhang", "Lionel Wong", "Mauricio Barba da Costa", "Graham Todd", "Adrian Weller", "Samuel J. Cheyette", "Thomas L. Griffiths", "Joshua B. Tenenbaum"], "title": "People use fast, flat goal-directed simulation to reason about novel problems", "categories": ["q-bio.NC", "cs.AI", "cs.GT"], "comment": "Pre-print", "summary": "Games have long been a microcosm for studying planning and reasoning in both\nnatural and artificial intelligence, especially with a focus on expert-level or\neven super-human play. But real life also pushes human intelligence along a\ndifferent frontier, requiring people to flexibly navigate decision-making\nproblems that they have never thought about before. Here, we use novice\ngameplay to study how people make decisions and form judgments in new problem\nsettings. We show that people are systematic and adaptively rational in how\nthey play a game for the first time, or evaluate a game (e.g., how fair or how\nfun it is likely to be) before they have played it even once. We explain these\ncapacities via a computational cognitive model that we call the \"Intuitive\nGamer\". The model is based on mechanisms of fast and flat (depth-limited)\ngoal-directed probabilistic simulation--analogous to those used in Monte Carlo\ntree-search models of expert game-play, but scaled down to use very few\nstochastic samples, simple goal heuristics for evaluating actions, and no deep\nsearch. In a series of large-scale behavioral studies with over 1000\nparticipants and 121 two-player strategic board games (almost all novel to our\nparticipants), our model quantitatively captures human judgments and decisions\nvarying the amount and kind of experience people have with a game--from no\nexperience at all (\"just thinking\"), to a single round of play, to indirect\nexperience watching another person and predicting how they should play--and\ndoes so significantly better than much more compute-intensive expert-level\nmodels. More broadly, our work offers new insights into how people rapidly\nevaluate, act, and make suggestions when encountering novel problems, and could\ninform the design of more flexible and human-like AI systems that can determine\nnot just how to solve new tasks, but whether a task is worth thinking about at\nall."}
{"id": "2510.11257", "pdf": "https://arxiv.org/pdf/2510.11257", "abs": "https://arxiv.org/abs/2510.11257", "authors": ["Davide Borghini", "Davide Marchi", "Angelo Nardone", "Giordano Scerra", "Silvia Giulia Galfrè", "Alessandro Pingitore", "Giuseppe Prencipe", "Corrado Priami", "Alina Sîrbu"], "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": "Presented in the Poster Session of Computational Intelligence methods\n  for Bioinformatics and Biostatistics (CIBB) 2025", "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase."}
{"id": "2510.11664", "pdf": "https://arxiv.org/pdf/2510.11664", "abs": "https://arxiv.org/abs/2510.11664", "authors": ["Caitlin Callaghan", "David J Reinkensmeyer"], "title": "Proprioceptive Misestimation of Hand Speed", "categories": ["q-bio.NC"], "comment": "7 pages, 6 figures", "summary": "The accuracy with which the human proprioceptive system estimates hand speed\nis not well understood. To investigate this, we designed an experiment using\nhobby-grade mechatronics parts and integrated it as a laboratory exercise in a\nlarge remote laboratory course. In a simple joint position reproduction task,\nparticipants (N = 191) grasped a servomotor-driven shaft with one hand as it\nfollowed a randomized trajectory composed of sinusoidal submovements. They\nsimultaneously attempted to reproduce the movement by turning the shaft of a\npotentiometer with the other hand. Focusing on the first movement of the\ntrajectory, we found that participants consistently overestimated the speed of\nthe slowest rotations by ~45% and underestimated the speed of the fastest\nrotations also by ~30%. Speed estimation errors were near zero for trajectories\nwith peak velocities ~63 deg/s. Participants' movements also overshot slow\ntrajectories and undershot fast trajectories. We show that these trajectory\nerrors can be explained by a model in which the proprioceptive system\nintegrates velocity misestimates to infer position."}
{"id": "2510.10276", "pdf": "https://arxiv.org/pdf/2510.10276", "abs": "https://arxiv.org/abs/2510.10276", "authors": ["Nikolaus Salvatore", "Hao Wang", "Qiong Zhang"], "title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "The performance of Large Language Models (LLMs) often degrades when crucial\ninformation is in the middle of a long context, a \"lost-in-the-middle\"\nphenomenon that mirrors the primacy and recency effects in human memory. We\npropose that this behavior is not simply a flaw indicative of information loss\nbut an adaptation to different information retrieval demands during\npre-training: some tasks require uniform recall across the entire input (a\nlong-term memory demand), while others prioritize the most recent information\n(a short-term memory demand). Consistent with this view, we show that this\nU-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are\ntrained from scratch on two simple human memory paradigms simulating long-term\nand short-term memory demands. Our analysis reveals that while the recency\neffect directly aligns with short-term memory demand in the training data, the\nprimacy effect is induced by the uniform long-term memory demand and is\nadditionally influenced by the model's autoregressive properties and the\nformation of attention sinks. Our main findings from simple human memory\nparadigms also generalize to a sequence completion task, which more closely\nresembles the next-token prediction process in LLM pre-training. Together, our\nfindings reveal how information retrieval demands, model architecture, and\nstructural attention dynamics during model training can jointly produce\npositional bias observed in LLMs."}
{"id": "2510.10289", "pdf": "https://arxiv.org/pdf/2510.10289", "abs": "https://arxiv.org/abs/2510.10289", "authors": ["Ke Ma", "Andrey Vlasov", "Zeynep B. Simsek", "Jinshui Zhang", "Yiru Li", "Boshuo Wang", "David L. K. Murphy", "Jessica Y. Choi", "Maya E. Clinton", "Noreen Bukhari-Parlakturk", "Angel V. Peterchev", "Stephan M. Goetz"], "title": "Optimal monophasic, asymmetric electric field pulses for selective transcranial magnetic stimulation (TMS) with minimised power and coil heating", "categories": ["eess.SY", "cs.SY", "q-bio.NC"], "comment": "31 pages, 8 figures", "summary": "Transcranial magnetic stimulation (TMS) with asymmetric electric field\npulses, such as monophasic, offers directional selectivity for neural\nactivation but requires excessive energy. Previous pulse shape optimisation has\nbeen limited to symmetric pulses or heavily constrained variations of\nconventional waveforms without achieving general optimality in energy\nefficiency or neural selectivity. We implemented an optimisation framework that\nincorporates neuron model activation constraints and flexible control of pulse\nasymmetry. The optimised electric field waveforms achieved up to 92 % and 88 %\nreduction in energy loss and thus coil heating respectively compared to\nconventional monophasic pulses and previously improved monophasic-equivalent\npulses. In the human experiments, OUR pulses showed similar motor thresholds to\nmonophasic pulses in both AP and PA directions with significantly lower energy\nloss, particularly in the AP direction. Moreover, there was a significant MEP\nlatency difference of (1.79 +/- 0.41) ms between AP and PA direction with OUR\npulses, which suggests directional selectivity. Our framework successfully\nidentified highly energy-efficient asymmetric pulses for\ndirectionally-selective neural engagement. These pulses can enable selective\nrapid-rate repetitive TMS protocols with reduced power consumption and coil\nheating, with potential benefits for precision and potency of neuro-modulation."}
{"id": "2510.10300", "pdf": "https://arxiv.org/pdf/2510.10300", "abs": "https://arxiv.org/abs/2510.10300", "authors": ["Giulio Ruffini"], "title": "The algorithmic regulator", "categories": ["cs.CC", "cs.AI", "cs.IT", "cs.SY", "eess.SY", "math.IT", "q-bio.NC"], "comment": "2 Figures", "summary": "The regulator theorem states that, under certain conditions, any optimal\ncontroller must embody a model of the system it regulates, grounding the idea\nthat controllers embed, explicitly or implicitly, internal models of the\ncontrolled. This principle underpins neuroscience and predictive brain theories\nlike the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However,\nthe theorem is only proven in limited settings. Here, we treat the\ndeterministic, closed, coupled world-regulator system $(W,R)$ as a single\nself-delimiting program $p$ via a constant-size wrapper that produces the world\noutput string~$x$ fed to the regulator. We analyze regulation from the\nviewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to\nbe a \\emph{good algorithmic regulator} if it \\emph{reduces} the algorithmic\ncomplexity of the readout relative to a null (unregulated) baseline\n$\\varnothing$, i.e., \\[ \\Delta = K\\big(O_{W,\\varnothing}\\big) -\nK\\big(O_{W,R}\\big) > 0. \\] We then prove that the larger $\\Delta$ is, the more\nworld-regulator pairs with high mutual algorithmic information are favored.\nMore precisely, a complexity gap $\\Delta > 0$ yields \\[ \\Pr\\big((W,R)\\mid\nx\\big) \\le C\\,2^{\\,M(W{:}R)}\\,2^{-\\Delta}, \\] making low $M(W{:}R)$\nexponentially unlikely as $\\Delta$ grows. This is an AIT version of the idea\nthat ``the regulator contains a model of the world.'' The framework is\ndistribution-free, applies to individual sequences, and complements the\nInternal Model Principle. Beyond this necessity claim, the same coding-theorem\ncalculus singles out a \\emph{canonical scalar objective} and implicates a\n\\emph{planner}. On the realized episode, a regulator behaves \\emph{as if} it\nminimized the conditional description length of the readout."}
{"id": "2510.10586", "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers."}
{"id": "2510.11162", "pdf": "https://arxiv.org/pdf/2510.11162", "abs": "https://arxiv.org/abs/2510.11162", "authors": ["Roman A. Kononov", "Nikita A. Pospelov", "Konstantin V. Anokhin", "Vladimir V. Nekorkin", "Oleg V. Maslennikov"], "title": "Emergence of hybrid computational dynamics through reinforcement learning", "categories": ["cs.LG", "cs.NE", "nlin.AO", "q-bio.NC"], "comment": "22 pages, 11 figures", "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems."}
