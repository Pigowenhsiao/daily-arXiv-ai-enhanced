{"id": "2506.08023", "pdf": "https://arxiv.org/pdf/2506.08023", "abs": "https://arxiv.org/abs/2506.08023", "authors": ["Qifeng Wu", "Zhengzhe Liu", "Han Zhu", "Yizhou Zhao", "Daisuke Kihara", "Min Xu"], "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR\n  2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and\n  Opportunities(MMFM-BIOMED)", "summary": "This paper aims to retrieve proteins with similar structures and semantics\nfrom large-scale protein dataset, facilitating the functional interpretation of\nprotein structures derived by structural determination methods like\ncryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of\nvision-language models (VLMs), we propose a CLIP-style framework for aligning\n3D protein structures with functional annotations using contrastive learning.\nFor model training, we propose a large-scale dataset of approximately 200,000\nprotein-caption pairs with rich functional descriptors. We evaluate our model\nin both in-domain and more challenging cross-database retrieval on Protein Data\nBank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In\nboth cases, our approach demonstrates promising zero-shot retrieval\nperformance, highlighting the potential of multimodal foundation models for\nstructure-function understanding in protein biology."}
{"id": "2506.08293", "pdf": "https://arxiv.org/pdf/2506.08293", "abs": "https://arxiv.org/abs/2506.08293", "authors": ["Logan Hallee", "Nikolaos Rafailidis", "David B. Bichara", "Jason P. Gleghorn"], "title": "Diffusion Sequence Models for Enhanced Protein Representation and Generation", "categories": ["q-bio.BM"], "comment": "20 pages, 15 figures", "summary": "Proteins are fundamental to biology, executing diverse functions through\ncomplex physicochemical interactions, and they hold transformative potential\nacross medicine, materials science, and environmental applications. Protein\nLanguage Models (pLMs) aim to unlock insights from the vast space of unlabeled\nprotein sequences by learning rich, semantic representations from primary\nsequences via masked language modeling. However, these models typically exhibit\nlimited generative capacity. In this work, we introduce the Diffusion Sequence\nModel (DSM), a novel pLM trained with masked diffusion to enable both\nhigh-quality representation learning and generative protein design. DSM builds\nupon the ESM2 architecture by incorporating a masked forward diffusion process\ninspired by the LLaDA framework. After training, DSM is capable of generating\ndiverse, biomimetic sequences that align with expected amino acid compositions,\nsecondary structures, and predicted functions, even with 90\\% token corruption.\nFurthermore, DSM's learned representations match or exceed those of similarly\nsized pLMs on downstream tasks. We also introduce DSM(ppi), a variant\nfine-tuned to generate protein binders by attending to target sequences. We\ndemonstrate DSM(ppi)'s effectiveness on the challenging Bench-tested Binder\nBenchmark (BenchBB), where both DSM and DSM(ppi) produce candidates with\nsuperior predicted binding affinity compared to known binders. Our results\nestablish masked diffusion as a powerful paradigm for unifying protein\nrepresentation and generation in a single framework."}
{"id": "2506.08365", "pdf": "https://arxiv.org/pdf/2506.08365", "abs": "https://arxiv.org/abs/2506.08365", "authors": ["Cheng Tan", "Zhenxiao Cao", "Zhangyang Gao", "Siyuan Li", "Yufei Huang", "Stan Z. Li"], "title": "AlphaFold Database Debiasing for Robust Inverse Folding", "categories": ["cs.LG", "q-bio.BM"], "comment": "Under review", "summary": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding."}
{"id": "2506.08304", "pdf": "https://arxiv.org/pdf/2506.08304", "abs": "https://arxiv.org/abs/2506.08304", "authors": ["Davi Arrais Nobre", "Karen C. Abbott", "Jonathan Machta", "Alan Hastings"], "title": "Long-range dispersal promotes spatial synchrony but reduces the length and time scales of synchronous fluctuations", "categories": ["q-bio.PE", "cond-mat.stat-mech"], "comment": null, "summary": "Synchronous oscillations of spatially disjunct populations are widely\nobserved in ecology. Even in the absence of spatially synchronized exogenous\nforces, metapopulations may synchronize via dispersal. For many species, most\ndispersal is local, but rare long-distance dispersal events also occur. While\neven small amounts of long-range dispersal are known to be important for\nprocesses like invasion and spatial spread rates, their potential influence on\npopulation synchrony is often overlooked, since local dispersal on its own can\nbe strongly synchronizing. In this work, we investigate the effect of random,\nrare, long-range dispersal on the spatial synchrony of a metapopulation and\nfind profound effects not only on synchrony but also on properties of the\nresulting spatial patterns. While controlling for the overall amount of\nemigration from each local subpopulation, we vary the fraction of dispersal\nthat occurs locally (to nearest neighbors) versus globally (to random\nlocations, irrespective of distance). Using a metric that measures the\ninstantaneous level of global synchrony, we show that this form of long-range\ndispersal significantly favors the spatially synchronous state and homogenizes\nthe population by decreasing the size of clusters of subpopulations that are\nout of phase with the rest of the metapopulation. Moreover, the addition of\nnon-local dispersal significantly decreases the equilibration time of the\nmetapopulation."}
{"id": "2506.08059", "pdf": "https://arxiv.org/pdf/2506.08059", "abs": "https://arxiv.org/abs/2506.08059", "authors": ["Huong Van Le", "Weibin Ren", "Junhong Kim", "Yukyung Yun", "Young Bin Park", "Young Jun Kim", "Bok Kyung Han", "Inho Choi", "Jong IL Park", "Hwi-Yeol Yun", "Jae-Mun Choi"], "title": "CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "49 pages, 11 figures", "summary": "Caco-2 permeability serves as a critical in vitro indicator for predicting\nthe oral absorption of drug candidates during early-stage drug discovery. To\nenhance the accuracy and efficiency of computational predictions, we\nsystematically investigated the impact of eight molecular feature\nrepresentation types including 2D/3D descriptors, structural fingerprints, and\ndeep learning-based embeddings combined with automated machine learning\ntechniques to predict Caco-2 permeability. Using two datasets of differing\nscale and diversity (TDC benchmark and curated OCHEM data), we assessed model\nperformance across representations and identified PaDEL, Mordred, and RDKit\ndescriptors as particularly effective for Caco-2 prediction. Notably, the\nAutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,\nfor both PaDEL and Mordred representations, the incorporation of 3D descriptors\nresulted in a 15.73% reduction in MAE compared to using 2D features alone, as\nconfirmed by feature importance analysis. These findings highlight the\neffectiveness of AutoML approaches in ADMET modeling and offer practical\nguidance for feature selection in data-limited prediction tasks."}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos]."}
{"id": "2506.08614", "pdf": "https://arxiv.org/pdf/2506.08614", "abs": "https://arxiv.org/abs/2506.08614", "authors": ["Mareike Fischer", "Tom Niklas Hamann", "Kristina Wicke"], "title": "Metaconcepts of rooted tree balance", "categories": ["q-bio.PE", "math.CO"], "comment": null, "summary": "Measures of tree balance play an important role in many different research\nareas such as mathematical phylogenetics or theoretical computer science.\nTypically, tree balance is quantified by a single number which is assigned to\nthe tree by a balance or imbalance index, of which several exist in the\nliterature. Most of these indices are based on structural aspects of tree\nshape, such as clade sizes or leaf depths. For instance, indices like the\nSackin index, total cophenetic index, and $\\widehat{s}$-shape statistic all\nquantify tree balance through clade sizes, albeit with different definitions\nand properties.\n  In this paper, we formalize the idea that many tree (im)balance indices are\nfunctions of similar underlying tree shape characteristics by introducing\nmetaconcepts of tree balance. A metaconcept is a function $\\Phi_f$ that depends\non a function $f$ capturing some aspect of tree shape, such as balance values,\nclade sizes, or leaf depths. These metaconcepts encompass existing indices but\nalso provide new means of measuring tree balance. The versatility and\ngenerality of metaconcepts allow for the systematic study of entire families of\n(im)balance indices, providing deeper insights that extend beyond\nindex-by-index analysis."}
{"id": "2506.08954", "pdf": "https://arxiv.org/pdf/2506.08954", "abs": "https://arxiv.org/abs/2506.08954", "authors": ["Ruben Weitzman", "Peter Mørch Groth", "Lood Van Niekerk", "Aoi Otani", "Yarin Gal", "Debora Marks", "Pascal Notin"], "title": "Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction", "categories": ["q-bio.QM", "cs.LG", "q-bio.BM"], "comment": "Accepted at ICML 2025", "summary": "Retrieving homologous protein sequences is essential for a broad range of\nprotein modeling tasks such as fitness prediction, protein design, structure\nmodeling, and protein-protein interactions. Traditional workflows have relied\non a two-step process: first retrieving homologs via Multiple Sequence\nAlignments (MSA), then training models on one or more of these alignments.\nHowever, MSA-based retrieval is computationally expensive, struggles with\nhighly divergent sequences or complex insertions & deletions patterns, and\noperates independently of the downstream modeling objective. We introduce\nProtriever, an end-to-end differentiable framework that learns to retrieve\nrelevant homologs while simultaneously training for the target task. When\napplied to protein fitness prediction, Protriever achieves state-of-the-art\nperformance compared to sequence-based models that rely on MSA-based homolog\nretrieval, while being two orders of magnitude faster through efficient vector\nsearch. Protriever is both architecture- and task-agnostic, and can flexibly\nadapt to different retrieval strategies and protein databases at inference time\n-- offering a scalable alternative to alignment-centric approaches."}
{"id": "2506.08511", "pdf": "https://arxiv.org/pdf/2506.08511", "abs": "https://arxiv.org/abs/2506.08511", "authors": ["Nikola Kölbl", "Konstantin Tziridis", "Andreas Maier", "Thomas Kinfe", "Ricardo Chavarriaga", "Achim Schilling", "Patrick Krauss"], "title": "The Predictive Brain: Neural Correlates of Word Expectancy Align with Large Language Model Prediction Probabilities", "categories": ["q-bio.NC"], "comment": null, "summary": "Predictive coding theory suggests that the brain continuously anticipates\nupcoming words to optimize language processing, but the neural mechanisms\nremain unclear, particularly in naturalistic speech. Here, we simultaneously\nrecorded EEG and MEG data from 29 participants while they listened to an audio\nbook and assigned predictability scores to nouns using the BERT language model.\nOur results show that higher predictability is associated with reduced neural\nresponses during word recognition, as reflected in lower N400 amplitudes, and\nwith increased anticipatory activity before word onset. EEG data revealed\nincreased pre-activation in left fronto-temporal regions, while MEG showed a\ntendency for greater sensorimotor engagement in response to low-predictability\nwords, suggesting a possible motor-related component to linguistic\nanticipation. These findings provide new evidence that the brain dynamically\nintegrates top-down predictions with bottom-up sensory input to facilitate\nlanguage comprehension. To our knowledge, this is the first study to\ndemonstrate these effects using naturalistic speech stimuli, bridging\ncomputational language models with neurophysiological data. Our findings\nprovide novel insights for cognitive computational neuroscience, advancing the\nunderstanding of predictive processing in language and inspiring the\ndevelopment of neuroscience-inspired AI. Future research should explore the\nrole of prediction and sensory precision in shaping neural responses and\nfurther refine models of language processing."}
{"id": "2506.08058", "pdf": "https://arxiv.org/pdf/2506.08058", "abs": "https://arxiv.org/abs/2506.08058", "authors": ["Wenpo Yao"], "title": "Fuzzy permutation time irreversibility for nonequilibrium analysis of complex system", "categories": ["physics.data-an", "cond-mat.stat-mech", "q-bio.QM"], "comment": "13 pages, 5 figures", "summary": "Permutation time irreversibility is an important method to quantify\nnonequilibrium characteristics of complex systems; however, ordinal pattern is\na coarse-graining alternative of temporal structure and cannot accurately\nrepresent detailed structural information. This study aims to propose a fuzzy\npermutation time irreversibility (fpTIR) by measuring the difference between\nvector elements based on a negative exponential function. The amplitude\npermutation of vector is constructed and its membership degree is calculated;\nthen, the difference in probability distribution between the forward and\nbackward sequences is measured for fpTIR. To compare and measure the system's\ncomplexity, the Shannon entropy is calculated as the average amount of\ninformation in the fuzzy permutation probability distribution, i.e., fuzzy\npermutation entropy (fPEn). According to the surrogate theory, mode series are\ngenerated using logistic, Henon, and first-order autoregressive systems to\nverify the fpTIR, which is then used to analyze the heartbeats of patients with\ncongestive heart failure and healthy elderly and young participants from the\nPhysioNet database. Results suggest that the fpTIR effectively measures the\nsystem's nonequilibrium characteristics, thus improving the accuracy of\nheartbeat analysis. However, in analyzing probability distributions, the fpTIR\nand fPEn exhibit discrepancies in the chaotic series and even opposite results\nin the heartbeats, wherein the results of fpTIR are consistent with the theory\nof complexity loss in aging and disease. Overall, the fpTIR accurately\ncharacterizes the structure of the sequences and enhances the accuracy of the\nnonequilibrium analysis of complex systems, providing a theoretical basis for\nexploring complex systems from the perspectives of nonequilibrium dynamics and\nentropy complexity."}
{"id": "2506.08599", "pdf": "https://arxiv.org/pdf/2506.08599", "abs": "https://arxiv.org/abs/2506.08599", "authors": ["Nicolas Hinrichs", "Mahault Albarracin", "Dimitris Bolis", "Yuyue Jiang", "Leonardo Christov-Moore", "Leonhard Schilbach"], "title": "Geometric Hyperscanning under Active Inference", "categories": ["q-bio.NC"], "comment": "12 pages excl. references, 2 figures, submitted to the 6th\n  International Workshop on Active Inference", "summary": "Second-person neuroscience holds social cognition as embodied meaning\nco-regulation through reciprocal interaction, modeled here as coupled active\ninference with affect emerging as inference over identity-relevant surprise.\nEach agent maintains a self-model that tracks violations in its predictive\ncoherence while recursively modeling the other. Valence is computed from\nself-model prediction error, weighted by self-relevance, and modulated by prior\naffective states and by what we term temporal aiming, which captures affective\nappraisal over time. This accommodates shifts in the self-other boundary,\nallowing affect to emerge at individual and dyadic levels. We propose a novel\nmethod termed geometric hyperscanning, based on the Forman-Ricci curvature, to\nempirically operationalize these processes: it tracks topological\nreconfigurations in inter-brain networks, with its entro-py serving as a proxy\nfor affective phase transitions such as rupture, co-regulation, and\nre-attunement."}
{"id": "2506.08916", "pdf": "https://arxiv.org/pdf/2506.08916", "abs": "https://arxiv.org/abs/2506.08916", "authors": ["Maria-Veronica Ciocanel", "John T. Nardini", "Kevin B. Flores", "Erica M. Rutter", "Suzanne S. Sindi", "Alexandria Volkening"], "title": "Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)", "categories": ["cs.LG", "math.DS", "q-bio.QM"], "comment": "31 pages, 10 figures", "summary": "Agent-based modeling (ABM) is a powerful tool for understanding\nself-organizing biological systems, but it is computationally intensive and\noften not analytically tractable. Equation learning (EQL) methods can derive\ncontinuum models from ABM data, but they typically require extensive\nsimulations for each parameter set, raising concerns about generalizability. In\nthis work, we extend EQL to Multi-experiment equation learning (ME-EQL) by\nintroducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns\nindividual models for each parameter set and connects them via interpolation,\nand embedded structure ME-EQL (ES ME-EQL), which builds a unified model library\nacross parameters. We demonstrate these methods using a birth--death mean-field\nmodel and an on-lattice agent-based model of birth, death, and migration with\nspatial structure. Our results show that both methods significantly reduce the\nrelative error in recovering parameters from agent-based simulations, with OAT\nME-EQL offering better generalizability across parameter space. Our findings\nhighlight the potential of equation learning from multiple experiments to\nenhance the generalizability and interpretability of learned models for complex\nbiological systems."}
{"id": "2506.08138", "pdf": "https://arxiv.org/pdf/2506.08138", "abs": "https://arxiv.org/abs/2506.08138", "authors": ["William Gebhardt", "Alexander G. Ororbia", "Nathan McDonald", "Clare Thiem", "Jack Lombardi"], "title": "A Practical Guide to Tuning Spiking Neuronal Dynamics", "categories": ["cs.NE", "q-bio.NC"], "comment": null, "summary": "In this work, we examine fundamental elements of spiking neural networks\n(SNNs) as well as how to tune them. Concretely, we focus on two different\nfoundational neuronal units utilized in SNNs -- the leaky integrate-and-fire\n(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how\nhyperparameter values affect behavior. Beyond hyperparameters, we discuss other\nimportant design elements of SNNs -- the choice of input encoding and the setup\nfor excitatory-inhibitory populations -- and how these impact LIF and RAF\ndynamics."}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184", "abs": "https://arxiv.org/abs/2506.08184", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."}
{"id": "2506.08517", "pdf": "https://arxiv.org/pdf/2506.08517", "abs": "https://arxiv.org/abs/2506.08517", "authors": ["Mayar Elfares", "Salma Younis", "Pascal Reisert", "Ralf Küsters", "Tobias Renner", "Andreas Bulling"], "title": "Guidelines for Gaze-based Neural Preliminary Diagnosis", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Neural disorders refer to any condition affecting the nervous system and that\ninfluence how individuals perceive and interact with the world. Traditional\nneural diagnoses rely on cumbersome, time-consuming, or subjective methods,\nsuch as clinical interviews, behavioural observations, or medical imaging. Eye\ntracking is an attractive alternative because analysing eye movements, such as\nfixations and saccades, can provide more objective insights into brain function\nand cognitive processing by capturing non-verbal and unconscious responses.\nDespite its potential, existing gaze-based studies presented seemingly\ncontradictory findings. They are dispersed across diverse fields, requiring\nfurther research to standardise protocols and expand their application,\nparticularly as a preliminary indicator of neural processes for differential\ndiagnosis. Therefore, this paper outlines the main agreed-upon findings and\nprovides a systematisation of knowledge and key guidelines towards advancing\ngaze-based neural preliminary diagnosis."}
{"id": "2506.08583", "pdf": "https://arxiv.org/pdf/2506.08583", "abs": "https://arxiv.org/abs/2506.08583", "authors": ["Nicolas R. Chevalier", "Alexis Peaucelle", "Thomas Guilbert", "Pierre Bourdoncle", "Wang Xi"], "title": "The enteric nervous system is 10 times stiffer than the brain", "categories": ["physics.bio-ph", "q-bio.NC"], "comment": null, "summary": "Neural tissues of the central nervous system are among the softest and most\nfragile in the human body, protected from mechanical perturbation by the skull\nand the spine. In contrast, the enteric nervous system is embedded in a\ncompliant, contractile tissue and subject to chronic, high-magnitude mechanical\nstress. Do neurons and glia of the enteric nervous system display specific\nmechanical properties to withstand these forces? Using nano-indentation\ncombined with immunohistochemistry and second harmonic generation imaging of\ncollagen, we discovered that enteric ganglia in adult mice are an order of\nmagnitude more resistant to deformation than brain tissue. We found that\nglia-rich regions in ganglia have a similar stiffness to neuron-rich regions\nand to the surrounding smooth muscle, of ~3 kPa at 3 $\\mu$m indentation depth\nand of ~7 kPa at 8 $\\mu$m depth. Differences in the adhesion strength of the\ndifferent tissue layers to the glass indenter were scarce. The collagen shell\nsurrounding ganglia and inter-ganglionic fibers may play a key role in\nstrengthening the enteric nervous system to resist the manifold mechanical\nchallenges it faces."}
