{"id": "2512.24192", "pdf": "https://arxiv.org/pdf/2512.24192", "abs": "https://arxiv.org/abs/2512.24192", "authors": ["Wei Qu", "Yiming Ma", "Fei Ye", "Chan Lu", "Yi Zhou", "Kexin Zhang", "Lan Wang", "Minrui Gui", "Quanquan Gu"], "title": "SeedProteo: Accurate De Novo All-Atom Design of Protein Binders", "categories": ["q-bio.BM"], "comment": null, "summary": "We present SeedProteo, a diffusion-based model for de novo all-atom protein design. We demonstrate how to repurpose a cutting-edge folding architecture into a powerful generative design framework by effectively integrating self-conditioning features. Extensive benchmarks highlight the model's capabilities across two distinct tasks: in unconditional generation, SeedProteo exhibits superior length generalization and structural diversity, maintaining robustness for long sequences and complex topologies; in binder design, it achieves state-of-the-art performance among open-source methods, attaining the highest in-silico design success rates, structural diversity and novelty."}
{"id": "2512.24354", "pdf": "https://arxiv.org/pdf/2512.24354", "abs": "https://arxiv.org/abs/2512.24354", "authors": ["Yi Zhou", "Chan Lu", "Yiming Ma", "Wei Qu", "Fei Ye", "Kexin Zhang", "Lan Wang", "Minrui Gui", "Quanquan Gu"], "title": "SeedFold: Scaling Biomolecular Structure Prediction", "categories": ["q-bio.BM"], "comment": null, "summary": "Highly accurate biomolecular structure prediction is a key component of developing biomolecular foundation models, and one of the most critical aspects of building foundation models is identifying the recipes for scaling the model. In this work, we present SeedFold, a folding model that successfully scales up the model capacity. Our contributions are threefold: first, we identify an effective width-scaling strategy for the Pairformer to increase representation capacity; second, we introduce a novel linear triangular attention that reduces computational complexity to enable efficient scaling; finally, we construct a large-scale distillation dataset to substantially enlarge the training set. Experiments on FoldBench show that SeedFold outperforms AlphaFold3 on most protein-related tasks."}
{"id": "2512.24930", "pdf": "https://arxiv.org/pdf/2512.24930", "abs": "https://arxiv.org/abs/2512.24930", "authors": ["John Marangola", "Azadeh Sheikholeslami", "José Bento"], "title": "Constraints on the perfect phylogeny mixture model and their effect on reducing degeneracy", "categories": ["q-bio.PE", "stat.OT"], "comment": null, "summary": "The perfect phylogeny mixture (PPM) model is useful due to its simplicity and applicability in scenarios where mutations can be assumed to accumulate monotonically over time. It is the underlying model in many tools that have been used, for example, to infer phylogenetic trees for tumor evolution and reconstruction. Unfortunately, the PPM model gives rise to substantial ambiguity -- in that many different phylogenetic trees can explain the same observed data -- even in the idealized setting where data are observed perfectly, i.e. fully and without noise. This ambiguity has been studied in this perfect setting by Pradhan et al. 2018, which proposed a procedure to bound the number of solutions given a fixed instance of observation data. Beyond this, studies have been primarily empirical. Recent work (Myers et al. 2019) proposed adding extra constraints to the PPM model to tackle ambiguity. In this paper, we first show that the extra constraints of Myers et al. 2019, called longitudinal constraints (LC), often fail to reduce the number of distinct trees that explain the observations. We then propose novel alternative constraints to limit solution ambiguity and study their impact when the data are observed perfectly. Unlike the analysis in Pradhan et al. 2018, our theoretical results regarding both the inefficacy of the LC and the extent to which our new constrains reduce ambiguity are not tied to a single observation instance. Rather, our theorems hold over large ensembles of possible inference problems. To the best of our knowledge, we are the first to study degeneracy in the PPM model in this ensemble-based theoretical framework."}
{"id": "2512.23728", "pdf": "https://arxiv.org/pdf/2512.23728", "abs": "https://arxiv.org/abs/2512.23728", "authors": ["Sepideh Adamiat", "Wouter M. Kouw", "Bert de Vries"], "title": "Spike-Timing-Dependent Plasticity for Bernoulli Message Passing", "categories": ["q-bio.NC", "cs.LG", "cs.NE"], "comment": null, "summary": "Bayesian inference provides a principled framework for understanding brain function, while neural activity in the brain is inherently spike-based. This paper bridges these two perspectives by designing spiking neural networks that simulate Bayesian inference through message passing for Bernoulli messages. To train the networks, we employ spike-timing-dependent plasticity, a biologically plausible mechanism for synaptic plasticity which is based on the Hebbian rule. Our results demonstrate that the network's performance closely matches the true numerical solution. We further demonstrate the versatility of our approach by implementing a factor graph example from coding theory, illustrating signal transmission over an unreliable channel."}
{"id": "2512.23754", "pdf": "https://arxiv.org/pdf/2512.23754", "abs": "https://arxiv.org/abs/2512.23754", "authors": ["Eran Agmon", "Ryan K Spangler"], "title": "Process Bigraphs and the Architecture of Compositional Systems Biology", "categories": ["q-bio.QM"], "comment": "16 pages, 7 figures, Supplementary Materials available", "summary": "Building multiscale biological models requires integrating independently developed submodels, which involves sharing variables and coordinating execution. Most existing tools focus on isolated mechanisms and numerical methods, but rarely specify model interfaces: which variables are read or written, how they are translated, or how updates are synchronized. We present Process Bigraphs, a framework for composing and simulating multiscale biological models. Process Bigraphs generalize architectural principles from the Vivarium software into a shared specification that defines process interfaces, hierarchical data structures, composition patterns, and orchestration patterns. The paper describes the organization of the framework and explains how it improves model clarity, reuse, and extensibility; formal definitions are provided in the Supplementary Materials. We introduce Vivarium 2.0 as an open-source implementation of the Process Bigraph framework and demonstrate its utility with Spatio-Flux, a standalone library for microbial ecosystem simulations that integrate kinetic ODEs, dynamic flux balance analysis, and spatial processes. We conclude by discussing implications for emerging standards in multiscale modeling. Availability and implementation: Vivarium 2.0 is an open-source suite of libraries including: (1) bigraph-schema for hierarchical, JSON-based data typing; (2) process-bigraph for defining process interfaces and executing composite simulations; (3) bigraph-viz for interactive visualization of system structure and data flow; and (4) spatio-flux, the reference application used in this work. Detailed descriptions are provided in the Supplementary Materials. All software is available at https://github.com/vivarium-collective"}
{"id": "2512.24779", "pdf": "https://arxiv.org/pdf/2512.24779", "abs": "https://arxiv.org/abs/2512.24779", "authors": ["Jan Lukas Igelbrink", "Charline Smadi", "Anton Wakolbinger"], "title": "The tournament ratchet's clicktime process, and metastability in a Moran model", "categories": ["math.PR", "q-bio.PE"], "comment": null, "summary": "Muller's ratchet, in its prototype version, models a haploid, asexual population whose size~$N$ is constant over the generations. Slightly deleterious mutations are acquired along the lineages at a constant rate, and individuals carrying less mutations have a selective advantage. In the classical variant, an individual's selective advantage is proportional to the difference between the population average and the individual's mutation load, whereas in the ratchet with {\\em tournament selection} only the signs of the differences of the individual mutation loads matter. In a parameter regime which leads to slow clicking (i.e. to a loss of the currently fittest class at a rate $\\ll 1/N$) we prove that the rescaled process of click times of the tournament ratchet converges as $N\\to \\infty$ to a Poisson process. Central ingredients in the proof are a thorough analysis of the metastable behaviour of a two-type Moran model with selection and deleterious mutation (which describes the size of the fittest class up to its extinction time) and a lower estimate on the size of the new fittest class at a clicktime."}
{"id": "2512.24977", "pdf": "https://arxiv.org/pdf/2512.24977", "abs": "https://arxiv.org/abs/2512.24977", "authors": ["Barna Zajzon", "Younes Bouhadjar", "Maxime Fabre", "Felix Schmidt", "Noah Ostendorf", "Emre Neftci", "Abigail Morrison", "Renato Duarte"], "title": "SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Sequential structure is a key feature of multiple domains of natural cognition and behavior, such as language, movement and decision-making. Likewise, it is also a central property of tasks to which we would like to apply artificial intelligence. It is therefore of great importance to develop frameworks that allow us to evaluate sequence learning and processing in a domain agnostic fashion, whilst simultaneously providing a link to formal theories of computation and computability. To address this need, we introduce two complementary software tools: SymSeq, designed to rigorously generate and analyze structured symbolic sequences, and SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks to evaluate the performance of artificial learning systems in cognitively relevant domains. In combination, SymSeqBench offers versatility in investigating sequential structure across diverse knowledge domains, including experimental psycholinguistics, cognitive psychology, behavioral analysis, neuromorphic computing and artificial intelligence. Due to its basis in Formal Language Theory (FLT), SymSeqBench provides researchers in multiple domains with a convenient and practical way to apply the concepts of FLT to conceptualize and standardize their experiments, thus advancing our understanding of cognition and behavior through shared computational frameworks and formalisms. The tool is modular, openly available and accessible to the research community."}
{"id": "2512.23861", "pdf": "https://arxiv.org/pdf/2512.23861", "abs": "https://arxiv.org/abs/2512.23861", "authors": ["Sayeh Rezaee", "Cesar Nieto", "Abhyudai Singh"], "title": "Stochastic multi-step cell size homeostasis model for cycling human cells", "categories": ["q-bio.QM", "q-bio.CB"], "comment": null, "summary": "Measurements of cell size dynamics have established the adder principle as a robust mechanism of cell size homeostasis. In this framework, cells add a nearly constant amount of size during each cell cycle, independent of their size at birth. Theoretical studies have shown that the adder principle can be achieved when cell-cycle progression is coupled to cell size. Here, we extend this framework by considering a general growth law modeled as a Hill-type function of cell size. This assumption introduces growth saturation to the model, such that very large cells grow approximately linearly rather than exponentially. Additionally, to capture the sequential nature of division, we implement a stochastic multi-step adder model in which cells progress through internal regulatory stages before dividing. From this model, we derive exact analytical expressions for the moments of cell size distributions. Our results show that stronger growth saturation increases the mean cell size in steady state, while slightly reducing fluctuations compared to exponential growth. Importantly, despite these changes, the adder property is preserved. This emphasizes that the reduction in size variability is a consequence of~the growth law rather than simple scaling with mean size. Finally, we analyze stochastic clonal proliferation and find that growth saturation influences both single-cell size statistics and variability across populations. Our results provide a generalized framework for connecting multi-step adder mechanisms with proliferation dynamics, extending size control theory beyond exponential growth."}
{"id": "2512.23922", "pdf": "https://arxiv.org/pdf/2512.23922", "abs": "https://arxiv.org/abs/2512.23922", "authors": ["Luca Falorsi", "Gianni V. Vinci", "Maurizio Mattia"], "title": "Non-stationary dynamics of interspike intervals in neuronal populations", "categories": ["cond-mat.dis-nn", "q-bio.NC"], "comment": "10 pages, 4 figures", "summary": "We study the joint dynamics of membrane potential and time since the last spike in a population of integrate-and-fire neurons using a population density framework. This leads to a two-dimensional Fokker-Planck equation that captures the evolution of the full neuronal state, along with a one-dimensional hierarchy of equations for the moments of the inter-spike interval (ISI). The formalism allows us to characterize the time-dependent ISI distribution, even when the population is far from stationarity, such as under time-varying external input or during network oscillations. By performing a perturbative expansion around the stationary state, we also derive an analytic expression for the linear response of the ISI distribution to weak input modulations."}
{"id": "2512.23877", "pdf": "https://arxiv.org/pdf/2512.23877", "abs": "https://arxiv.org/abs/2512.23877", "authors": ["Vincent Beguin", "Jean Grétillat", "Kornelija Kaminskaitė", "Simonas Juzenas", "Dainius Kirsnauskas", "Pierre-Yves Burgi", "Samuel Wenger", "Valentin Remonnay", "Silvia Angeloni", "Bart van der Schoot", "Augustin Cerveaux", "Thomas Heinis", "Renaldas Raisutis", "Martin Jost", "Lukas Zemaitis", "Ignas Galminas", "Jérôme Charmet"], "title": "High-fidelity robotic PCR amplification for DNA data storage", "categories": ["q-bio.QM"], "comment": "28 pages total (manuscript: 18 pages, including 10 figures and references) (supplementary materials: 10 pages, including 9 figures)", "summary": "Polymerase chain reaction (PCR) is fundamental to molecular biology, yet conventional thermocyclers pose significant challenges for emerging applications such as DNA data storage, where full automation, contamination control, and cost-effectiveness are critical. Here, we introduce a disruptive approach that revisits the original water bath-based PCR method and integrates it with modern robotic liquid-handling technology. Our system performs amplification entirely within sealed pipette tips using automated immersion and withdrawal in a single temperature-controlled oil bath, eliminating the need for sophisticated thermal management while enabling precise temperature control across denaturation, annealing, and extension steps. We demonstrate that this approach achieves amplification efficiency and sequencing fidelity comparable to high-performance thermocyclers when applied to DNA-encoded datasets. The platform minimizes reagent waste, reduces contamination risks through complete tip isolation, and enables full sample recovery. This modular, automation-ready design provides a scalable and cost-effective solution for PCR workflows in DNA data storage, high-throughput diagnostics, and distributed laboratory settings."}
{"id": "2512.24439", "pdf": "https://arxiv.org/pdf/2512.24439", "abs": "https://arxiv.org/abs/2512.24439", "authors": ["Nimrod Sherf", "Si Tang", "Dylan Hafner", "Jonathan D. Touboul", "Xaq Pitkow", "Kevin E. Bassler", "Krešimir Josić"], "title": "Complexity and dynamics of partially symmetric random neural networks", "categories": ["physics.bio-ph", "q-bio.NC"], "comment": null, "summary": "Neural circuits exhibit structured connectivity, including an overrepresentation of reciprocal connections between neuron pairs. Despite important advances, a full understanding of how such partial symmetry in connectivity shapes neural dynamics remains elusive. Here we ask how correlations between reciprocal connections in a random, recurrent neural network affect phase-space complexity, defined as the exponential proliferation rate (with network size) of the number of fixed points that accompanies the transition to chaotic dynamics. We find a striking pattern: partial anti-symmetry strongly amplifies complexity, while partial symmetry suppresses it. These opposing trends closely track changes in other measures of dynamical behavior, such as dimensionality, Lyapunov exponents, and transient path length, supporting the view that fixed-point structure is a key determinant of network dynamics. Thus, positive reciprocal correlations favor low-dimensional, slowly varying activity, whereas negative correlations promote high-dimensional, rapidly fluctuating chaotic activity. These results yield testable predictions about the link between connection reciprocity, neural dynamics and function."}
{"id": "2512.24654", "pdf": "https://arxiv.org/pdf/2512.24654", "abs": "https://arxiv.org/abs/2512.24654", "authors": ["Ye Ma", "Shixin Lin", "Shengxing Fu", "Yuwei Liu", "Chenyi Guo", "Dongwei Liu", "Meijin Hou"], "title": "Muscle Synergy Patterns During Running: Coordinative Mechanisms From a Neuromechanical Perspective", "categories": ["q-bio.QM", "q-bio.NC"], "comment": "18 pages, 1 figure, 2 tables, Chinese", "summary": "Running is a fundamental form of human locomotion and a key task for evaluating neuromuscular control and lower-limb coordination. In recent years, muscle synergy analysis based on surface electromyography (sEMG) has become an important approach in this area. This review focuses on muscle synergies during running, outlining core neural control theories and biomechanical optimization hypotheses, summarizing commonly used decomposition methods (e.g., PCA, ICA, FA, NMF) and emerging autoencoder-based approaches. We synthesize findings on the development and evolution of running-related synergies across the lifespan, examine how running surface, speed, foot-strike pattern, fatigue, and performance level modulate synergy patterns, and describe characteristic alterations in populations with knee osteoarthritis, patellofemoral pain, and stroke. Current evidence suggests that the number and basic structure of lower-limb synergies during running are relatively stable, whereas spatial muscle weightings and motor primitives are highly plastic and sensitive to task demands, fatigue, and pathology. However, substantial methodological variability remains in EMG channel selection, preprocessing pipelines, and decomposition algorithms, and direct neurophysiological validation and translational application are still limited. Future work should prioritize standardized processing protocols, integration of multi-source neuromusculoskeletal data, nonlinear modeling, and longitudinal intervention studies to better exploit muscle synergy analysis in sports biomechanics, athletic training, and rehabilitation medicine."}
{"id": "2512.24654", "pdf": "https://arxiv.org/pdf/2512.24654", "abs": "https://arxiv.org/abs/2512.24654", "authors": ["Ye Ma", "Shixin Lin", "Shengxing Fu", "Yuwei Liu", "Chenyi Guo", "Dongwei Liu", "Meijin Hou"], "title": "Muscle Synergy Patterns During Running: Coordinative Mechanisms From a Neuromechanical Perspective", "categories": ["q-bio.QM", "q-bio.NC"], "comment": "18 pages, 1 figure, 2 tables, Chinese", "summary": "Running is a fundamental form of human locomotion and a key task for evaluating neuromuscular control and lower-limb coordination. In recent years, muscle synergy analysis based on surface electromyography (sEMG) has become an important approach in this area. This review focuses on muscle synergies during running, outlining core neural control theories and biomechanical optimization hypotheses, summarizing commonly used decomposition methods (e.g., PCA, ICA, FA, NMF) and emerging autoencoder-based approaches. We synthesize findings on the development and evolution of running-related synergies across the lifespan, examine how running surface, speed, foot-strike pattern, fatigue, and performance level modulate synergy patterns, and describe characteristic alterations in populations with knee osteoarthritis, patellofemoral pain, and stroke. Current evidence suggests that the number and basic structure of lower-limb synergies during running are relatively stable, whereas spatial muscle weightings and motor primitives are highly plastic and sensitive to task demands, fatigue, and pathology. However, substantial methodological variability remains in EMG channel selection, preprocessing pipelines, and decomposition algorithms, and direct neurophysiological validation and translational application are still limited. Future work should prioritize standardized processing protocols, integration of multi-source neuromusculoskeletal data, nonlinear modeling, and longitudinal intervention studies to better exploit muscle synergy analysis in sports biomechanics, athletic training, and rehabilitation medicine."}
{"id": "2512.24843", "pdf": "https://arxiv.org/pdf/2512.24843", "abs": "https://arxiv.org/abs/2512.24843", "authors": ["Alexandra Suvorikova", "Alexey Kroshnin", "Dmirijs Lvovs", "Vera Mukhina", "Andrey Mironov", "Elana J. Fertig", "Ludmila Danilova", "Alexander Favorov"], "title": "friends.test: rank-based method for feature selection in interaction matrices", "categories": ["q-bio.QM"], "comment": "12 pages, 3 figures. The first two listed authors contributed equally to this work", "summary": "The analysis of the interaction matrix between two distinct sets is essential across diverse fields, from pharmacovigilance to transcriptomics. Not all interactions are equally informative: a marker gene associated with a few specific biological processes is more informative than a highly expressed non-specific gene associated with most observed processes. Identifying these interactions is challenging due to background connections. Furthermore, data heterogeneity across sources precludes universal identification criteria.\n  To address this challenge, we introduce \\textsf{friends.test}, a method for identifying specificity by detecting structural breaks in entity interactions. Rank-based representation of the interaction matrix ensures invariance to heterogeneous data and allows for integrating data from diverse sources. To automatically locate the boundary between specific interactions and background activity, we employ model fitting. We demonstrate the applicability of \\textsf{friends.test} on the GSE112026 -- transnational data from head and neck cancer. A computationally efficient \\textsf{R} implementation is available at https://github.com/favorov/friends.test."}
{"id": "2512.24969", "pdf": "https://arxiv.org/pdf/2512.24969", "abs": "https://arxiv.org/abs/2512.24969", "authors": ["Colin Scheibner", "Lindsay M. Smith", "William Bialek"], "title": "Large language models and the entropy of English", "categories": ["cond-mat.stat-mech", "cs.CL", "physics.bio-ph", "q-bio.NC"], "comment": "8 pages, 6 figures", "summary": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself."}
{"id": "2512.24945", "pdf": "https://arxiv.org/pdf/2512.24945", "abs": "https://arxiv.org/abs/2512.24945", "authors": ["Eduardo D. Sontag"], "title": "Dynamic response phenotypes and model discrimination in systems and synthetic biology", "categories": ["q-bio.QM", "math.OC"], "comment": null, "summary": "Biological systems encode function not primarily in steady states, but in the structure of transient responses elicited by time-varying stimuli. Overshoots, biphasic dynamics, adaptation kinetics, fold-change detection, entrainment, and cumulative exposure effects often determine phenotypic outcomes, yet are poorly captured by classical steady-state or dose-response analyses. This paper develops an input-output perspective on such \"dynamic phenotypes,\" emphasizing how qualitative features of transient behavior constrain underlying network architectures independently of detailed parameter values. A central theme is the role of sign structure and interconnection logic, particularly the contrast between monotone systems and architectures containing antagonistic pathways. We show how incoherent feedforward (IFF) motifs provide a simple and recurrent mechanism for generating non-monotonic and adaptive responses across multiple levels of biological organization, from molecular signaling to immune regulation and population dynamics. Conversely, monotonicity imposes sharp impossibility results that can be used to falsify entire classes of models from transient data alone. Beyond step inputs, we highlight how periodic forcing, ramps, and integral-type readouts such as cumulative dose responses offer powerful experimental probes that reveal otherwise hidden structure, separate competing motifs, and expose invariances such as fold-change detection. Throughout, we illustrate how control-theoretic concepts, including monotonicity, equivariance, and input-output analysis, can be used not as engineering metaphors, but as precise mathematical tools for biological model discrimination. Thus we argue for a shift in emphasis from asymptotic behavior to transient and input-driven dynamics as a primary lens for understanding, testing, and reverse-engineering biological networks."}
{"id": "2512.24401", "pdf": "https://arxiv.org/pdf/2512.24401", "abs": "https://arxiv.org/abs/2512.24401", "authors": ["Shani Martinez-Weissberg", "Will Pazner", "Zohar Yosibash"], "title": "Finite element analysis of very large bone models based on micro-CT scans", "categories": ["physics.med-ph", "math.NA", "q-bio.QM"], "comment": "23 pages, 21 figures", "summary": "High-resolution voxel-based micro-finite element ($μ$FE) models derived from $μ$CT imaging enable detailed investigation of bone mechanics but remain computationally challenging at anatomically relevant scales. This study presents a comprehensive $μ$FE framework for large-scale biomechanical analysis of an intact New Zealand White (NZW) rabbit femur, integrating advanced segmentation, scalable finite element solvers, and experimental validation using predominantly open-source libraries. Bone geometries were segmented from $μ$CT data using the MIA clustering algorithm and converted into voxel-based $μ$FE meshes, which were solved using the open-source MFEM library with algorithms designed for large-scale linear elasticity systems.\n  The numerical solutions were verified by comparing with a commercial finite element solver, and by evaluating the performance of full assembly and element-by-element formulations within MFEM. Models containing over $8\\times10^{8}$ DOFs were solved using moderate HPC resources, demonstrating the feasibility of anatomically realistic $μ$FE simulations at this scale. Resolution effects were investigated by comparing models with voxel sizes of 20, 40, and 80 $μ$m, revealing that 40 $μ$m preserves boundary displacement and principal strain distributions with minimal bias while significantly reducing computational cost. Sensitivity analyses further showed that segmentation parameters influence the global mechanical response.\n  Finally, $μ$FE predictions were coupled with Digital Image Correlation measurements on an NZW rabbit femur under compression to calibrate effective bone material properties at the micron scale. The results demonstrate that large-scale, experimentally informed $μ$FE modeling can be achieved using open-source tools, providing a robust foundation for preclinical assessment of bone mechanics and treatment-related risks."}
{"id": "2512.24427", "pdf": "https://arxiv.org/pdf/2512.24427", "abs": "https://arxiv.org/abs/2512.24427", "authors": ["Sascha H. Hauck", "Sandip Saha", "Narsis A. Kiani", "Jesper N. Tegner"], "title": "Epigenetic Control and Reprogramming-Induced Potential Landscapes of Gene Regulatory Networks: A Quantitative Theoretical Approach", "categories": ["q-bio.MN", "nlin.AO", "nlin.CD", "physics.bio-ph", "q-bio.QM"], "comment": "18 pages, 7 figures", "summary": "We develop an extended Dynamical Mean Field Theory framework to analyze gene regulatory networks (GRNs) incorporating epigenetic modifications. Building on the Hopfield network model analogy to spin glass systems, our approach introduces dynamic terms representing DNA methylation and histone modification to capture their regulatory influence on gene expression. The resulting formulation reduces high-dimensional GRN dynamics to effective stochastic equations, enabling the characterization of both stable and oscillatory states in epigenetically regulated systems. This framework provides a tractable and quantitative method for linking gene regulatory dynamics with epigenetic control, offering new theoretical insights into developmental processes and cell fate decisions."}
