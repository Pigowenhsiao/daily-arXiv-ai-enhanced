{"id": "2512.02030", "pdf": "https://arxiv.org/pdf/2512.02030", "abs": "https://arxiv.org/abs/2512.02030", "authors": ["Hao Qian", "Pu You", "Lin Zeng", "Jingyuan Zhou", "Dengdeng Huang", "Kaicheng Li", "Shikui Tu", "Lei Xu"], "title": "Generative design and validation of therapeutic peptides for glioblastoma based on a potential target ATP5A", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Glioblastoma (GBM) remains the most aggressive tumor, urgently requiring novel therapeutic strategies. Here, we present a dry-to-wet framework combining generative modeling and experimental validation to optimize peptides targeting ATP5A, a potential peptide-binding protein for GBM. Our framework introduces the first lead-conditioned generative model, which focuses exploration on geometrically relevant regions around lead peptides and mitigates the combinatorial complexity of de novo methods. Specifically, we propose POTFlow, a \\underline{P}rior and \\underline{O}ptimal \\underline{T}ransport-based \\underline{Flow}-matching model for peptide optimization. POTFlow employs secondary structure information (e.g., helix, sheet, loop) as geometric constraints, which are further refined by optimal transport to produce shorter flow paths. With this design, our method achieves state-of-the-art performance compared with five popular approaches. When applied to GBM, our method generates peptides that selectively inhibit cell viability and significantly prolong survival in a patient-derived xenograft (PDX) model. As the first lead peptide-conditioned flow matching model, POTFlow holds strong potential as a generalizable framework for therapeutic peptide design."}
{"id": "2512.02033", "pdf": "https://arxiv.org/pdf/2512.02033", "abs": "https://arxiv.org/abs/2512.02033", "authors": ["Zijun Gao", "Mutian He", "Shijia Sun", "Hanqun Cao", "Jingjie Zhang", "Zihao Luo", "Xiaorui Wang", "Xiaojun Yao", "Chang-Yu Hsieh", "Chunbin Gu", "Pheng Ann Heng"], "title": "CONFIDE: Hallucination Assessment for Reliable Biomolecular Structure Prediction and Design", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Reliable evaluation of protein structure predictions remains challenging, as metrics like pLDDT capture energetic stability but often miss subtle errors such as atomic clashes or conformational traps reflecting topological frustration within the protein folding energy landscape. We present CODE (Chain of Diffusion Embeddings), a self evaluating metric empirically found to quantify topological frustration directly from the latent diffusion embeddings of the AlphaFold3 series of structure predictors in a fully unsupervised manner. Integrating this with pLDDT, we propose CONFIDE, a unified evaluation framework that combines energetic and topological perspectives to improve the reliability of AlphaFold3 and related models. CODE strongly correlates with protein folding rates driven by topological frustration, achieving a correlation of 0.82 compared to pLDDT's 0.33 (a relative improvement of 148\\%). CONFIDE significantly enhances the reliability of quality evaluation in molecular glue structure prediction benchmarks, achieving a Spearman correlation of 0.73 with RMSD, compared to pLDDT's correlation of 0.42, a relative improvement of 73.8\\%. Beyond quality assessment, our approach applies to diverse drug design tasks, including all-atom binder design, enzymatic active site mapping, mutation induced binding affinity prediction, nucleic acid aptamer screening, and flexible protein modeling. By combining data driven embeddings with theoretical insight, CODE and CONFIDE outperform existing metrics across a wide range of biomolecular systems, offering robust and versatile tools to refine structure predictions, advance structural biology, and accelerate drug discovery."}
{"id": "2512.02315", "pdf": "https://arxiv.org/pdf/2512.02315", "abs": "https://arxiv.org/abs/2512.02315", "authors": ["Felix Teufel", "Aaron W. Kollasch", "Yining Huang", "Ole Winther", "Kevin K. Yang", "Pascal Notin", "Debora S. Marks"], "title": "Few-shot Protein Fitness Prediction via In-context Learning and Test-time Training", "categories": ["q-bio.BM", "cs.LG"], "comment": "AI for Science Workshop (NeurIPS 2025)", "summary": "Accurately predicting protein fitness with minimal experimental data is a persistent challenge in protein engineering. We introduce PRIMO (PRotein In-context Mutation Oracle), a transformer-based framework that leverages in-context learning and test-time training to adapt rapidly to new proteins and assays without large task-specific datasets. By encoding sequence information, auxiliary zero-shot predictions, and sparse experimental labels from many assays as a unified token set in a pre-training masked-language modeling paradigm, PRIMO learns to prioritize promising variants through a preference-based loss function. Across diverse protein families and properties-including both substitution and indel mutations-PRIMO outperforms zero-shot and fully supervised baselines. This work underscores the power of combining large-scale pre-training with efficient test-time adaptation to tackle challenging protein design tasks where data collection is expensive and label availability is limited."}
{"id": "2512.02031", "pdf": "https://arxiv.org/pdf/2512.02031", "abs": "https://arxiv.org/abs/2512.02031", "authors": ["Omar Mahmood", "Pedro O. Pinheiro", "Richard Bonneau", "Saeed Saremi", "Vishnu Sresht"], "title": "Pharmacophore-based design by learning on voxel grids", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce \\emph{VoxCap}, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: \\emph{de-novo} design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse \\textit{de-novo} hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force."}
{"id": "2512.02478", "pdf": "https://arxiv.org/pdf/2512.02478", "abs": "https://arxiv.org/abs/2512.02478", "authors": ["Thomas P. Steele", "David J. Warne"], "title": "Simulation and inference methods for non-Markovian stochastic biochemical reaction networks", "categories": ["q-bio.MN", "math.NA", "stat.CO"], "comment": null, "summary": "Stochastic models of biochemical reaction networks are widely used to capture intrinsic noise in cellular systems. The typical formulation of these models are based on Markov processes for which there is extensive research on efficient simulation and inference. However, there are biological processes, such as gene transcription and translation, that introduce history dependent dynamics requiring non-Markovian processes to accurately capture the stochastic dynamics of the system. This greater realism comes with additional computational challenges for simulation and parameter inference. We develop efficient stochastic simulation algorithms for well-mixed non-Markovian stochastic biochemical reaction networks with delays that depend on system state and time. Our methods generalize the next reaction method and $τ$-leaping method to support arbitrary inter-event time distributions while preserving computational scalability. We also introduce a coupling scheme to generate exact non-Markovian sample paths that are positively correlated to an approximate non-Markovian $τ$-leaping sample path. This enables substantial computational gains for Bayesian inference of model parameters though multifidelity simulation-based inference schemes. We demonstrate the effectiveness of our approach on a gene regulation model with delayed auto-inhibition, showing substantial gains in both simulation accuracy and inference efficiency of two orders of magnitude. These results extend the practical applicability of non-Markovian models in systems biology and beyond."}
{"id": "2512.02204", "pdf": "https://arxiv.org/pdf/2512.02204", "abs": "https://arxiv.org/abs/2512.02204", "authors": ["Johannes Harth-Kitzerow", "Ulrich Gerland", "Torsten A. Enßlin"], "title": "MoRSAIK: Sequence Motif Reactor Simulation, Analysis and Inference Kit in Python", "categories": ["physics.bio-ph", "q-bio.BM", "q-bio.MN", "q-bio.PE"], "comment": "5 pages, 1 figure", "summary": "Origins of life research investigates how life could emerge from prebiotic chemistry only. One possible explanation provides the RNA world hypothesis. It states that life could emerge from RNA strands only, storing and transferring biological information, as well as catalyzing reactions as ribozymes. Before this state could have emerged, however, the prebiotic world was probably a purely chemical pool of short RNA strands with random sequences and without biological function performing hybridization and dehybridization, as well as ligation and cleavage. In this context relevant questions are what are the conditions that allow longer RNA strands to be built and how can information carrying in RNA sequence emerge?\n  In order to investigate such RNA reactors, efficient simulations are needed because the space of possible RNA sequences increases exponentially with the length of the strands, as well as the number of reactions between two strands. In addition, simulations have to be compared to experimental data for validation and parameter calibration. Here, we present the MoRSAIK python package for sequence motif (or k-mer) reactor simulation, analysis and inference. It enables users to simulate RNA sequence motif dynamics in the mean field approximation as well as to infer the reaction parameters from data with Bayesian methods and to analyze results by computing observables and plotting. MoRSAIK simulates an RNA reactor by following the reactions and the concentrations of all strands inside up to a certain length (of four nucleotides by default). Longer strands are followed indirectly, by tracking the concentrations of their containing sequence motifs of that maximum length."}
{"id": "2512.02032", "pdf": "https://arxiv.org/pdf/2512.02032", "abs": "https://arxiv.org/abs/2512.02032", "authors": ["Gaurav Rudravaram", "Lianrui Zuo", "Adam M. Saunders", "Michael E. Kim", "Praitayini Kanakaraj", "Nancy R. Newlin", "Aravind R. Krishnan", "Elyssa M. McMaster", "Chloe Cho", "Susan M. Resnick", "Lori L. Beason Held", "Derek Archer", "Timothy J. Hohman", "Daniel C. Moyer", "Bennett A. Landman"], "title": "Characterizing Continuous and Discrete Hybrid Latent Spaces for Structural Connectomes", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "Structural connectomes are detailed graphs that map how different brain regions are physically connected, offering critical insight into aging, cognition, and neurodegenerative diseases. However, these connectomes are high-dimensional and densely interconnected, which makes them difficult to interpret and analyze at scale. While low-dimensional spaces like PCA and autoencoders are often used to capture major sources of variation, their latent spaces are generally continuous and cannot fully reflect the mixed nature of variability in connectomes, which include both continuous (e.g., connectivity strength) and discrete factors (e.g., imaging site). Motivated by this, we propose a variational autoencoder (VAE) with a hybrid latent space that jointly models the discrete and continuous components. We analyze a large dataset of 5,761 connectomes from six Alzheimer's disease studies with ten acquisition protocols. Each connectome represents a single scan from a unique subject (3579 females, 2182 males), aged 22 to 102, with 4338 cognitively normal, 809 with mild cognitive impairment (MCI), and 614 with Alzheimer's disease (AD). Each connectome contains 121 brain regions defined by the BrainCOLOR atlas. We train our hybrid VAE in an unsupervised way and characterize what each latent component captures. We find that the discrete space is particularly effective at capturing subtle site-related differences, achieving an Adjusted Rand Index (ARI) of 0.65 with site labels, significantly outperforming PCA and a standard VAE followed by clustering (p < 0.05). These results demonstrate that the hybrid latent space can disentangle distinct sources of variability in connectomes in an unsupervised manner, offering potential for large-scale connectome analysis."}
{"id": "2512.02089", "pdf": "https://arxiv.org/pdf/2512.02089", "abs": "https://arxiv.org/abs/2512.02089", "authors": ["Shelby Lacouture", "Mitchell Kelley", "Noah Plues", "Laszlo Hunyadi", "Emily Sundman", "Annette Sobel", "Robert V. Duncan"], "title": "A Compact, Data-Logging Breath-Gas Analyzer", "categories": ["q-bio.QM"], "comment": null, "summary": "Respiratory ailments are increasing globally at an alarming rate and are currently one of the leading factors of death and infirmity worldwide. Among respiratory diseases, those linked to poor air quality and pollutants are increasing at a proportionally higher rate than those linked to viral or other factors. Diagnosing disorders of the respiratory system is often performed initially by routine physical examinations and questionnaires. Once most patients have symptoms that are severe enough to warrant clinical testing, the ailment could have already caused pulmonary damage. Clinical diagnosis involves the use of cumbersome, expensive equipment that measures different parameters separately, e.g., Capnography (CO2) and spirometry (bidirectional tidal mass flow). These disparate sets of data must then be interpreted collectively by a qualified medical practitioner. This paper details the design of a portable, inexpensive, mixed-signal data-logging system that measures a chosen set of parameters in exhaled breath from humans or animals. The data is a comprehensive set of pertinent gases and mass flow that when looked at simultaneously, gives a synergistic view of these interrelated breathing biomarkers and thus the state of the respiratory system as a whole. A mask-mounted, tabletop, and handheld version was developed for different applications. The system, when fully developed, would enable a new set of clinical vitals that only require a patient to breathe through a single, small device for a few moments. This new set of clinical vitals could enable the early diagnosis of many respiratory ailments, something that could have a large positive impact on disease prognosis and quality of life."}
{"id": "2512.02204", "pdf": "https://arxiv.org/pdf/2512.02204", "abs": "https://arxiv.org/abs/2512.02204", "authors": ["Johannes Harth-Kitzerow", "Ulrich Gerland", "Torsten A. Enßlin"], "title": "MoRSAIK: Sequence Motif Reactor Simulation, Analysis and Inference Kit in Python", "categories": ["physics.bio-ph", "q-bio.BM", "q-bio.MN", "q-bio.PE"], "comment": "5 pages, 1 figure", "summary": "Origins of life research investigates how life could emerge from prebiotic chemistry only. One possible explanation provides the RNA world hypothesis. It states that life could emerge from RNA strands only, storing and transferring biological information, as well as catalyzing reactions as ribozymes. Before this state could have emerged, however, the prebiotic world was probably a purely chemical pool of short RNA strands with random sequences and without biological function performing hybridization and dehybridization, as well as ligation and cleavage. In this context relevant questions are what are the conditions that allow longer RNA strands to be built and how can information carrying in RNA sequence emerge?\n  In order to investigate such RNA reactors, efficient simulations are needed because the space of possible RNA sequences increases exponentially with the length of the strands, as well as the number of reactions between two strands. In addition, simulations have to be compared to experimental data for validation and parameter calibration. Here, we present the MoRSAIK python package for sequence motif (or k-mer) reactor simulation, analysis and inference. It enables users to simulate RNA sequence motif dynamics in the mean field approximation as well as to infer the reaction parameters from data with Bayesian methods and to analyze results by computing observables and plotting. MoRSAIK simulates an RNA reactor by following the reactions and the concentrations of all strands inside up to a certain length (of four nucleotides by default). Longer strands are followed indirectly, by tracking the concentrations of their containing sequence motifs of that maximum length."}
{"id": "2512.02908", "pdf": "https://arxiv.org/pdf/2512.02908", "abs": "https://arxiv.org/abs/2512.02908", "authors": ["Iryna Zabaikina", "Ramon Grima"], "title": "Imperfect molecular detection renormalizes apparent kinetic rates in stochastic gene regulatory networks", "categories": ["q-bio.MN", "q-bio.QM", "q-bio.SC"], "comment": "24 pages, 5 figures", "summary": "Imperfect molecular detection in single-cell experiments introduces technical noise that obscures the true stochastic dynamics of gene regulatory networks. While binomial models of molecular capture provide a principled description of imperfect detection, they have so far been analyzed only for simple gene-expression models that do not explicitly account for regulation. Here, we extend binomial models of capture to general gene regulatory networks to understand how imperfect capture reshapes the observed time-dependent statistics of molecular counts. Our results reveal when capture effects correspond to a renormalization of a subset of the kinetic rates and when they cannot be absorbed into effective rates, providing a systematic basis for interpreting noisy single-cell measurements. In particular, we show that rate renormalization emerges either under significant transcription factor abundance or when promoter-state transitions occur on a distinct (much slower or faster) timescale than other reactions. In these cases, technical noise causes the apparent mean burst size of synthesized gene products to appear reduced while transcription factor binding reactions appear faster. These effects hold for gene regulatory networks of arbitrary connectivity and remain valid under time-dependent kinetic rates."}
{"id": "2512.02223", "pdf": "https://arxiv.org/pdf/2512.02223", "abs": "https://arxiv.org/abs/2512.02223", "authors": ["Benjamin K. Rosenzweig", "Matthew W. Hahn"], "title": "On the Approximation of Phylogenetic Distance Functions by Artificial Neural Networks", "categories": ["cs.LG", "q-bio.PE"], "comment": "10 pages", "summary": "Inferring the phylogenetic relationships among a sample of organisms is a fundamental problem in modern biology. While distance-based hierarchical clustering algorithms achieved early success on this task, these have been supplanted by Bayesian and maximum likelihood search procedures based on complex models of molecular evolution. In this work we describe minimal neural network architectures that can approximate classic phylogenetic distance functions and the properties required to learn distances under a variety of molecular evolutionary models. In contrast to model-based inference (and recently proposed model-free convolutional and transformer networks), these architectures have a small computational footprint and are scalable to large numbers of taxa and molecular characters. The learned distance functions generalize well and, given an appropriate training dataset, achieve results comparable to state-of-the art inference methods."}
{"id": "2512.02419", "pdf": "https://arxiv.org/pdf/2512.02419", "abs": "https://arxiv.org/abs/2512.02419", "authors": ["Shogo Ohmae", "Keiko Ohmae"], "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.NE"], "comment": "22 pages, 4 figures. Related to our earlier preprint \"The brain versus AI\" (arXiv:2411.16075) but a distinct article. The earlier work surveyed broad brain-AI parallels; here we focus on world-model-based computation and convergent evolution between the brain and AI, especially large language models", "summary": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence."}
{"id": "2512.02260", "pdf": "https://arxiv.org/pdf/2512.02260", "abs": "https://arxiv.org/abs/2512.02260", "authors": ["Hammed A. Akande", "Abdulrauf A. Gidado"], "title": "EcoCast: A Spatio-Temporal Model for Continual Biodiversity and Climate Risk Forecasting", "categories": ["q-bio.QM", "stat.ML"], "comment": "9 pages, 3 figures, 1 table. Accepted to the NeurIPS 2025 Workshop on Tackling Climate Change with Machine Learning", "summary": "Increasing climate change and habitat loss are driving unprecedented shifts in species distributions. Conservation professionals urgently need timely, high-resolution predictions of biodiversity risks, especially in ecologically diverse regions like Africa. We propose EcoCast, a spatio-temporal model designed for continual biodiversity and climate risk forecasting. Utilizing multisource satellite imagery, climate data, and citizen science occurrence records, EcoCast predicts near-term (monthly to seasonal) shifts in species distributions through sequence-based transformers that model spatio-temporal environmental dependencies. The architecture is designed with support for continual learning to enable future operational deployment with new data streams. Our pilot study in Africa shows promising improvements in forecasting distributions of selected bird species compared to a Random Forest baseline, highlighting EcoCast's potential to inform targeted conservation policies. By demonstrating an end-to-end pipeline from multi-modal data ingestion to operational forecasting, EcoCast bridges the gap between cutting-edge machine learning and biodiversity management, ultimately guiding data-driven strategies for climate resilience and ecosystem conservation throughout Africa."}
{"id": "2512.02303", "pdf": "https://arxiv.org/pdf/2512.02303", "abs": "https://arxiv.org/abs/2512.02303", "authors": ["Max W. Shen", "Ewa Nowara", "Michael Maser", "Kyunghyun Cho"], "title": "Training Dynamics of Learning 3D-Rotational Equivariance", "categories": ["cs.LG", "q-bio.BM"], "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\\leq$2\\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters."}
{"id": "2512.02312", "pdf": "https://arxiv.org/pdf/2512.02312", "abs": "https://arxiv.org/abs/2512.02312", "authors": ["Gregory M Ellison", "Liang Liu"], "title": "Fast and Accurate Node-Age Estimation Under Fossil Calibration Uncertainty Using the Adjusted Pairwise Likelihood", "categories": ["q-bio.QM", "q-bio.PE"], "comment": "32 pages, 11 figures", "summary": "Estimating divergence times from molecular sequence data is central to reconstructing the evolutionary history of lineages. Although Bayesian relaxed-clock methods provide a principled framework for incorporating fossil information, their dependence on repeated evaluations of the full phylogenetic likelihood makes them computationally demanding for large genomic datasets. Furthermore, because disagreements in divergence-time estimates often arise from uncertainty or error in fossil placement and prior specification, there is a need for methods that are both computationally efficient and robust to fossil-calibration uncertainty. In this study, we introduce fast and accurate alternatives based on the phylogenetic pairwise composite likelihood, presenting two adjusted pairwise likelihood (APW) formulations that employ asymptotic moment-matching weights to better approximate the behavior of the full likelihood within a Bayesian MCMC framework. Extensive simulations across diverse fossil-calibration scenarios show that APW methods produce node-age estimates comparable to those obtained from the full likelihood while offering greater robustness to fossil misplacement and prior misspecification, due to the reduced sensitivity of composite likelihoods to local calibration errors. Applied to a genome-scale dataset of modern birds, APW methods recover divergence time patterns consistent with recent studies, while reducing computational cost by more than an order of magnitude. Overall, our results demonstrate that adjusted pairwise likelihoods provide a calibration-robust and computationally efficient framework for Bayesian node dating, especially suited for large phylogenomic datasets and analyses in which fossil priors may be uncertain or imperfectly placed."}
{"id": "2512.02503", "pdf": "https://arxiv.org/pdf/2512.02503", "abs": "https://arxiv.org/abs/2512.02503", "authors": ["Simon Leipold", "Ryssa Moffat"], "title": "Individual-specific precision neuroimaging of learning-related plasticity", "categories": ["q-bio.NC"], "comment": null, "summary": "Studying learning-related plasticity is central to understanding the acquisition of complex skills, for example learning to master a musical instrument. Over the past three decades, conventional group-based functional magnetic resonance imaging (fMRI) studies have advanced our understanding of how humans' neural representations change during skill acquisition. However, group-based fMRI studies average across heterogeneous learners and often rely on coarse pre- versus post-training comparisons, limiting the spatial and temporal precision with which neural changes can be estimated. Here, we outline an individual-specific precision approach that tracks neural changes within individuals by collecting high-quality neuroimaging data frequently over the course of training, mapping brain function in each person's own anatomical space, and gathering detailed behavioral measures of learning, allowing neural trajectories to be directly linked to individual learning progress. Complementing fMRI with mobile neuroimaging methods, such as functional near-infrared spectroscopy (fNIRS), will enable researchers to track plasticity during naturalistic practice and across extended time scales. This multi-modal approach will enhance sensitivity to individual learning trajectories and will offer more nuanced insights into how neural representations change with training. We also discuss how findings can be generalized beyond individuals, including through statistical methods based on replication in additional individuals. Together, this approach allows researchers to design highly informative longitudinal training studies that advance a mechanistic, personalized account of skill learning in the human brain."}
{"id": "2512.02312", "pdf": "https://arxiv.org/pdf/2512.02312", "abs": "https://arxiv.org/abs/2512.02312", "authors": ["Gregory M Ellison", "Liang Liu"], "title": "Fast and Accurate Node-Age Estimation Under Fossil Calibration Uncertainty Using the Adjusted Pairwise Likelihood", "categories": ["q-bio.QM", "q-bio.PE"], "comment": "32 pages, 11 figures", "summary": "Estimating divergence times from molecular sequence data is central to reconstructing the evolutionary history of lineages. Although Bayesian relaxed-clock methods provide a principled framework for incorporating fossil information, their dependence on repeated evaluations of the full phylogenetic likelihood makes them computationally demanding for large genomic datasets. Furthermore, because disagreements in divergence-time estimates often arise from uncertainty or error in fossil placement and prior specification, there is a need for methods that are both computationally efficient and robust to fossil-calibration uncertainty. In this study, we introduce fast and accurate alternatives based on the phylogenetic pairwise composite likelihood, presenting two adjusted pairwise likelihood (APW) formulations that employ asymptotic moment-matching weights to better approximate the behavior of the full likelihood within a Bayesian MCMC framework. Extensive simulations across diverse fossil-calibration scenarios show that APW methods produce node-age estimates comparable to those obtained from the full likelihood while offering greater robustness to fossil misplacement and prior misspecification, due to the reduced sensitivity of composite likelihoods to local calibration errors. Applied to a genome-scale dataset of modern birds, APW methods recover divergence time patterns consistent with recent studies, while reducing computational cost by more than an order of magnitude. Overall, our results demonstrate that adjusted pairwise likelihoods provide a calibration-robust and computationally efficient framework for Bayesian node dating, especially suited for large phylogenomic datasets and analyses in which fossil priors may be uncertain or imperfectly placed."}
{"id": "2512.02864", "pdf": "https://arxiv.org/pdf/2512.02864", "abs": "https://arxiv.org/abs/2512.02864", "authors": ["Amandine Hong-Minh", "Yair Augusto Gutiérrez Fosado", "Abbie Guild", "Nicholas Mullin", "Laura Spagnolo", "Ian Chambers", "Davide Michieletto"], "title": "Modulation of DNA rheology by a transcription factor that forms aging microgels", "categories": ["cond-mat.soft", "cond-mat.other", "physics.bio-ph", "physics.comp-ph", "q-bio.BM"], "comment": null, "summary": "Proteins and nucleic acids form non-Newtonian liquids with complex rheological properties that contribute to their function in vivo. Here we investigate the rheology of the transcription factor NANOG, a key protein in sustaining embryonic stem cell self-renewal. We discover that at high concentrations NANOG forms macroscopic aging gels through its intrinsically disordered tryptophan-rich domain. By combining molecular dynamics simulations, mass photometry and Cryo-EM, we also discover that NANOG forms self-limiting micelle-like clusters which expose their DNA-binding domains. In dense solutions of DNA, NANOG micelle-like structures stabilize intermolecular entanglements and crosslinks, forming microgel-like structures. Our findings suggest that NANOG may contribute to regulate gene expression in a unconventional way: by restricting and stabilizing genome dynamics at key transcriptional sites through the formation of an aging microgel-like structure, potentially enabling mechanical memory in the gene network."}
{"id": "2512.02671", "pdf": "https://arxiv.org/pdf/2512.02671", "abs": "https://arxiv.org/abs/2512.02671", "authors": ["D. Rebbin", "K. J. A. Down", "T. F. Varley", "R. Ince", "A. Canales-Johnson"], "title": "Translating Measures onto Mechanisms: The Cognitive Relevance of Higher-Order Information", "categories": ["q-bio.NC"], "comment": null, "summary": "Higher-order information theory has become a rapidly growing toolkit in computational neuroscience, motivated by the idea that multivariate dependencies can reveal aspects of neural computation and communication that are invisible to pairwise analyses. Yet functional interpretations of synergy and redundancy often outpace principled arguments for how statistical quantities map onto mechanistic cognitive processes. Here we review the main families of higher-order measures with the explicit goal of translating mathematical properties into defensible mechanistic inferences. First, we systematize Shannon-based multivariate metrics and demonstrate that higher-order dependence is parsimoniously characterized by two largely independent axes: interaction strength and redundancy-synergy balance. We argue that balanced layering of synergistic integration and redundant broadcasting optimizes multiscale complexity, formalizing a computation-communication tradeoff. We then examine the partial information decomposition and outline pragmatic considerations for its deployment in neural data. Equipped with the relevant mathematical essentials, we connect redundancy-synergy balance to cognitive function by progressively embedding their mathematical properties in real-world constraints, starting with small synthetic systems before gradually building up to neuroimaging. We close by identifying key future directions for mechanistic insight: cross-scale bridging, intervention-based validation, and thermodynamically grounded unification of information dynamics."}
{"id": "2512.02328", "pdf": "https://arxiv.org/pdf/2512.02328", "abs": "https://arxiv.org/abs/2512.02328", "authors": ["Jiabao Brad Wang", "Siyuan Cao", "Hongxuan Wu", "Yiliang Yuan", "Mustafa Misir"], "title": "Molecular Embedding-Based Algorithm Selection in Protein-Ligand Docking", "categories": ["q-bio.QM", "cs.LG"], "comment": "25 pages, 13 figures, 5 tables. Protein-ligand docking, algorithm selection, pretrained embeddings (ESM, ChemBERTa), docking benchmarks, oracle-landscape analysis. Code and data available", "summary": "Selecting an effective docking algorithm is highly context-dependent, and no single method performs reliably across structural, chemical, or protocol regimes. We introduce MolAS, a lightweight algorithm selection system that predicts per-algorithm performance from pretrained protein-ligand embeddings using attentional pooling and a shallow residual decoder. With only hundreds to a few thousand labelled complexes, MolAS achieves up to 15% absolute improvement over the single-best solver (SBS) and closes 17-66% of the Virtual Best Solver (VBS)-SBS gap across five diverse docking benchmarks. Analyses of reliability, embedding geometry, and solver-selection patterns show that MolAS succeeds when the oracle landscape exhibits low entropy and separable solver behaviour, but collapses under protocol-induced hierarchy shifts. These findings indicate that the main barrier to robust docking AS is not representational capacity but instability in solver rankings across pose-generation regimes, positioning MolAS as both a practical in-domain selector and a diagnostic tool for assessing when AS is feasible."}
{"id": "2512.02978", "pdf": "https://arxiv.org/pdf/2512.02978", "abs": "https://arxiv.org/abs/2512.02978", "authors": ["Paul Barbaste", "Olivier Oullier", "Xavier Vasques"], "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding", "categories": ["q-bio.NC", "cs.AI", "cs.HC", "cs.LG"], "comment": "28 pages, 8 figures, 2 tables", "summary": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique."}
{"id": "2512.02528", "pdf": "https://arxiv.org/pdf/2512.02528", "abs": "https://arxiv.org/abs/2512.02528", "authors": ["Vincent Wieland", "Nils Wassmuth", "Lorenzo Contento", "Martin Kühn", "Jan Hasenauer"], "title": "Assessment of Simulation-based Inference Methods for Stochastic Compartmental Models", "categories": ["q-bio.QM"], "comment": null, "summary": "Global pandemics, such as the recent COVID-19 crisis, highlight the need for stochastic epidemic models that can capture the randomness inherent in the spread of disease. Such models must be accompanied by methods for estimating parameters in order to generate fast nowcasts and short-term forecasts that can inform public health decisions. This paper presents a comparison of two advanced Bayesian inference methods: 1) pseudo-marginal particle Markov chain Monte Carlo, short Particle Filters (PF), and 2) Conditional Normalizing Flows (CNF). We investigate their performance on two commonly used compartmental models: a classical Susceptible-Infected-Recovered (SIR) model and a two-variant Susceptible-Exposed-Infected-Recovered (SEIR) model, complemented by an observation model that maps latent trajectories to empirical data. Addressing the challenges of intractable likelihoods for parameter inference in stochastic settings, our analysis highlights how these likelihood-free methods provide accurate and robust inference capabilities. The results of our simulation study further underscore the effectiveness of these approaches in capturing the stochastic dynamics of epidemics, providing prediction capabilities for the control of epidemic outbreaks. Results on an Ethiopian cohort study demonstrate operational robustness under real-world noise and irregular data sampling. To facilitate reuse and to enable building pipelines that ultimately contribute to better informed decision making in public health, we make code and synthetic datasets publicly available."}
{"id": "2512.02028", "pdf": "https://arxiv.org/pdf/2512.02028", "abs": "https://arxiv.org/abs/2512.02028", "authors": ["Yiping Wang", "Peiren Wang", "Zhenye Li", "Fang Liu", "Jinguo Huang"], "title": "Seizure-NGCLNet: Representation Learning of SEEG Spatial Pathological Patterns for Epileptic Seizure Detection via Node-Graph Dual Contrastive Learning", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Complex spatial connectivity patterns, such as interictal suppression and ictal propagation, complicate accurate drug-resistant epilepsy (DRE) seizure detection using stereotactic electroencephalography (SEEG) and traditional machine learning methods. Two critical challenges remain:(1)a low signal-to-noise ratio in functional connectivity estimates, making it difficult to learn seizure-related interactions; and (2)expert labels for spatial pathological connectivity patterns are difficult to obtain, meanwhile lacking the patterns' representation to improve seizure detection. To address these issues, we propose a novel node-graph dual contrastive learning framework, Seizure-NGCLNet, to learn SEEG interictal suppression and ictal propagation patterns for detecting DRE seizures with high precision. First, an adaptive graph augmentation strategy guided by centrality metrics is developed to generate seizure-related brain networks. Second, a dual-contrastive learning approach is integrated, combining global graph-level contrast with local node-graph contrast, to encode both spatial structural and semantic epileptogenic features. Third, the pretrained embeddings are fine-tuned via a top-k localized graph attention network to perform the final classification. Extensive experiments on a large-scale public SEEG dataset from 33 DRE patients demonstrate that Seizure-NGCLNet achieves state-of-the-art performance, with an average accuracy of 95.93%, sensitivity of 96.25%, and specificity of 94.12%. Visualizations confirm that the learned embeddings clearly separate ictal from interictal states, reflecting suppression and propagation patterns that correspond to the clinical mechanisms. These results highlight Seizure-NGCLNet's ability to learn interpretable spatial pathological patterns, enhancing both seizure detection and seizure onset zone localization."}
{"id": "2512.02811", "pdf": "https://arxiv.org/pdf/2512.02811", "abs": "https://arxiv.org/abs/2512.02811", "authors": ["Timo Jakumeit", "Lukas Brand", "Jens Kirchner", "Robert Schober", "Sebastian Lotter"], "title": "Vessel Network Topology in Molecular Communication: Insights from Experiments and Theory", "categories": ["q-bio.QM", "cs.ET"], "comment": "29 pages, 10 figures; This paper has been submitted to IEEE Transactions on Molecular, Biological, and Multi-Scale Communications", "summary": "The notion of synthetic molecular communication (MC) refers to the transmission of information via signaling molecules and is foreseen to enable innovative medical applications in the human cardiovascular system (CVS). Crucially, the design of such applications requires accurate and experimentally validated channel models that characterize the propagation of signaling molecules, not just in individual blood vessels, but in complex vessel networks (VNs), as prevalent in the CVS. However, experimentally validated models for MC in VNs remain scarce. To address this gap, we propose a novel channel model for MC in complex VN topologies, which captures molecular transport via advection, molecular and turbulent diffusion, as well as adsorption and desorption at the vessel walls. We specialize this model for superparamagnetic iron-oxide nanoparticles (SPIONs) as signaling molecules by introducing a new receiver (RX) model for planar coil inductive sensors, enabling end-to-end experimental validation with a dedicated SPION testbed. Validation covers a range of channel topologies, from single-vessel topologies to branched VNs with multiple paths between transmitter (TX) and RX. Additionally, to quantify how the VN topology impacts signal quality, and inspired by multi-path propagation models in conventional wireless communications, we introduce two metrics, namely molecule delay and multi-path spread. We show that these metrics link the VN structure to molecule dispersion induced by the VN and mediately to the resulting signal-to-noise ratio (SNR) at the RX. The proposed VN structure-SNR link is validated experimentally, demonstrating that the proposed framework can support tasks such as optimal sensor placement in the CVS or the identification of suitable testbed topologies for specific SNR requirements. All experimental data are openly available on Zenodo."}
{"id": "2512.02719", "pdf": "https://arxiv.org/pdf/2512.02719", "abs": "https://arxiv.org/abs/2512.02719", "authors": ["Julian Ma", "Jun Wang", "Zafeirios Fountas"], "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs."}
{"id": "2512.02162", "pdf": "https://arxiv.org/pdf/2512.02162", "abs": "https://arxiv.org/abs/2512.02162", "authors": ["Rahul Mehta"], "title": "Mapping of Lesion Images to Somatic Mutations", "categories": ["cs.CV", "q-bio.QM"], "comment": "https://dl.acm.org/doi/abs/10.1145/3340531.3414074#sec-terms", "summary": "Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains."}
{"id": "2512.02908", "pdf": "https://arxiv.org/pdf/2512.02908", "abs": "https://arxiv.org/abs/2512.02908", "authors": ["Iryna Zabaikina", "Ramon Grima"], "title": "Imperfect molecular detection renormalizes apparent kinetic rates in stochastic gene regulatory networks", "categories": ["q-bio.MN", "q-bio.QM", "q-bio.SC"], "comment": "24 pages, 5 figures", "summary": "Imperfect molecular detection in single-cell experiments introduces technical noise that obscures the true stochastic dynamics of gene regulatory networks. While binomial models of molecular capture provide a principled description of imperfect detection, they have so far been analyzed only for simple gene-expression models that do not explicitly account for regulation. Here, we extend binomial models of capture to general gene regulatory networks to understand how imperfect capture reshapes the observed time-dependent statistics of molecular counts. Our results reveal when capture effects correspond to a renormalization of a subset of the kinetic rates and when they cannot be absorbed into effective rates, providing a systematic basis for interpreting noisy single-cell measurements. In particular, we show that rate renormalization emerges either under significant transcription factor abundance or when promoter-state transitions occur on a distinct (much slower or faster) timescale than other reactions. In these cases, technical noise causes the apparent mean burst size of synthesized gene products to appear reduced while transcription factor binding reactions appear faster. These effects hold for gene regulatory networks of arbitrary connectivity and remain valid under time-dependent kinetic rates."}
