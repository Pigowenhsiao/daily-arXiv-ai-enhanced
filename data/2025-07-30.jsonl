{"id": "2507.21063", "pdf": "https://arxiv.org/pdf/2507.21063", "abs": "https://arxiv.org/abs/2507.21063", "authors": ["Mingxuan Gao", "Jingjing Chen", "Yun Long", "Xiaomeng Xu", "Yu Zhang"], "title": "Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data", "categories": ["q-bio.NC", "cs.CY"], "comment": "25 pages, 6 figures", "summary": "Background: Silence is a common phenomenon in classrooms, yet its implicit\nnature limits a clear understanding of students' underlying learning statuses.\nAim: This study proposed a nuanced framework to classify classroom silence\nbased on class events and student status, and examined neurophysiological\nmarkers to reveal similarities and differences in silent states across\nachievement groups. Sample: The study involved 54 middle school students during\n34 math lessons, with simultaneous recordings of electroencephalogram (EEG),\nelectrodermal activity (EDA), and heart rate signals, alongside video coding of\nclassroom behaviors. Results: We found that high-achieving students showed no\nsignificant difference in mean EDA features between strategic silence (i.e.,\nstudents choose silence deliberately) and active speaking during open\nquestioning but exhibited higher EEG high-frequency relative power spectral\ndensity (RPSD) during strategic silence. In structural silence (i.e., students\nmaintain silence following an external command) during directed questioning,\nthey demonstrated significantly higher heart rates while listening to lectures\ncompared to group activities, indicating heightened engagement. Both high- and\nmedium-achieving students displayed elevated heart rates and EDA tonic\ncomponents in structural silence during questioning compared to teaching.\nFurthermore, high-achieving students exhibited lower high-frequency RPSD during\nstructural silence than strategic silence, a pattern not observed in other\ngroups, highlighting group heterogeneity. Conclusions: The findings contribute\nto validating the complexity of silence, challenge its traditional association\nwith passivity, and offer a novel classification framework along with\npreliminary empirical evidence to deepen the understanding of silent learning\nbehaviors in classroom contexts."}
{"id": "2507.21348", "pdf": "https://arxiv.org/pdf/2507.21348", "abs": "https://arxiv.org/abs/2507.21348", "authors": ["Izabel Valdez", "Paramahansa Pramanik"], "title": "Exploring the Interplay of Adiposity, Ethnicity, and Hormone Receptor Profiles in Breast Cancer Subtypes", "categories": ["q-bio.QM", "stat.AP"], "comment": "27 pages, 2 figures, 4 tables", "summary": "This study explores how obesity and race jointly influence the development\nand prognosis of Luminal subtypes of breast cancer, with a focus on\ndistinguishing Luminal A from the more aggressive Luminal B tumors. Drawing on\nlarge-scale epidemiological data and employing statistical approaches such as\nlogistic regression and mediation analysis, the research examines biological\nfactors like estrogen metabolism, adipokines, and chronic inflammation\nalongside social determinants including healthcare access, socioeconomic\nstatus, and cultural attitudes toward body weight. The findings reveal that\nboth obesity and racial background are significant predictors of risk for\nLuminal B breast cancers. The study highlights the need for a dual approach\nthat combines medical treatment with targeted social interventions aimed at\nreducing disparities. These insights can improve individualized risk\nassessments, guide tailored screening programs, and support policies that\naddress the heightened cancer burden experienced by marginalized communities."}
{"id": "2507.21140", "pdf": "https://arxiv.org/pdf/2507.21140", "abs": "https://arxiv.org/abs/2507.21140", "authors": ["Tatsuru Kikuchi"], "title": "Gender Similarities Dominate Mathematical Cognition at the Neural Level: A Japanese fMRI Study Using Advanced Wavelet Analysis and Generative AI", "categories": ["q-bio.NC", "econ.GN", "q-fin.EC"], "comment": "14 pages, 4 figures, submitted to Nature", "summary": "Recent large scale behavioral studies suggest early emergence of gender\ndifferences in mathematical performance within months of school entry. However,\nthese findings lack direct neural evidence and are constrained by cultural\ncontexts. We conducted functional magnetic resonance imaging (fMRI) during\nmathematical tasks in Japanese participants (N = 156), employing an advanced\nwavelet time frequency analysis to examine dynamic brain processes rather than\nstatic activation patterns. Wavelet decomposition across four frequency bands\n(0.01-0.25 Hz) revealed that neural processing mechanisms underlying\nmathematical cognition are fundamentally similar between genders. Time\nfrequency analysis demonstrated 89.1% similarity in dynamic activation patterns\n(p = 0.734, d = 0.05), with identical temporal sequences and frequency profiles\nduring mathematical processing. Individual variation in neural dynamics\nexceeded group differences by 3.2:1 (p $<$ 0.001). Machine learning classifiers\nachieved only 53.8% accuracy in distinguishing gender based neural patterns\nessentially at chance level even when analyzing sophisticated temporal spectral\nfeatures. Cross frequency coupling analysis revealed similar network\ncoordination patterns between genders, indicating shared fundamental cognitive\narchitecture. These findings provide robust process level neural evidence that\ngender similarities dominate mathematical cognition, particularly in early\ndevelopmental stages, challenging recent claims of inherent differences and\ndemonstrating that dynamic brain analysis reveals neural mechanisms that static\nbehavioral assessments cannot access."}
{"id": "2507.21260", "pdf": "https://arxiv.org/pdf/2507.21260", "abs": "https://arxiv.org/abs/2507.21260", "authors": ["Amartya Banerjee", "Xingyu Xu", "Caroline MoosmÃ¼ller", "Harlin Lee"], "title": "Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "Code: https://github.com/amartya21/Adam-PnP", "summary": "In an inverse problem, the goal is to recover an unknown parameter (e.g., an\nimage) that has typically undergone some lossy or noisy transformation during\nmeasurement. Recently, deep generative models, particularly diffusion models,\nhave emerged as powerful priors for protein structure generation. However,\nintegrating noisy experimental data from multiple sources to guide these models\nremains a significant challenge. Existing methods often require precise\nknowledge of experimental noise levels and manually tuned weights for each data\nmodality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that\nguides a pre-trained protein diffusion model using gradients from multiple,\nheterogeneous experimental sources. Our framework features an adaptive noise\nestimation scheme and a dynamic modality weighting mechanism integrated into\nthe diffusion process, which reduce the need for manual hyperparameter tuning.\nExperiments on complex reconstruction tasks demonstrate significantly improved\naccuracy using Adam-PnP."}
{"id": "2507.21871", "pdf": "https://arxiv.org/pdf/2507.21871", "abs": "https://arxiv.org/abs/2507.21871", "authors": ["Katerina Marie Simkova", "Adrien Doerig", "Clayton Hickey", "Ian Charest"], "title": "Representations in vision and language converge in a shared, multidimensional space of perceived similarities", "categories": ["q-bio.NC", "cs.LG"], "comment": "51 pages, 15 figures", "summary": "Humans can effortlessly describe what they see, yet establishing a shared\nrepresentational format between vision and language remains a significant\nchallenge. Emerging evidence suggests that human brain representations in both\nvision and language are well predicted by semantic feature spaces obtained from\nlarge language models (LLMs). This raises the possibility that sensory systems\nconverge in their inherent ability to transform their inputs onto shared,\nembedding-like representational space. However, it remains unclear how such a\nspace manifests in human behaviour. To investigate this, sixty-three\nparticipants performed behavioural similarity judgements separately on 100\nnatural scene images and 100 corresponding sentence captions from the Natural\nScenes Dataset. We found that visual and linguistic similarity judgements not\nonly converge at the behavioural level but also predict a remarkably similar\nnetwork of fMRI brain responses evoked by viewing the natural scene images.\nFurthermore, computational models trained to map images onto LLM-embeddings\noutperformed both category-trained and AlexNet controls in explaining the\nbehavioural similarity structure. These findings demonstrate that human visual\nand linguistic similarity judgements are grounded in a shared,\nmodality-agnostic representational structure that mirrors how the visual system\nencodes experience. The convergence between sensory and artificial systems\nsuggests a common capacity of how conceptual representations are formed-not as\narbitrary products of first order, modality-specific input, but as structured\nrepresentations that reflect the stable, relational properties of the external\nworld."}
{"id": "2507.21404", "pdf": "https://arxiv.org/pdf/2507.21404", "abs": "https://arxiv.org/abs/2507.21404", "authors": ["Amber Huang", "Ian Scott Knight", "Slava Naprienko"], "title": "Data Leakage and Redundancy in the LIT-PCBA Benchmark", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "LIT-PCBA is a widely used benchmark for virtual screening, but our audit\nreveals it is fundamentally compromised. The dataset suffers from egregious\ndata leakage, rampant duplication, and pervasive analog redundancy -- flaws\nthat invalidate its use for fair model evaluation. Notably, we identify 2,491\ninactives duplicated across training and validation sets, and thousands more\nrepeated within individual data splits (2,945 in training, 789 in validation).\nCritically, three ligands in the query set -- meant to represent unseen test\ncases -- are leaked: two appear in the training set, one in validation.\nStructural redundancy compounds these issues: for some targets, over 80% of\nquery ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1\nalone, we find 323 highly similar active pairs between training and validation\nsets, invalidating claims of chemical diversity. These and other flaws\ncollectively cause models trained on LIT-PCBA to memorize rather than\ngeneralize. To demonstrate the consequences of these data integrity failures,\nwe implement a trivial memorization-based baseline -- using no learning, no\nphysics, and no modeling -- that outperforms state-of-the-art models, including\ndeep neural networks like CHEESE, on LIT-PCBA simply by exploiting these\nartifacts. Our findings render the benchmark unfit for its intended purpose and\ncall into question previous results based on its use. We share this audit to\nraise awareness and provide tooling to help the community develop more rigorous\nand reliable datasets going forward. All scripts necessary to reproduce our\naudit and the baseline implementation are available at:\nhttps://github.com/sievestack/LIT-PCBA-audit"}
{"id": "2507.21474", "pdf": "https://arxiv.org/pdf/2507.21474", "abs": "https://arxiv.org/abs/2507.21474", "authors": ["Daniel Szelogowski"], "title": "Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning", "categories": ["cs.NE", "cs.AI", "cs.IR", "cs.LG", "q-bio.NC", "I.2.0; I.2.4; I.2.6; I.2.m; E.1; E.2; E.4; H.3; J.3; J.4"], "comment": "20 pages, 11 figures, 4 tables", "summary": "Despite success across diverse tasks, current artificial recurrent network\narchitectures rely primarily on implicit hidden-state memories, limiting their\ninterpretability and ability to model long-range dependencies. In contrast,\nbiological neural systems employ explicit, associative memory traces (i.e.,\nengrams) strengthened through Hebbian synaptic plasticity and activated\nsparsely during recall. Motivated by these neurobiological insights, we\nintroduce the Engram Neural Network (ENN), a novel recurrent architecture\nincorporating an explicit, differentiable memory matrix with Hebbian plasticity\nand sparse, attention-driven retrieval mechanisms. The ENN explicitly models\nmemory formation and recall through dynamic Hebbian traces, improving\ntransparency and interpretability compared to conventional RNN variants. We\nevaluate the ENN architecture on three canonical benchmarks: MNIST digit\nclassification, CIFAR-10 image sequence modeling, and WikiText-103 language\nmodeling. Our empirical results demonstrate that the ENN achieves accuracy and\ngeneralization performance broadly comparable to classical RNN, GRU, and LSTM\narchitectures, with all models converging to similar accuracy and perplexity on\nthe large-scale WikiText-103 task. At the same time, the ENN offers significant\nenhancements in interpretability through observable memory dynamics. Hebbian\ntrace visualizations further reveal biologically plausible, structured memory\nformation processes, validating the potential of neuroscience-inspired\nmechanisms to inform the development of more interpretable and robust deep\nlearning models."}
{"id": "2507.21149", "pdf": "https://arxiv.org/pdf/2507.21149", "abs": "https://arxiv.org/abs/2507.21149", "authors": ["Keyvan Alavi"], "title": "A Mini Review on Tumor Organoid-on-a-Chip Technologies in Personalized Oncology", "categories": ["q-bio.TO"], "comment": "16 pages, 1 figure, 2 tables", "summary": "Tumor organoid-on-a-chip platforms represent a cutting-edge fusion of\npatient-derived organoids with microfluidic technologies, offering\nunprecedented capabilities for personalized cancer research. These systems\novercome limitations of conventional models by enabling precise control over\nthe tumor microenvironment, including nutrient gradients, fluid flow, and\nimmune interactions. Tumor organoids recapitulate patient-specific tumor\nheterogeneity and genetic landscapes, while microfluidic chips provide dynamic\nperfusion and mechanical stimuli, enhancing physiological relevance. Together,\nthey facilitate advanced applications such as high-throughput drug screening,\nimmunotherapy testing, and metastasis modeling, showing superior predictive\npower for clinical outcomes. Despite challenges in standardization,\nscalability, and integration of complex tumor components, ongoing advances in\nhydrogel engineering, automation, and artificial intelligence are poised to\naccelerate their clinical translation. This review highlights current\ntechnologies, applications, and future directions of tumor organoid-on-a-chip\nsystems, emphasizing their transformative potential in precision oncology."}
{"id": "2507.21417", "pdf": "https://arxiv.org/pdf/2507.21417", "abs": "https://arxiv.org/abs/2507.21417", "authors": ["Xiang Liu", "Xuefei Huang", "Guo-Wei Wei"], "title": "Machine-Learning Prediction of Virus-like Particle Stoichiometry and Stability using Persistent Topological Laplacians", "categories": ["q-bio.BM"], "comment": null, "summary": "Understanding the stoichiometry and associated stability of virus-like\nparticles (VLPs) is crucial for optimizing their assembly efficiency and\nimmunogenic properties, which are essential for advancing biotechnology,\nvaccine design, and drug delivery. However, current experimental methods for\ndetermining VLP stoichiometry are labor-intensive, and time consuming. Machine\nlearning approaches have hardly been applied to the study of VLPs. To address\nthis challenge, we introduce a novel persistent Laplacian-based machine\nlearning (PLML) mode that leverages both harmonic and non-harmonic spectra to\ncapture intricate topological and geometric features of VLP structures. This\napproach achieves superior performance on the VLP200 dataset compared to\nexisting methods. To further assess robustness and generalizability, we\ncollected a new dataset, VLP706, containing 706 VLP samples with expanded\nstoichiometry diversity. Our PLML model maintains strong predictive accuracy on\nVLP706. Additionally, through random sequence perturbative mutation analysis,\nwe found that 60-mers and 180-mers exhibit greater stability than 240-mers and\n420-mers."}
{"id": "2507.21938", "pdf": "https://arxiv.org/pdf/2507.21938", "abs": "https://arxiv.org/abs/2507.21938", "authors": ["Alex Abrudan", "Sebastian Pujalte Ojeda", "Chaitanya K. Joshi", "Matthew Greenig", "Felipe Engelberger", "Alena Khmelinskaia", "Jens Meiler", "Michele Vendruscolo", "Tuomas P. J. Knowles"], "title": "Multi-state Protein Design with DynamicMPNN", "categories": ["cs.LG", "q-bio.BM", "I.2.6; J.3"], "comment": "ICML 2025 GenBio Workshop", "summary": "Structural biology has long been dominated by the one sequence, one\nstructure, one function paradigm, yet many critical biological processes - from\nenzyme catalysis to membrane transport - depend on proteins that adopt multiple\nconformational states. Existing multi-state design approaches rely on post-hoc\naggregation of single-state predictions, achieving poor experimental success\nrates compared to single-state design. We introduce DynamicMPNN, an inverse\nfolding model explicitly trained to generate sequences compatible with multiple\nconformations through joint learning across conformational ensembles. Trained\non 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated\nusing AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%\non structure-normalized RMSD across our challenging multi-state protein\nbenchmark."}
