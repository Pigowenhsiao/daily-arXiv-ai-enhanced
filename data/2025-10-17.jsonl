{"id": "2510.14455", "pdf": "https://arxiv.org/pdf/2510.14455", "abs": "https://arxiv.org/abs/2510.14455", "authors": ["Wenyu Zhu", "Chengzhu Li", "Xiaohe Tian", "Yifan Wang", "Yinjun Jia", "Jianhui Wang", "Bowen Gao", "Ya-Qin Zhang", "Wei-Ying Ma", "Yanyan Lan"], "title": "Coder as Editor: Code-driven Interpretable Molecular Optimization", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Molecular optimization is a central task in drug discovery that requires\nprecise structural reasoning and domain knowledge. While large language models\n(LLMs) have shown promise in generating high-level editing intentions in\nnatural language, they often struggle to faithfully execute these\nmodifications-particularly when operating on non-intuitive representations like\nSMILES. We introduce MECo, a framework that bridges reasoning and execution by\ntranslating editing actions into executable code. MECo reformulates molecular\noptimization for LLMs as a cascaded framework: generating human-interpretable\nediting intentions from a molecule and property goal, followed by translating\nthose intentions into executable structural edits via code generation. Our\napproach achieves over 98% accuracy in reproducing held-out realistic edits\nderived from chemical reactions and target-specific compound pairs. On\ndownstream optimization benchmarks spanning physicochemical properties and\ntarget activities, MECo substantially improves consistency by 38-86 percentage\npoints to 90%+ and achieves higher success rates over SMILES-based baselines\nwhile preserving structural similarity. By aligning intention with execution,\nMECo enables consistent, controllable and interpretable molecular design,\nlaying the foundation for high-fidelity feedback loops and collaborative\nhuman-AI workflows in drug discovery."}
{"id": "2510.13815", "pdf": "https://arxiv.org/pdf/2510.13815", "abs": "https://arxiv.org/abs/2510.13815", "authors": ["Khartik Uppalapati", "Bora Yimenicioglu", "Shakeel Abdulkareem", "Bhavya Uppalapati", "Viraj Kamath", "Adan Eftekhari", "Pranav Ayyappan"], "title": "A Two-Feature Quantitative EEG Index of Pediatric Epilepsy Severity: External Pre-Validation on CHB-MIT and Roadmap to Dravet Cohorts", "categories": ["q-bio.NC"], "comment": null, "summary": "Objective biomarkers for staging pediatric epileptic encephalopathies are\nscarce. We revisited a large open repository -- the CHB-MIT Scalp EEG Database,\n22 subjects aged 1.5-19 y recorded at 256 Hz under the 10-20 montage -- to\nderive and validate a compact quantitative index, DS-Qi =\n(theta/alpha)_posterior + (1 - wPLI_beta). The first term captures excess\nposterior slow-wave power, a recognized marker of impaired cortical maturation;\nthe second employs the debiased weighted Phase-Lag Index to measure loss of\nbeta-band synchrony, robust to volume conduction and small-sample bias. In\n30-min awake, eyes-open segments, DS-Qi was 1.69 +/- 0.21 in epilepsy versus\n1.23 +/- 0.17 in age-matched normative EEG (Cohen's d = 1.1, p < 0.001). A\nlogistic model trained with 10 x 10-fold cross-validation yielded an AUC of\n0.90 (95% CI 0.81-0.97) and optimal sensitivity/specificity of 86%/83% at DS-Qi\n= 1.46. Across multi-day recordings, test-retest reliability was ICC = 0.74,\nand higher DS-Qi correlated with greater seizure burden (rho = 0.58, p =\n0.004). These results establish DS-Qi as a reproducible, single-number summary\nof electrophysiological severity that can be computed from short scalp EEG\nsegments using only posterior and standard 10-20 electrodes."}
{"id": "2510.14282", "pdf": "https://arxiv.org/pdf/2510.14282", "abs": "https://arxiv.org/abs/2510.14282", "authors": ["Kazuya Horibe", "Daichi G. Suzuki"], "title": "Evolvable Chemotons: Toward the Integration of Autonomy and Evolution", "categories": ["q-bio.PE"], "comment": "Accepted as a late-breaking abstract in the ALIFE 2025", "summary": "In this study, we provide a relatively simple simulation framework for\nconstructing artificial life (ALife) with both autonomous and evolutionary\naspects by extending chemoton model. While the original chemoton incorporates\nmetabolism, membrane, and genetic templates, it lacks a mechanism for\nphenotypic variation, preventing true evolutionary dynamics. To address this,\nwe introduced a genotype-phenotype coupling by linking templates to a second\nautocatalytic cycle, enabling mutations to affect phenotype and be subject to\nselection. Using a genetic algorithm, we simulated populations of chemotons\nover generations. Results showed that chemotons without access to the new cycle\nremained in a stable but complexity-limited regime, while lineages acquiring\nthe additional metabolic set evolved longer templates. These findings\ndemonstrate that even simple replicator systems can achieve primitive\nevolvability, highlighting structural thresholds and rare innovations as key\ndrivers. Our framework provides a tractable model for exploring autonomy and\nevolution in ALife."}
{"id": "2510.13886", "pdf": "https://arxiv.org/pdf/2510.13886", "abs": "https://arxiv.org/abs/2510.13886", "authors": ["Pierre Fayolle", "Alexandre Bône", "Noëlie Debs", "Mathieu Naudin", "Pascal Bourdon", "Remy Guillevin", "David Helbert"], "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading", "categories": ["q-bio.QM", "cs.AI", "eess.IV", "eess.SP"], "comment": "5 pages, 5 figures, IEEE ISBI 2025, Houston, Tx, USA", "summary": "DSC-MRI perfusion is a medical imaging technique for diagnosing and\nprognosing brain tumors and strokes. Its analysis relies on mathematical\ndeconvolution, but noise or motion artifacts in a clinical environment can\ndisrupt this process, leading to incorrect estimate of perfusion parameters.\nAlthough deep learning approaches have shown promising results, their\ncalibration typically rely on third-party deconvolution algorithms to generate\nreference outputs and are bound to reproduce their limitations.\n  To adress this problem, we propose a physics-informed autoencoder that\nleverages an analytical model to decode the perfusion parameters and guide the\nlearning of the encoding network. This autoencoder is trained in a\nself-supervised fashion without any third-party software and its performance is\nevaluated on a database with glioma patients. Our method shows reliable results\nfor glioma grading in accordance with other well-known deconvolution algorithms\ndespite a lower computation time. It also achieved competitive performance even\nin the presence of high noise which is critical in a medical environment."}
{"id": "2510.13826", "pdf": "https://arxiv.org/pdf/2510.13826", "abs": "https://arxiv.org/abs/2510.13826", "authors": ["Noorbakhsh Amiri Golilarz", "Hassan S. Al Khatib", "Shahram Rahimi"], "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Artificial intelligence has advanced significantly through deep learning,\nreinforcement learning, and large language and vision models. However, these\nsystems often remain task specific, struggle to adapt to changing conditions,\nand cannot generalize in ways similar to human cognition. Additionally, they\nmainly focus on mimicking brain structures, which often leads to black-box\nmodels with limited transparency and adaptability. Inspired by the structure\nand function of biological cognition, this paper introduces the concept of\n\"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines\nneuroscience, cognitive science, computer vision, and AI to develop more\ngeneral, adaptive, and robust intelligent systems capable of rapid learning,\nlearning from less data, and leveraging prior experience. These systems aim to\nemulate the human brain's ability to flexibly learn, reason, remember,\nperceive, and act in real-world settings with minimal supervision. We review\nthe limitations of current AI methods, define core principles of\nneurocognitive-inspired intelligence, and propose a modular, biologically\ninspired architecture that emphasizes integration, embodiment, and\nadaptability. We also discuss potential implementation strategies and outline\nvarious real-world applications, from robotics to education and healthcare.\nImportantly, this paper offers a hybrid roadmap for future research, laying the\ngroundwork for building AI systems that more closely resemble human cognition."}
{"id": "2510.14481", "pdf": "https://arxiv.org/pdf/2510.14481", "abs": "https://arxiv.org/abs/2510.14481", "authors": ["Seong Jun Park"], "title": "Viral population dynamics at the cellular level, considering the replication cycle", "categories": ["q-bio.PE", "q-bio.QM"], "comment": null, "summary": "Viruses are microscopic infectious agents that require a host cell for\nreplication. Viral replication occurs in several stages, and the completion\ntime for each stage varies due to differences in the cellular environment.\nThus, the time to complete each stage in viral replication is a random\nvariable. However, no analytic expression exists for the viral population at\nthe cellular level when the completion time for each process constituting viral\nreplication is a random variable. This paper presents a simplified model of\nviral replication, treating each stage as a renewal process with independently\nand identically distributed completion times. Using the proposed model, we\nderive an analytical formula for viral populations at the cellular level, based\non viewing viral replication as a birth-death process. The mean viral count is\nexpressed via probability density functions representing the completion time\nfor each step in the replication process. This work validates the results with\nstochastic simulations. This study provides a new quantitative framework for\nunderstanding viral infection dynamics."}
{"id": "2510.13896", "pdf": "https://arxiv.org/pdf/2510.13896", "abs": "https://arxiv.org/abs/2510.13896", "authors": ["Xi Yu", "Yang Yang", "Qun Liu", "Yonghua Du", "Sean McSweeney", "Yuewei Lin"], "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.MA"], "comment": "43 pages", "summary": "Cellular image segmentation is essential for quantitative biology yet remains\ndifficult due to heterogeneous modalities, morphological variability, and\nlimited annotations. We present GenCellAgent, a training-free multi-agent\nframework that orchestrates specialist segmenters and generalist\nvision-language models via a planner-executor-evaluator loop (choose tool\n$\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The\nsystem (i) automatically routes images to the best tool, (ii) adapts on the fly\nusing a few reference images when imaging conditions differ from what a tool\nexpects, (iii) supports text-guided segmentation of organelles not covered by\nexisting models, and (iv) commits expert edits to memory, enabling\nself-evolution and personalized workflows. Across four cell-segmentation\nbenchmarks, this routing yields a 15.7\\% mean accuracy gain over\nstate-of-the-art baselines. On endoplasmic reticulum and mitochondria from new\ndatasets, GenCellAgent improves average IoU by 37.6\\% over specialist models.\nIt also segments novel objects such as the Golgi apparatus via iterative\ntext-guided refinement, with light human correction further boosting\nperformance. Together, these capabilities provide a practical path to robust,\nadaptable cellular image segmentation without retraining, while reducing\nannotation burden and matching user preferences."}
{"id": "2510.13841", "pdf": "https://arxiv.org/pdf/2510.13841", "abs": "https://arxiv.org/abs/2510.13841", "authors": ["Ashley Chen"], "title": "Hybrid Deep Learning Approaches for Classifying Autism from Brain MRI", "categories": ["q-bio.NC", "cs.LG", "68T07, 68T45", "I.2.6; I.5.4; J.3"], "comment": "25 pages, 13 figures, 4 tables, 19 references", "summary": "Autism spectrum disorder (ASD) is most often diagnosed using behavioral\nevaluations, which can vary between clinicians. Brain imaging, combined with\nmachine learning, may help identify more objective patterns linked to ASD. This\nproject used magnetic resonance imaging (MRI) data from the publicly available\nABIDE I dataset (n = 1,112) to test two approaches for classifying ASD and\ncontrol participants. The first was a 3D convolutional neural network (CNN)\ntrained end-to-end. The second was a hybrid approach that used the CNN as a\nfeature extractor and then applied a support vector machine (SVM) classifier.\nThe baseline CNN reached moderate performance (accuracy = 0.66, AUC = 0.70),\nwhile the hybrid CNN + SVM achieved higher overall accuracy (0.76) and AUC\n(0.80). The hybrid model also produced more balanced results between ASD and\ncontrol groups. Separating feature extraction and classification improved\nperformance and reduced bias between diagnostic groups. These findings suggest\nthat combining deep learning and traditional machine learning methods could\nenhance the reliability of MRI-based research on ASD."}
{"id": "2510.14917", "pdf": "https://arxiv.org/pdf/2510.14917", "abs": "https://arxiv.org/abs/2510.14917", "authors": ["Hasan Ahmed", "Deena Goodgold", "Khushali Kothari", "Rustom Antia"], "title": "Cumulants, Moments and Selection: The Connection Between Evolution and Statistics", "categories": ["q-bio.PE"], "comment": null, "summary": "Cumulants and moments are closely related to the basic mathematics of\ncontinuous and discrete selection (respectively). These relationships\ngeneralize Fisher's fundamental theorem of natural selection and also make\nclear some of its limitation. The relationship between cumulants and continuous\nselection is especially intuitive and also provides an alternative way to\nunderstand cumulants. We show that a similarly simple relationship exists\nbetween moments and discrete selection. In more complex scenarios, we show that\nthinking of selection over discrete generations has significant advantages. For\na simple mutation model, we find exact solutions for the equilibrium moments of\nthe fitness distribution. These solutions are surprisingly simple and have some\ninteresting implications including: a necessary and sufficient condition for\nmutation selection balance, a very simple formula for mean fitness and the fact\nthat the shape of the equilibrium fitness distribution is determined solely by\nmutation (whereas the scale is determined by the starting fitness\ndistribution)."}
{"id": "2510.13897", "pdf": "https://arxiv.org/pdf/2510.13897", "abs": "https://arxiv.org/abs/2510.13897", "authors": ["Naomi Fridman", "Anat Goldstein"], "title": "Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Breast cancer is the most diagnosed cancer in women, with HER2 status\ncritically guiding treatment decisions. Noninvasive prediction of HER2 status\nfrom dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and\nreduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI\ninto standardized 8-bit RGB format for pretrained neural networks is\nnontrivial, and normalization strategy significantly affects model performance.\nWe benchmarked intensity normalization strategies using a Triple-Head\nDual-Attention ResNet that processes RGB-fused temporal sequences from three\nDCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and\nexternally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed\ntransformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY\ntest data. N4 bias field correction slightly degraded performance. Without\nfine-tuning, external validation yielded 0.66 AUC, demonstrating\ncross-institutional generalizability. These findings highlight the\neffectiveness of dual-attention mechanisms in capturing transferable\nspatiotemporal features for HER2 stratification, advancing reproducible deep\nlearning biomarkers in breast cancer imaging."}
{"id": "2510.13845", "pdf": "https://arxiv.org/pdf/2510.13845", "abs": "https://arxiv.org/abs/2510.13845", "authors": ["Akila Kadambi", "Lisa Aziz-Zadeh", "Antonio Damasio", "Marco Iacoboni", "Srini Narayanan"], "title": "Embodiment in multimodal large language models", "categories": ["q-bio.NC"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated extraordinary\nprogress in bridging textual and visual inputs. However, MLLMs still face\nchallenges in situated physical and social interactions in sensorally rich,\nmultimodal and real-world settings where the embodied experience of the living\norganism is essential. We posit that next frontiers for MLLM development\nrequire incorporating both internal and external embodiment -- modeling not\nonly external interactions with the world, but also internal states and drives.\nHere, we describe mechanisms of internal and external embodiment in humans and\nrelate these to current advances in MLLMs in early stages of aligning to human\nrepresentations. Our dual-embodied framework proposes to model interactions\nbetween these forms of embodiment in MLLMs to bridge the gap between multimodal\ndata and world experience."}
{"id": "2510.14311", "pdf": "https://arxiv.org/pdf/2510.14311", "abs": "https://arxiv.org/abs/2510.14311", "authors": ["Ken-Ichi Nakamura", "Toshiko Ogiwara"], "title": "Propagation speed of traveling waves for diffusive Lotka-Volterra system with strong competition", "categories": ["math.AP", "q-bio.PE"], "comment": "15 pages, 2 figures", "summary": "We study the propagation speed of bistable traveling waves in the classical\ntwo-component diffusive Lotka-Volterra system under strong competition. From an\necological perspective, the sign of the propagation speed determines the\nlong-term outcome of competition between two species and thus plays a central\nrole in predicting the success or failure of invasion of an alien species into\nhabitats occupied by a native species. Using comparison arguments, we establish\nsufficient conditions determining the sign of the propagation speed, which\nrefine previously known results. In particular, we show that in the symmetric\ncase, where the two species differ only in their diffusion rates, the faster\ndiffuser prevails over a substantially broader parameter range than previously\nestablished. Moreover, we demonstrate that when the interspecific competition\ncoefficients differ significantly, the outcome of competition cannot be\nreversed by adjusting diffusion or growth rates. These findings provide a\nrigorous theoretical framework for analyzing invasion dynamics, offering\nsharper mathematical criteria for invasion success or failure."}
{"id": "2510.13911", "pdf": "https://arxiv.org/pdf/2510.13911", "abs": "https://arxiv.org/abs/2510.13911", "authors": ["Jia Zhang", "Bodong Du", "Yitong Miao", "Dongwei Sun", "Xiangyong Cao"], "title": "OralGPT: A Two-Stage Vision-Language Model for Oral Mucosal Disease Diagnosis and Description", "categories": ["q-bio.QM"], "comment": null, "summary": "Oral mucosal diseases such as leukoplakia, oral lichen planus, and recurrent\n  aphthous ulcers exhibit diverse and overlapping visual features,\n  making diagnosis challenging for non-specialists. While vision-language\n  models (VLMs) have shown promise in medical image interpretation,\n  their application in oral healthcare remains underexplored due to\n  the lack of large-scale, well-annotated datasets. In this work, we present\n  \\textbf{OralGPT}, the first domain-specific two-stage vision-language\n  framework designed for oral mucosal disease diagnosis and captioning.\n  In Stage 1, OralGPT learns visual representations and disease-related\n  concepts from classification labels. In Stage 2, it enhances its language\n  generation ability using long-form expert-authored captions. To\n  overcome the annotation bottleneck, we propose a novel similarity-guided\n  data augmentation strategy that propagates descriptive knowledge from\n  expert-labeled images to weakly labeled ones. We also construct the\n  first benchmark dataset for oral mucosal diseases, integrating multi-source\n  image data with both structured and unstructured textual annotations.\n  Experimental results on four common oral conditions demonstrate that\n  OralGPT achieves competitive diagnostic performance while generating\n  fluent, clinically meaningful image descriptions. This study\n  provides a foundation for language-assisted diagnostic tools in oral\n  healthcare."}
{"id": "2510.13883", "pdf": "https://arxiv.org/pdf/2510.13883", "abs": "https://arxiv.org/abs/2510.13883", "authors": ["Dinh-Nguyen Nguyen", "Sadia Shakil", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "title": "Large Language Model Agents Enable Autonomous Design and Image Analysis of Microwell Microfluidics", "categories": ["q-bio.NC", "cs.MA"], "comment": null, "summary": "Microwell microfluidics has been utilized for single-cell analysis to reveal\nheterogeneity in gene expression, signaling pathways, and phenotypic responses\nfor identifying rare cell types, understanding disease progression, and\ndeveloping more precise therapeutic strategies. However, designing microwell\nmicrofluidics is a considerably complex task, requiring knowledge, experience,\nand CAD software, as well as manual intervention, which often fails initial\ndesigns, demanding multiple costly and time-consuming iterations. In this\nstudy, we establish an autonomous large language model (LLM)-driven microwell\ndesign framework to generate code-based computer-aided design (CAD) scripts,\nthat enables the rapid and reproducible creation of microwells with diverse\ngeometries and imaging-based analysis. We propose a multimodal large language\nmodel (MLLM)-logistic regression framework based on integrating high-level\nsemantic descriptions generated by MLLMs with image embeddings for image\nclassification tasks, aiming to identify microwell occupancy and microwell\nshape. The fused multimodal representation is input to a logistic regression\nmodel, which is both interpretable and computationally efficient. We achieved\nsignificant improvements, exceeding 0.92 for occupancy classification and 0.99\nfor shape classification, across all evaluated MLLMs, compared with 0.50 and\n0.55, respectively, when relying solely on direct classification. The\nMLLM-logistic regression framework is a scalable, efficient solution for\nhigh-throughput microwell image analysis. Our study demonstrates an autonomous\ndesign microwell platform by translating natural language prompts into\noptimized device geometries, CAD scripts and image analysis, facilitating the\ndevelopment of next-generation digital discovery by integration of literature\nmining, autonomous design and experimental data analysis."}
{"id": "2510.14787", "pdf": "https://arxiv.org/pdf/2510.14787", "abs": "https://arxiv.org/abs/2510.14787", "authors": ["Lorenzo Zino", "Alessandro Casu", "Alessandro Rizzo"], "title": "A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases", "categories": ["eess.SY", "cs.SY", "math.OC", "q-bio.PE"], "comment": "To appear in the Proceedings of the 2025 European Control Conference\n  (ECC)", "summary": "We propose an epidemic model for the spread of vector-borne diseases. The\nmodel, which is built extending the classical susceptible-infected-susceptible\nmodel, accounts for two populations -- humans and vectors -- and for\ncross-contagion between the two species, whereby humans become infected upon\ninteraction with carrier vectors, and vectors become carriers after interaction\nwith infected humans. We formulate the model as a system of ordinary\ndifferential equations and leverage monotone systems theory to rigorously\ncharacterize the epidemic dynamics. Specifically, we characterize the global\nasymptotic behavior of the disease, determining conditions for quick\neradication of the disease (i.e., for which all trajectories converge to a\ndisease-free equilibrium), or convergence to a (unique) endemic equilibrium.\nThen, we incorporate two control actions: namely, vector control and incentives\nto adopt protection measures. Using the derived mathematical tools, we assess\nthe impact of these two control actions and determine the optimal control\npolicy."}
{"id": "2510.13932", "pdf": "https://arxiv.org/pdf/2510.13932", "abs": "https://arxiv.org/abs/2510.13932", "authors": ["Henrik Podéus", "Gustav Magnusson", "Sasan Keshmiri", "Kajsa Tunedal", "Nicolas Sundqvist", "William Lövfors", "Gunnar Cedersund"], "title": "SUND: simulation using nonlinear dynamic models - a toolbox for simulating multi-level, time-dynamic systems in a modular way", "categories": ["q-bio.QM", "65L05 (Primary) 37M05, 92C42 (Secondary)"], "comment": "6 pages, 1 figure, software paper. The last two listed authors\n  contributed equally to this work. Gunnar Cedersund is the corresponding\n  author", "summary": "When modeling complex, hierarchical, and time-dynamic systems, such as\nbiological systems, good computational tools are essential. Current tools,\nwhile powerful, often lack comprehensive frameworks for modular model\ncomposition, hierarchical system building, and time-dependent input handling,\nparticularly within the Python ecosystem. We present SUND (Simulation Using\nNonlinear Dynamic models), a Python toolbox designed to address these\nchallenges. SUND provides a unified framework for defining, combining, and\nsimulating multi-level time-dynamic systems. The toolbox enables users to\ndefine models with interconnectable inputs and outputs, facilitating the\nconstruction of complex systems from simpler, reusable components. It supports\ntime-dependent functions and piecewise constant inputs, enabling intuitive\nsimulation of various experimental conditions such as multiple dosing schemes.\nWe demonstrate the toolbox's capabilities through simulation of a multi-level\nhuman glucose-insulin system model, showcasing its flexibility in handling\nmultiple temporal scales, and levels of biological detail. SUND is open-source,\neasily extensible, and available at PyPI (https://pypi.org/project/sund/) and\nat Gitlab (https://gitlab.liu.se/ISBgroup/projects/sund/)."}
{"id": "2510.13894", "pdf": "https://arxiv.org/pdf/2510.13894", "abs": "https://arxiv.org/abs/2510.13894", "authors": ["Volker Tresp Hang Li", "Federico Harjes", "Yunpu Ma"], "title": "Bayes or Heisenberg: Who(se) Rules?", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "quant-ph"], "comment": null, "summary": "Although quantum systems are generally described by quantum state vectors, we\nshow that in certain cases their measurement processes can be reformulated as\nprobabilistic equations expressed in terms of probabilistic state vectors.\nThese probabilistic representations can, in turn, be approximated by the neural\nnetwork dynamics of the Tensor Brain (TB) model.\n  The Tensor Brain is a recently proposed framework for modeling perception and\nmemory in the brain, providing a biologically inspired mechanism for\nefficiently integrating generated symbolic representations into reasoning\nprocesses."}
{"id": "2510.14143", "pdf": "https://arxiv.org/pdf/2510.14143", "abs": "https://arxiv.org/abs/2510.14143", "authors": ["Alexandr A. Kalinin", "Anne E. Carpenter", "Shantanu Singh", "Matthew J. O'Meara"], "title": "cubic: CUDA-accelerated 3D Bioimage Computing", "categories": ["cs.CV", "q-bio.QM", "92C55, 68U10", "I.4.0; J.3"], "comment": "accepted to BioImage Computing workshop @ ICCV 2025", "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic"}
{"id": "2510.14188", "pdf": "https://arxiv.org/pdf/2510.14188", "abs": "https://arxiv.org/abs/2510.14188", "authors": ["Eric Albers", "Paul Marriott", "Masami Tatsuno"], "title": "Using Information Geometry to Characterize Higher-Order Interactions in EEG", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "In neuroscience, methods from information geometry (IG) have been\nsuccessfully applied in the modelling of binary vectors from spike train data,\nusing the orthogonal decomposition of the Kullback-Leibler divergence and\nmutual information to isolate different orders of interaction between neurons.\nWhile spike train data is well-approximated with a binary model, here we apply\nthese IG methods to data from electroencephalography (EEG), a continuous signal\nrequiring appropriate discretization strategies. We developed and compared\nthree different binarization methods and used them to identify third-order\ninteractions in an experiment involving imagined motor movements. The\nstatistical significance of these interactions was assessed using\nphase-randomized surrogate data that eliminated higher-order dependencies while\npreserving the spectral characteristics of the original signals. We validated\nour approach by implementing known second- and third-order dependencies in a\nforward model and quantified information attenuation at different steps of the\nanalysis. This revealed that the greatest loss in information occurred when\ngoing from the idealized binary case to enforcing these dependencies using\noscillatory signals. When applied to the real EEG dataset, our analysis\ndetected statistically significant third-order interactions during the task\ncondition despite the relatively sparse data (45 trials per condition). This\nwork demonstrates that IG methods can successfully extract genuine higher-order\ndependencies from continuous neural recordings when paired with appropriate\nbinarization schemes."}
{"id": "2510.14188", "pdf": "https://arxiv.org/pdf/2510.14188", "abs": "https://arxiv.org/abs/2510.14188", "authors": ["Eric Albers", "Paul Marriott", "Masami Tatsuno"], "title": "Using Information Geometry to Characterize Higher-Order Interactions in EEG", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "In neuroscience, methods from information geometry (IG) have been\nsuccessfully applied in the modelling of binary vectors from spike train data,\nusing the orthogonal decomposition of the Kullback-Leibler divergence and\nmutual information to isolate different orders of interaction between neurons.\nWhile spike train data is well-approximated with a binary model, here we apply\nthese IG methods to data from electroencephalography (EEG), a continuous signal\nrequiring appropriate discretization strategies. We developed and compared\nthree different binarization methods and used them to identify third-order\ninteractions in an experiment involving imagined motor movements. The\nstatistical significance of these interactions was assessed using\nphase-randomized surrogate data that eliminated higher-order dependencies while\npreserving the spectral characteristics of the original signals. We validated\nour approach by implementing known second- and third-order dependencies in a\nforward model and quantified information attenuation at different steps of the\nanalysis. This revealed that the greatest loss in information occurred when\ngoing from the idealized binary case to enforcing these dependencies using\noscillatory signals. When applied to the real EEG dataset, our analysis\ndetected statistically significant third-order interactions during the task\ncondition despite the relatively sparse data (45 trials per condition). This\nwork demonstrates that IG methods can successfully extract genuine higher-order\ndependencies from continuous neural recordings when paired with appropriate\nbinarization schemes."}
{"id": "2510.14227", "pdf": "https://arxiv.org/pdf/2510.14227", "abs": "https://arxiv.org/abs/2510.14227", "authors": ["Denizhan Pak"], "title": "Sensorimotor Contingencies and The Sensorimotor Approach to Cognition", "categories": ["q-bio.NC"], "comment": null, "summary": "4E views of cognition seek to replace many of the long-held assumptions of\ntra- ditional cognitive science. One of the most radical shifts is the\nrejection of the sandwich model of cognition [8], which holds that mental\nprocesses are located be- tween action and perception. Subversion of such a\nlong-held assumption requires an accessible theoretical alternative with firm\nexperimental support. One unifying thread among the emerging 4E camps is their\nshared insistence that sensorimotor contingencies (SMCs) are such an\nalternative."}
{"id": "2510.14481", "pdf": "https://arxiv.org/pdf/2510.14481", "abs": "https://arxiv.org/abs/2510.14481", "authors": ["Seong Jun Park"], "title": "Viral population dynamics at the cellular level, considering the replication cycle", "categories": ["q-bio.PE", "q-bio.QM"], "comment": null, "summary": "Viruses are microscopic infectious agents that require a host cell for\nreplication. Viral replication occurs in several stages, and the completion\ntime for each stage varies due to differences in the cellular environment.\nThus, the time to complete each stage in viral replication is a random\nvariable. However, no analytic expression exists for the viral population at\nthe cellular level when the completion time for each process constituting viral\nreplication is a random variable. This paper presents a simplified model of\nviral replication, treating each stage as a renewal process with independently\nand identically distributed completion times. Using the proposed model, we\nderive an analytical formula for viral populations at the cellular level, based\non viewing viral replication as a birth-death process. The mean viral count is\nexpressed via probability density functions representing the completion time\nfor each step in the replication process. This work validates the results with\nstochastic simulations. This study provides a new quantitative framework for\nunderstanding viral infection dynamics."}
{"id": "2510.14382", "pdf": "https://arxiv.org/pdf/2510.14382", "abs": "https://arxiv.org/abs/2510.14382", "authors": ["Yohei Yamada", "Zenas C. Chao"], "title": "Joint encoding of \"what\" and \"when\" predictions through error-modulated plasticity in reservoir spiking networks", "categories": ["q-bio.NC"], "comment": null, "summary": "The brain understands the external world through an internal model that\ngenerates predictions and refines them based on prediction errors. A complete\nprediction specifies what will happen, when it will happen, and with what\nprobability, which we refer to as a \"prediction object\". Existing models\ntypically capture only what and when, omit probabilities, and rely on\nbiologically-implausible algorithms. Here we show that a single population of\nspiking neurons can jointly encode the prediction object through a biologically\ngrounded learning mechanism. We implement a heterogeneous Izhikevich spiking\nreservoir with readouts trained by an error-modulated, attention-gated\nthree-factor Hebbian rule and test it on a novel paradigm that controls both\nthe timing and probability of upcoming stimuli. By integrating real-time\nlearning of \"when\" with offline consolidation of \"what\", the model encodes the\ncomplete prediction object, firing at the correct times with magnitudes\nproportional to the probabilities. Critically, it rapidly adapts to changes in\nboth stimulus timing and probability, an ability that global least-squares\nmethods such as FORCE lack without explicit resets. During learning, the model\nself-organizes its readout weights into near-orthogonal subspaces for \"what\"\nand \"when,\" showing that multiplexed encoding arises naturally from generic\nrecurrent dynamics under local, error-gated modulation. These results challenge\nthe view that \"what\" and \"when\" predictions require separate modules,\nsuggesting instead that mixed selectivity within shared populations supports\nflexible predictive cognition. The model also predicts phase-specific\nneuromodulation and overlapping neural subspaces, offering a parsimonious\nalternative to hierarchical predictive-coding accounts."}
{"id": "2510.14486", "pdf": "https://arxiv.org/pdf/2510.14486", "abs": "https://arxiv.org/abs/2510.14486", "authors": ["Roy Urbach", "Elad Schneidman"], "title": "Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks", "categories": ["q-bio.NC", "cs.AI"], "comment": "29 pages, 8 figures, 2 supplementary figures", "summary": "Brains learn to represent information from a large set of stimuli, typically\nby weak supervision. Unsupervised learning is therefore a natural approach for\nexploring the design of biological neural networks and their computations.\nAccordingly, redundancy reduction has been suggested as a prominent design\nprinciple of neural encoding, but its ``mechanistic'' biological implementation\nis unclear. Analogously, unsupervised training of artificial neural networks\nyields internal representations that allow for accurate stimulus classification\nor decoding, but typically rely on biologically-implausible implementations. We\nsuggest that interactions between parallel subnetworks in the brain may\nunderlie such learning: we present a model of representation learning by\nensembles of neural networks, where each network learns to encode stimuli into\nan abstract representation space by cross-supervising interactions with other\nnetworks, for inputs they receive simultaneously or in close temporal\nproximity. Aiming for biological plausibility, each network has a small\n``receptive field'', thus receiving a fixed part of the external input, and the\nnetworks do not share weights. We find that for different types of network\narchitectures, and for both visual or neuronal stimuli, these cross-supervising\nnetworks learn semantic representations that are easily decodable and that\ndecoding accuracy is comparable to supervised networks -- both at the level of\nsingle networks and the ensemble. We further show that performance is optimal\nfor small receptive fields, and that sparse connectivity between networks is\nnearly as accurate as all-to-all interactions, with far fewer computations. We\nthus suggest a sparsely interacting collective of cross-supervising networks as\nan algorithmic framework for representational learning and collective\ncomputation in the brain."}
{"id": "2510.14601", "pdf": "https://arxiv.org/pdf/2510.14601", "abs": "https://arxiv.org/abs/2510.14601", "authors": ["Fan Cao", "Yuqi Yuan", "Xiaohui Yan", "Bohan Zhang", "Kyle Perkins"], "title": "Nonlinear shift along the sensorimotor-association-axis in brain responses to task performance", "categories": ["q-bio.NC"], "comment": null, "summary": "In the literature of cognitive neuroscience, researchers tend to assume a\nlinear relationship between brain activation level and task performance;\nhowever, controversial findings have been reported in participants at different\nages and different proficiency levels. Therefore, there may be a non-linear\nrelationship between task performance and brain activation if a full range of\ntask performance is considered. In the current study, using the Human\nConnectome Project (HCP) dataset we examined the relationship between brain\nactivation and working memory performance in two conditions (i.e. faces and\nplaces). We found a gradual change from a U-shaped relationship to an inverted\nU-shaped relationship along the sensorimotor-association (S-A) axis in the face\ncondition. In other words, in low-order sensorimotor areas, it is U-shaped and\nin the high-order prefrontal and association areas, it is inverted U-shaped,\nwhich suggests different properties in the encoding/representation region and\nin the cognitive calculation regions. However, in the place condition, such a\nshift is missing, presumably because most of the regions that are sensitive to\ntask performance in the place condition are in the lower end of the S-A axis.\nTaken together, our study revealed a novel difference of functional property in\nresponse to task performance in the sensorimotor areas versus the association\nareas."}
