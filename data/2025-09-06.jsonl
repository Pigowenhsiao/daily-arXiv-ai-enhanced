{"id": "2509.03524", "pdf": "https://arxiv.org/pdf/2509.03524", "abs": "https://arxiv.org/abs/2509.03524", "authors": ["Feipeng Zhang", "Bingxin Lin", "Lei Zhou", "Long Wang"], "title": "Evolutionary dynamics under coordinated reciprocity", "categories": ["q-bio.PE", "cs.GT"], "comment": null, "summary": "Using past behaviors to guide future actions is essential for fostering\ncooperation in repeated social dilemmas. Traditional memory-based strategies\nthat focus on recent interactions have yielded valuable insights into the\nevolution of cooperative behavior. However, as memory length increases, the\ncomplexity of analysis grows exponentially, since these strategies need to map\nevery possible action sequence of a given length to subsequent responses. Due\nto their inherent reliance on exhaustive mapping and a lack of explicit\ninformation processing, it remains unclear how individuals can handle extensive\ninteraction histories to make decisions under cognitive constraints. To fill\nthis gap, we introduce coordinated reciprocity strategies ($CORE$), which\nincrementally evaluate the entire game history by tallying instances of\nconsistent actions between individuals without storing round-to-round details.\nOnce this consistency index surpasses a threshold, $CORE$ prescribes\ncooperation. Through equilibrium analysis, we derive an analytical condition\nunder which $CORE$ constitutes an equilibrium. Moreover, our numerical results\nshow that $CORE$ effectively promotes cooperation between variants of itself,\nand it outperforms a range of existing strategies including memory-$1$,\nmemory-$2$, and those from a documented strategy library in evolutionary\ndynamics. Our work thus underscores the pivotal role of cumulative action\nconsistency in enhancing cooperation, developing robust strategies, and\noffering cognitively low-burden information processing mechanisms in repeated\nsocial dilemmas."}
{"id": "2509.04188", "pdf": "https://arxiv.org/pdf/2509.04188", "abs": "https://arxiv.org/abs/2509.04188", "authors": ["Ma. Cristina R. Bargo"], "title": "Sensitivity analysis of an epidemic model with a mass vaccination program of a homogeneous population", "categories": ["q-bio.PE", "cs.NA", "math.NA", "92D30 (Primary), 65L05, 90C31 (Secondary)"], "comment": "23 pages, 15 figures, accepted for publication to SciEnggJ", "summary": "The COVID-19 pandemic forced the rapid development of vaccines and the\nimplementation of mass vaccination programs around the world. However, many\nhesitated to take the vaccine due to concerns about its effectiveness. By\nlooking at an ordinary differential equation (ODE) model of disease spread that\nincorporates a mass vaccination program, this study aims to determine the\nsensitivity of the cumulative count of infected individuals ($W$) and the\ncumulative death count ($D$) to the following model parameters: disease\ntransmission rate ($\\beta$), reciprocal of the disease latency period\n($\\kappa$), reciprocal of the infectious period ($\\gamma$), death ratio\n($\\alpha$), vaccine efficacy rate ($r$), and vaccine rollout rate ($\\delta$).\nThis was implemented using Latin hypercube sampling and partial rank\ncorrelation coefficient. Results show that $D$ is highly sensitive to $\\alpha$\nand shows increasing sensitivity to $\\delta$ in the long run. On the other\nhand, $W$ is highly sensitive to $\\kappa$ at the beginning of the simulation,\nbut this weakens over time. In contrast, $W$ is not very sensitive to $\\delta$\ninitially but becomes very significant in the long run. This supports the\nimportance of the vaccine rollout rate over the vaccine efficacy rate in\ncurbing the spread of the disease in the population. It is also worthwhile to\nreduce the death ratio by developing a cure for the disease or improving the\nhealthcare system as a whole."}
{"id": "2509.03534", "pdf": "https://arxiv.org/pdf/2509.03534", "abs": "https://arxiv.org/abs/2509.03534", "authors": ["Devansh Vimal", "Cole Mathis", "Westley Weimer", "Stephanie Forrest"], "title": "Prebiotic Functional Programs: Endogenous Selection in an Artificial Chemistry", "categories": ["cs.FL", "q-bio.PE"], "comment": null, "summary": "Artificial chemistry simulations produce many intriguing emergent behaviors,\nbut they are often difficult to steer or control. This paper proposes a method\nfor steering the dynamics of a classic artificial chemistry model, known as\nAlChemy (Algorithmic Chemistry), which is based on untyped lambda calculus. Our\napproach leverages features that are endogenous to AlChemy without constructing\nan explicit external fitness function or building learning into the dynamics.\nWe demonstrate the approach by synthesizing non-trivial lambda functions, such\nas Church addition and succession, from simple primitives. The results provide\ninsight into the possibility of endogenous selection in diverse systems such as\nautocatalytic chemical networks and software systems."}
{"id": "2509.03670", "pdf": "https://arxiv.org/pdf/2509.03670", "abs": "https://arxiv.org/abs/2509.03670", "authors": ["Hiwa Sheikh Ahmed Qalatobzany", "Kadhm Abdullah Muhammad", "Djshwar Dhahir Lateef", "Kamaran Salh Rasul", "Abdulrahman Smail Ibrahim", "Mariana Casari Parreira", "Weria Weisany"], "title": "Analysis of the Metabolic Profile and Biological Activity of Hawthorn Species Twigs: Crataegus azarolus and Crataegus monogyna", "categories": ["q-bio.OT"], "comment": null, "summary": "All parts of the hawthorn tree (Crataegus spp.), including fruits, flowers,\nand leaves, have been used as a source of bioactive compounds. Thus, in this\ninvestigation, the twigs of two species of hawthorn plant of Crataegus azarolus\n(C. azarolus) and Crataegus monogyna (C. monogyna) were evaluated for bioactive\ncompositions and biological activity (antioxidant and antimicrobial\nactivities). To evaluate bioactive compositions, high-performance liquid\nchromatography (HPLC) was applied, and for biological activity, biochemical\nassays were performed. C. monogyna revealed a higher amount of total phenolic,\ntotal flavonoid, and total tannin contents compared to C. azarolus. The HPLC\nresults indicated the highest amount of kaempferol (14.40%), catechin (17.70%),\nand gallic acid (25%) in twigs of C. azarolus, while the maximum quercetin\n(72%) compound was present in C. monogyna. C. monogyna exhibited higher\nantioxidant activity by 1,1 dizarophenyl 2 picrylhydrazyl (DPPH) (86.13%) and\n2,2 azino bis (3 ethylbenzothiazoline 6 sulfonic acid (ABTS) (92.93%) compared\nto C. azarolus for antioxidant activity DPPH (81.86%) and ABTS (87.47%) assay.\nIn the case of antimicrobial activity, the twigs of both species (especially C.\nazarolus) have a capacity against Bacillus subtilis, Staphylococcus aureus, and\nmethicillin-resistant Staphylococcus aureus. The results of this study revealed\nthat the twigs of both species contained a high amount of phenolic metabolites\nand antioxidant activity, while they showed low antimicrobial activity."}
{"id": "2509.03521", "pdf": "https://arxiv.org/pdf/2509.03521", "abs": "https://arxiv.org/abs/2509.03521", "authors": ["Timothee Robert", "MohammadAli Shaeri", "Mahsa Shoaran"], "title": "BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces", "categories": ["q-bio.NC", "cs.AI", "eess.SP"], "comment": "Accepted for publication in IEEE Neural Engineering (NER)\n  Conference'25", "summary": "Decoding bimanual hand movements from intracortical recordings remains a\ncritical challenge for brain-computer interfaces (BCIs), due to overlapping\nneural representations and nonlinear interlimb interactions. We introduce BiND\n(Bimanual Neural Discriminator-Decoder), a two-stage model that first\nclassifies motion type (unimanual left, unimanual right, or bimanual) and then\nuses specialized GRU-based decoders, augmented with a trial-relative time\nindex, to predict continuous 2D hand velocities. We benchmark BiND against six\nstate-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a\npublicly available 13-session intracortical dataset from a tetraplegic patient.\nBiND achieves a mean $R^2$ of 0.76 ($\\pm$0.01) for unimanual and 0.69\n($\\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model\n(GRU) by 2% in both tasks. It also demonstrates greater robustness to session\nvariability than all other benchmarked models, with accuracy improvements of up\nto 4% compared to GRU in cross-session analyses. This highlights the\neffectiveness of task-aware discrimination and temporal modeling in enhancing\nbimanual decoding."}
{"id": "2509.03551", "pdf": "https://arxiv.org/pdf/2509.03551", "abs": "https://arxiv.org/abs/2509.03551", "authors": ["Shubham Mishra", "The Anh Han", "Bruno Silvester Lopes", "Shatha Ghareeb", "Zia Ush Shamszaman"], "title": "Predicting Antimicrobial Resistance (AMR) in Campylobacter, a Foodborne Pathogen, and Cost Burden Analysis Using Machine Learning", "categories": ["q-bio.QM", "cs.LG"], "comment": "9 pages, 3 figures, 1 table. Submitted to a Briefings in\n  Bioinformatics journal and waiting for the outcome", "summary": "Antimicrobial resistance (AMR) poses a significant public health and economic\nchallenge, increasing treatment costs and reducing antibiotic effectiveness.\nThis study employs machine learning to analyze genomic and epidemiological data\nfrom the public databases for molecular typing and microbial genome diversity\n(PubMLST), incorporating data from UK government-supported AMR surveillance by\nthe Food Standards Agency and Food Standards Scotland. We identify AMR patterns\nin Campylobacter jejuni and Campylobacter coli isolates collected in the UK\nfrom 2001 to 2017. The research integrates whole-genome sequencing (WGS) data,\nepidemiological metadata, and economic projections to identify key resistance\ndeterminants and forecast future resistance trends and healthcare costs. We\ninvestigate gyrA mutations for fluoroquinolone resistance and the tet(O) gene\nfor tetracycline resistance, training a Random Forest model validated with\nbootstrap resampling (1,000 samples, 95% confidence intervals), achieving 74%\naccuracy in predicting AMR phenotypes. Time-series forecasting models (SARIMA,\nSIR, and Prophet) predict a rise in campylobacteriosis cases, potentially\nexceeding 130 cases per 100,000 people by 2050, with an economic burden\nprojected to surpass 1.9 billion GBP annually if left unchecked. An enhanced\nRandom Forest system, analyzing 6,683 isolates, refines predictions by\nincorporating temporal patterns, uncertainty estimation, and resistance trend\nmodeling, indicating sustained high beta-lactam resistance, increasing\nfluoroquinolone resistance, and fluctuating tetracycline resistance."}
{"id": "2509.04064", "pdf": "https://arxiv.org/pdf/2509.04064", "abs": "https://arxiv.org/abs/2509.04064", "authors": ["Pedro Carvalho", "Bernd Ulmann", "Wolf Singer", "Felix Effenberger"], "title": "An analog-electronic implementation of a harmonic oscillator recurrent neural network", "categories": ["q-bio.NC", "physics.app-ph", "physics.comp-ph"], "comment": null, "summary": "Oscillatory recurrent networks, such as the Harmonic Oscillator Recurrent\nNetwork (HORN) model, offer advantages in parameter efficiency, learning speed,\nand robustness relative to traditional non-oscillating architectures. Yet,\nwhile many implementations of physical neural networks exploiting attractor\ndynamics have been studied, implementations of oscillatory models in\nanalog-electronic hardware that utilize the networks' transient dynamics so far\nare lacking. This study explores the feasibility of implementing HORNs in\nanalog-electronic hardware while maintaining the computational performance of\nthe digital counterpart. Using a digital twin approach, we trained a four-node\nHORN in silico for sequential MNIST classification and transferred the trained\nparameters to an analog electronic implementation. A set of custom error\nmetrics indicated that the analog system is able to successfully replicate the\ndynamics of the digital model in most test cases. However, despite the overall\nwell-matching dynamics, when using the readout layer of the digital model on\nthe data generated by the analog system, we only observed $28.39\\%$ agreement\nwith the predictions of the digital model. An analysis shows that this mismatch\nis due to a precision difference between the analog hardware and the\nfloating-point representation exploited by the digital model to perform\nclassification tasks. When the analog system was utilized as a reservoir with a\nre-trained linear readout, its classification performance could be recovered to\nthat of the digital twin, indicating preserved information content within the\nanalog dynamics. This proof-of-concept establishes that analog electronic\ncircuits can effectively implement oscillatory neural networks for computation,\nproviding a demonstration of energy-efficient analog systems that exploit\nbrain-inspired transient dynamics for computation."}
{"id": "2509.03681", "pdf": "https://arxiv.org/pdf/2509.03681", "abs": "https://arxiv.org/abs/2509.03681", "authors": ["Andrew J. Stier", "Naichen Shi", "Raed Al Kontar", "Chad Giusti", "Marc G. Berman"], "title": "ALBATROSS: Cheap Filtration Based Geometry via Stochastic Sub-Sampling", "categories": ["q-bio.QM"], "comment": null, "summary": "Topological data analysis (TDA) detects geometric structure in biological\ndata. However, many TDA algorithms are memory intensive and impractical for\nmassive datasets. Here, we introduce a statistical protocol that reduces TDA's\nmemory requirements and gives access to scientists with modest computing\nresources. We validate this protocol against two empirical datasets, showing\nthat it replicates previous findings with much lower memory requirements.\nFinally, we demonstrate the power of the protocol by mapping the topology of\nfunctional correlations for the human cortex at high spatial resolution,\nsomething that was previously infeasible without this novel approach."}
{"id": "2509.04106", "pdf": "https://arxiv.org/pdf/2509.04106", "abs": "https://arxiv.org/abs/2509.04106", "authors": ["Mauricio Girardi-Schappo", "Leonard Maler", "André Longtin"], "title": "Optimal rate-variance coding due to firing threshold adaptation near criticality", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cond-mat.stat-mech", "nlin.AO", "physics.bio-ph"], "comment": "14 pages, 3 figures", "summary": "Recurrently connected neuron populations play key roles in sensory perception\nand memory storage across various brain regions. While these populations are\noften assumed to encode information through firing rates, this method becomes\nunreliable with weak stimuli. We propose that in such cases, information can be\ntransmitted via spatial spike patterns, employing a sparse or combinatorial\ncoding based on firing rate variance. Around the critical point of a stochastic\nrecurrent excitable network, we uncover a synergistic dual-coding scheme,\nenabled by single-cell threshold adaptation. This scheme optimizes variance\ncoding for weak signals without compromising rate coding for stronger inputs,\nthus maximizing input/output mutual information. These optimizations are robust\nacross adaptation rules and coupling strengths through self-suppression of\ninternal noise, particularly around the network's phase transition, and are\nlinked to threshold recovery times observed in hippocampal memory circuits\n(~$10^2$-$10^3$ms). In contrast, nonadaptive networks perform similarly only at\ncriticality, suggesting that threshold adaptation is essential for reliable\nencoding of weak signals into diverse spatial patterns. Our results imply a\nfundamental role for near-critical latent adaptive dynamics enabled by dual\ncoding in biological and artificial neural networks."}
{"id": "2509.03765", "pdf": "https://arxiv.org/pdf/2509.03765", "abs": "https://arxiv.org/abs/2509.03765", "authors": ["Ilya Nemenman", "Pankaj Mehta"], "title": "Randomness with constraints: constructing minimal models for high-dimensional biology", "categories": ["physics.bio-ph", "q-bio.QM"], "comment": "9 pages, 2 figures", "summary": "Biologists and physicists have a rich tradition of modeling living systems\nwith simple models composed of a few interacting components. Despite the\nremarkable success of this approach, it remains unclear how to use such finely\ntuned models to study complex biological systems composed of numerous\nheterogeneous, interacting components. One possible strategy for taming this\nbiological complexity is to embrace the idea that many biological behaviors we\nobserve are ``typical'' and can be modeled using random systems that respect\nbiologically-motivated constraints. Here, we review recent works showing how\nthis approach can be used to make close connection with experiments in\nbiological systems ranging from neuroscience to ecology and evolution and\nbeyond. Collectively, these works suggest that the ``random-with-constraints''\nparadigm represents a promising new modeling strategy for capturing\nexperimentally observed dynamical and statistical features in high-dimensional\nbiological data and provides a powerful minimal modeling philosophy for\nbiology."}
{"id": "2509.01235", "pdf": "https://arxiv.org/pdf/2509.01235", "abs": "https://arxiv.org/abs/2509.01235", "authors": ["Yixiong Ren", "Wenkang Du", "Jianhui Zhou", "Haiping Huang"], "title": "Geometric origin of adversarial vulnerability in deep learning", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.NC"], "comment": null, "summary": "How to balance training accuracy and adversarial robustness has become a\nchallenge since the birth of deep learning. Here, we introduce a geometry-aware\ndeep learning framework that leverages layer-wise local training to sculpt the\ninternal representations of deep neural networks. This framework promotes\nintra-class compactness and inter-class separation in feature space, leading to\nmanifold smoothness and adversarial robustness against white or black box\nattacks. The performance can be explained by an energy model with Hebbian\ncoupling between elements of the hidden representation. Our results thus shed\nlight on the physics of learning in the direction of alignment between\nbiological and artificial intelligence systems. Using the current framework,\nthe deep network can assimilate new information into existing knowledge\nstructures while reducing representation interference."}
