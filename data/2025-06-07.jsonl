{"id": "2506.04490", "pdf": "https://arxiv.org/pdf/2506.04490", "abs": "https://arxiv.org/abs/2506.04490", "authors": ["Rishwanth Raghu", "Axel Levy", "Gordon Wetzstein", "Ellen D. Zhong"], "title": "Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Protein structure prediction models are now capable of generating accurate 3D\nstructural hypotheses from sequence alone. However, they routinely fail to\ncapture the conformational diversity of dynamic biomolecular complexes, often\nrequiring heuristic MSA subsampling approaches for generating alternative\nstates. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a\npowerful tool for imaging near-native structural heterogeneity, but is\nchallenged by arduous pipelines to go from raw experimental data to atomic\nmodels. Here, we bridge the gap between these modalities, combining cryo-EM\ndensity maps with the rich sequence and biophysical priors learned by protein\nstructure prediction models. Our method, CryoBoltz, guides the sampling\ntrajectory of a pretrained protein structure prediction model using both global\nand local structural constraints derived from density maps, driving predictions\ntowards conformational states consistent with the experimental data. We\ndemonstrate that this flexible yet powerful inference-time approach allows us\nto build atomic models into heterogeneous cryo-EM maps across a variety of\ndynamic biomolecular systems including transporters and antibodies."}
{"id": "2506.04247", "pdf": "https://arxiv.org/pdf/2506.04247", "abs": "https://arxiv.org/abs/2506.04247", "authors": ["Gage K. R. Hooper"], "title": "The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model", "categories": ["q-bio.NC", "cs.AI", "cs.NE", "92B20, 37N25, 68T05", "I.2.6; I.5.1; I.6.3"], "comment": "31 pages, 16 figures", "summary": "While many neural networks focus on layers to process information, the GAIN\nmodel uses a grid-based structure to improve biological plausibility and the\ndynamics of the model. The grid structure helps neurons to interact with their\nclosest neighbors and improve their connections with one another, which is seen\nin biological neurons. While also being implemented with the Izhikevich model\nthis approach allows for a computationally efficient and biologically accurate\nsimulation that can aid in the development of neural networks, large scale\nsimulations, and the development in the neuroscience field. This adaptation of\nthe Izhikevich model can improve the dynamics and accuracy of the model,\nallowing for its uses to be specialized but efficient."}
{"id": "2506.04549", "pdf": "https://arxiv.org/pdf/2506.04549", "abs": "https://arxiv.org/abs/2506.04549", "authors": ["Vardhan Palod", "Pranav Mahajan", "Veeky Baths", "Boris S. Gutkin"], "title": "Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning", "categories": ["q-bio.NC"], "comment": "Accepted in the Proceedings of Cognitive Computational Neuroscience\n  (CCN) 2025", "summary": "Despite a strong desire to quit, individuals with long-term substance use\ndisorder (SUD) often struggle to resist drug use, even when aware of its\nharmful consequences. This disconnect between knowledge and compulsive behavior\nreflects a fundamental cognitive-behavioral conflict in addiction.\nNeurobiologically, differential cue-induced activity within striatal\nsubregions, along with dopamine-mediated connectivity from the ventral to the\ndorsal striatum, contributes to compulsive drug-seeking. However, the\nfunctional mechanism linking these findings to behavioral conflict remains\nunclear. Another hallmark of addiction is temporal discounting: individuals\nwith drug dependence exhibit steeper discount rates than non-users. Assuming\nthe ventral-dorsal striatal organization reflects a gradient from cognitive to\nmotor representations, addiction can be modeled within a hierarchical\nreinforcement learning (HRL) framework. However, integrating discounting into\nbiologically grounded HRL remains an open challenge. In this work, we build on\na model showing how action choices reinforced with drug rewards become\ninsensitive to the negative consequences that follow. We address the\nintegration of discounting by ensuring natural reward values converge across\nall levels in the HRL hierarchy, while drug rewards diverge due to their\ndopaminergic effects. Our results show that high discounting amplifies\ndrug-seeking across the hierarchy, linking faster discounting with increased\naddiction severity and impulsivity. We demonstrate alignment with empirical\nfindings on temporal discounting and propose testable predictions, establishing\naddiction as a disorder of hierarchical decision-making."}
{"id": "2506.04875", "pdf": "https://arxiv.org/pdf/2506.04875", "abs": "https://arxiv.org/abs/2506.04875", "authors": ["Alexander Gorsky"], "title": "Consciousness via MIPT?", "categories": ["q-bio.NC"], "comment": "24 pages", "summary": "The measurement-induced phase transition (MIPT) is a recently formulated\nphenomenon in out-of-equilibrium systems. The competition between unitary\nevolutions and measurement-induced non-unitaries leads to the transition\nbetween the entangled and disentangled phases at some critical measurement\nrate. We conjecture that self-organized MIPT plays a key role in the generative\nmodel of cognitive networks and the formation of the state of consciousness in\nthe \"newborn-adult\" transition. To this aim, we formulate the probe-target\npicture for the brain and suggest that MIPT interpreted as learnability\ntransition takes place in the mental part of the target where the sites in the\ncognitive networks of semantic memory are concepts. Comparison with the\nsynchronization phase transitions in the probe is made."}
{"id": "2506.04906", "pdf": "https://arxiv.org/pdf/2506.04906", "abs": "https://arxiv.org/abs/2506.04906", "authors": ["Lisa Schmors", "Dominic Gonschorek", "Jan Niklas BÃ¶hm", "Yongrong Qiu", "Na Zhou", "Dmitry Kobak", "Andreas Tolias", "Fabian Sinz", "Jacob Reimer", "Katrin Franke", "Sebastian Damrich", "Philipp Berens"], "title": "TRACE: Contrastive learning for multi-trial time-series data in neuroscience", "categories": ["q-bio.NC"], "comment": null, "summary": "Modern neural recording techniques such as two-photon imaging allow to\nacquire vast time-series datasets with responses of hundreds or thousands of\nneurons. Contrastive learning is a powerful self-supervised framework for\nlearning representations of complex datasets. Existing applications for neural\ntime series rely on generic data augmentations and do not exploit the\nmulti-trial data structure inherent in many neural datasets. Here we present\nTRACE, a new contrastive learning framework that averages across different\nsubsets of trials to generate positive pairs. TRACE allows to directly learn a\ntwo-dimensional embedding, combining ideas from contrastive learning and\nneighbor embeddings. We show that TRACE outperforms other methods, resolving\nfine response differences in simulated data. Further, using in vivo recordings,\nwe show that the representations learned by TRACE capture both biologically\nrelevant continuous variation, cell-type-related cluster structure, and can\nassist data quality control."}
{"id": "2506.04943", "pdf": "https://arxiv.org/pdf/2506.04943", "abs": "https://arxiv.org/abs/2506.04943", "authors": ["Shujun Zhou", "Guozhang Chen"], "title": "The Hippocampal Place Field Gradient: An Eigenmode Theory Linking Grid Cell Projections to Multiscale Learning", "categories": ["q-bio.NC"], "comment": null, "summary": "The hippocampus encodes space through a striking gradient of place field\nsizes along its dorsal-ventral axis, yet the principles generating this\ncontinuous gradient from discrete grid cell inputs remain debated. We propose a\nunified theoretical framework establishing that hippocampal place fields arise\nnaturally as linear projections of grid cell population activity, interpretable\nas eigenmodes. Critically, we demonstrate that a frequency-dependent decay of\nthese grid-to-place connection weights naturally transforms inputs from\ndiscrete grid modules into a continuous spectrum of place field sizes. This\nmultiscale organization is functionally significant: we reveal it shapes the\ninductive bias of the population code, balancing a fundamental trade-off\nbetween precision and generalization. Mathematical analysis and simulations\ndemonstrate an optimal place field size for few-shot learning, which scales\nwith environment structure. Our results offer a principled explanation for the\nplace field gradient and generate testable predictions, bridging anatomical\nconnectivity with adaptive learning in both biological and artificial\nintelligence."}
{"id": "2506.05320", "pdf": "https://arxiv.org/pdf/2506.05320", "abs": "https://arxiv.org/abs/2506.05320", "authors": ["Avery Hee-Woon Ryoo", "Nanda H. Krishna", "Ximeng Mao", "Mehdi Azabou", "Eva L. Dyer", "Matthew G. Perich", "Guillaume Lajoie"], "title": "Generalizable, real-time neural decoding with hybrid state-space models", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint. Under review", "summary": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications."}
{"id": "2506.04289", "pdf": "https://arxiv.org/pdf/2506.04289", "abs": "https://arxiv.org/abs/2506.04289", "authors": ["Jesse Geerts", "Stephanie Chan", "Claudia Clopath", "Kimberly Stachenfeld"], "title": "Relational reasoning and inductive bias in transformers trained on a transitive inference task", "categories": ["cs.LG", "q-bio.NC"], "comment": "13 pages, 6 figures", "summary": "Transformer-based models have demonstrated remarkable reasoning abilities,\nbut the mechanisms underlying relational reasoning in different learning\nregimes remain poorly understood. In this work, we investigate how transformers\nperform a classic relational reasoning task from the Psychology literature,\n\\textit{transitive inference}, which requires inference about indirectly\nrelated items by integrating information across observed adjacent item pairs\n(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior\nacross two distinct learning regimes: in-weights learning (IWL), where models\nstore information in network parameters, and in-context learning (ICL), where\nmodels flexibly utilize information presented within the input sequence. Our\nfindings reveal that IWL naturally induces a generalization bias towards\ntransitive inference, despite being trained only on adjacent items, whereas ICL\nmodels trained solely on adjacent items do not generalize transitively.\nMechanistic analysis shows that ICL models develop induction circuits that\nimplement a simple match-and-copy strategy that performs well at relating\nadjacent pairs, but does not encoding hierarchical relationships among\nindirectly related items. Interestingly, when pre-trained on in-context linear\nregression tasks, transformers successfully exhibit in-context generalizable\ntransitive inference. Moreover, like IWL, they display both \\textit{symbolic\ndistance} and \\textit{terminal item effects} characteristic of human and animal\nperformance, without forming induction circuits. These results suggest that\npre-training on tasks with underlying structure promotes the development of\nrepresentations that can scaffold in-context relational reasoning."}
{"id": "2506.04379", "pdf": "https://arxiv.org/pdf/2506.04379", "abs": "https://arxiv.org/abs/2506.04379", "authors": ["Matthew W. Shinkle", "Mark D. Lescroart"], "title": "Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "Accepted to the Mechanistic Interpretability for Vision (MIV)\n  Workshop at the 2025 Conference on Computer Vision and Pattern Recognition\n  (CVPR) conference", "summary": "Deep neural networks (DNNs) trained on visual tasks develop feature\nrepresentations that resemble those in the human visual system. Although\nDNN-based encoding models can accurately predict brain responses to visual\nstimuli, they offer limited insight into the specific features driving these\nresponses. Here, we demonstrate that activation maximization -- a technique\ndesigned to interpret vision DNNs -- can be applied to DNN-based encoding\nmodels of the human brain. We extract and adaptively downsample activations\nfrom multiple layers of a pretrained Inception V3 network, then use linear\nregression to predict fMRI responses. This yields a full image-computable model\nof brain responses. Next, we apply activation maximization to generate images\noptimized for predicted responses in individual cortical voxels. We find that\nthese images contain visual characteristics that qualitatively correspond with\nknown selectivity and enable exploration of selectivity across the visual\ncortex. We further extend our method to whole regions of interest (ROIs) of the\nbrain and validate its efficacy by presenting these images to human\nparticipants in an fMRI study. We find that the generated images reliably drive\nactivity in targeted regions across both low- and high-level visual areas and\nacross subjects. These results demonstrate that activation maximization can be\nsuccessfully applied to DNN-based encoding models. By addressing key\nlimitations of alternative approaches that require natively generative models,\nour approach enables flexible characterization and modulation of responses\nacross the human visual system."}
{"id": "2506.04536", "pdf": "https://arxiv.org/pdf/2506.04536", "abs": "https://arxiv.org/abs/2506.04536", "authors": ["Luca Ghafourpour", "Valentin Duruisseaux", "Bahareh Tolooshams", "Philip H. Wong", "Costas A. Anastassiou", "Anima Anandkumar"], "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Characterizing the diverse computational properties of human neurons via\nmultimodal electrophysiological, transcriptomic, and morphological data\nprovides the foundation for constructing and validating bio-realistic neuron\nmodels that can advance our understanding of fundamental mechanisms underlying\nbrain function. However, current modeling approaches remain constrained by the\nlimited availability and intrinsic variability of experimental neuronal data.\nTo capture variability, ensembles of deterministic models are often used, but\nare difficult to scale as model generation requires repeating computationally\nexpensive optimization for each neuron. While deep learning is becoming\nincreasingly relevant in this space, it fails to capture the full biophysical\ncomplexity of neurons, their nonlinear voltage dynamics, and variability. To\naddress these shortcomings, we introduce NOBLE, a neural operator framework\nthat learns a mapping from a continuous frequency-modulated embedding of\ninterpretable neuron features to the somatic voltage response induced by\ncurrent injection. Trained on data generated from biophysically realistic\nneuron models, NOBLE predicts distributions of neural dynamics accounting for\nthe intrinsic experimental variability. Unlike conventional bio-realistic\nneuron models, interpolating within the embedding space offers models whose\ndynamics are consistent with experimentally observed responses. NOBLE is the\nfirst scaled-up deep learning framework validated on real experimental data,\nenabling efficient generation of synthetic neurons that exhibit trial-to-trial\nvariability and achieve a $4200\\times$ speedup over numerical solvers. To this\nend, NOBLE captures fundamental neural properties, opening the door to a better\nunderstanding of cellular composition and computations, neuromorphic\narchitectures, large-scale brain circuits, and general neuroAI applications."}
{"id": "2506.05178", "pdf": "https://arxiv.org/pdf/2506.05178", "abs": "https://arxiv.org/abs/2506.05178", "authors": ["Joshua Hess", "Quaid Morris"], "title": "Associative Memory and Generative Diffusion in the Zero-noise Limit", "categories": ["cs.LG", "cond-mat.dis-nn", "math.DS", "nlin.AO", "q-bio.NC"], "comment": null, "summary": "Connections between generative diffusion and continuous-state associative\nmemory models are studied. Morse-Smale dynamical systems are emphasized as\nuniversal approximators of gradient-based associative memory models and\ndiffusion models as white-noise perturbed systems thereof. Universal properties\nof associative memory that follow from this description are described and used\nto characterize a generic transition from generation to memory as noise levels\ndiminish. Structural stability inherited by Morse-Smale flows is shown to imply\na notion of stability for diffusions at vanishing noise levels. Applied to one-\nand two-parameter families of gradients, this indicates stability at all but\nisolated points of associative memory learning landscapes and the learning and\ngeneration landscapes of diffusion models with gradient drift in the zero-noise\nlimit, at which small sets of generic bifurcations characterize qualitative\ntransitions between stable systems. Examples illustrating the characterization\nof these landscapes by sequences of these bifurcations are given, along with\nstructural stability criterion for classic and modern Hopfield networks\n(equivalently, the attention mechanism)."}
{"id": "2506.05303", "pdf": "https://arxiv.org/pdf/2506.05303", "abs": "https://arxiv.org/abs/2506.05303", "authors": ["David G. Clark"], "title": "Transient dynamics of associative memory models", "categories": ["cond-mat.dis-nn", "q-bio.NC"], "comment": "18 pages, 7 figures", "summary": "Associative memory models such as the Hopfield network and its dense\ngeneralizations with higher-order interactions exhibit a \"blackout\ncatastrophe\"--a discontinuous transition where stable memory states abruptly\nvanish when the number of stored patterns exceeds a critical capacity. This\ntransition is often interpreted as rendering networks unusable beyond capacity\nlimits. We argue that this interpretation is largely an artifact of the\nequilibrium perspective. We derive dynamical mean-field equations using a\nbipartite cavity approach for graded-activity dense associative memory models,\nwith the Hopfield model as a special case, and solve them using a numerical\nscheme. We show that patterns can be transiently retrieved with high accuracy\nabove capacity despite the absence of stable attractors. This occurs because\nslow regions persist in the above-capacity energy landscape as shallow,\nunstable remnants of below-capacity stable basins. The same transient-retrieval\neffect occurs in below-capacity networks initialized outside basins of\nattraction. \"Transient-recovery curves\" provide a concise visual summary of\nthese effects, revealing graceful, non-catastrophic changes in retrieval\nbehavior above capacity and allowing us to compare the behavior across\ninteraction orders. This dynamical perspective reveals rich energy landscape\nstructure obscured by equilibrium analysis and suggests biological neural\ncircuits may exploit transient dynamics for memory retrieval. Furthermore, our\napproach suggests ways of understanding computational properties of neural\ncircuits without reference to fixed points, advances the technical repertoire\nof numerical mean-field solution methods for recurrent neural networks, and\nyields new theoretical results on generalizations of the Hopfield model."}
