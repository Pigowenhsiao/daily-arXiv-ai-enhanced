{"id": "2601.14313", "pdf": "https://arxiv.org/pdf/2601.14313", "abs": "https://arxiv.org/abs/2601.14313", "authors": ["Nicolas Elbaz", "Valérie Biran", "Chloé Ghozland", "Laurie Devisscher", "Aline Gonzalez Carpinteiro", "Aurélie Bourmaud", "Monique Elmaleh-Bergès", "Lucie Hertz-Pannier", "Yann Leprince", "Alice Frérot", "Alice Héneau", "Jessica Dubois", "Marianne Alison"], "title": "Investigating cerebral anomalies in preterm infants and associated risk factors with magnetic resonance imaging at term-equivalent age", "categories": ["q-bio.NC"], "comment": null, "summary": "Background: Being born very or extreme preterm is a major source of cerebral anomalies and neurodevelopmental disorders, whose occurrence depends on many perinatal factors. A better understanding of these factors could be provided by cerebral Magnetic Resonance Imaging (MRI) at term-equivalent age (TEA). Objective: To investigate, through cerebral TEA-MRIs, the relationship between the main perinatal factors, the occurrence of cerebral anomalies, and cerebral volumetry. Methods: We assembled a cohort of preterm babies born before 32 weeks of gestation who underwent a cerebral TEA-MRI. We assessed cerebral anomalies using a radiological scoring system -- the Kidokoro scoring -- and performed cerebral volumetry. We investigated the relationships between 9 clinical factors (birth characteristics, resuscitation treatments{\\ldots}), Kidokoro scores, and brain volumes. Results: Among 110 preterms who underwent a cerebral MRI at TEA, only 6% suffered moderate-to-severe brain anomalies. We identified mechanical ventilation as a risk factor for cerebral anomalies (adjusted Odds-Ratio aOR = 4.6, 95% Confidence Interval CI [1.7-12.8]) and prolonged parenteral nutrition as a protective factor for white matter anomalies (aOR = 0.2, 95%CI [0.1-0.8]). Mechanical ventilation (p = 0.01) and being born small for gestational age (p < 0.001) were risk factors for the reduction of cerebral volumes. An increase in brain lesion severity was associated with decreased cerebral volumes (p = 0.016). Conclusion: Our study highlights the importance of treatment-related perinatal factors on the occurrence of cerebral anomalies in very and extreme preterms, and the interest in using both qualitative (Kidokoro scoring) and quantitative (volumetry) MRI-tools."}
{"id": "2601.14961", "pdf": "https://arxiv.org/pdf/2601.14961", "abs": "https://arxiv.org/abs/2601.14961", "authors": ["Zhengdi Zhang", "Cong Han", "Wenjun Xia"], "title": "Power-Law Scaling in the Classification Performance of Small-Scale Spiking Neural Networks", "categories": ["q-bio.NC"], "comment": null, "summary": "This paper investigates the classification capability of small-scale spiking neural networks based on the Leaky Integrate-and-Fire (LIF) neuron model. We analyze the relationship between classification accuracy and three factors: the number of neurons, the number of stimulus nodes, and the number of classification categories. Notably, we employ a large language model (LLM) to assist in discovering the underlying functional relationships among these variables, and compare its performance against traditional methods such as linear and polynomial fitting. Experimental results show that classification accuracy follows a power-law scaling primarily with the number of categories, while the effects of neuron count and stimulus nodes are relatively minor. A key advantage of the LLM-based approach is its ability to propose plausible functional forms beyond pre-defined equation templates, often leading to more concise or accurate mathematical descriptions of the observed scaling laws. This finding has important implications for understanding efficient computation in biological neural systems and for pioneering new paradigms in AI-aided scientific discovery."}
{"id": "2601.15032", "pdf": "https://arxiv.org/pdf/2601.15032", "abs": "https://arxiv.org/abs/2601.15032", "authors": ["Zhengdi Zhang", "Yan Xu", "Wenjun Xia"], "title": "Single-Node Wilson--Cowan Model Accounts for Speech-Evoked $γ$-Band Deficits in Schizophrenia", "categories": ["q-bio.NC"], "comment": null, "summary": "Cortical gamma ($γ$)-band activity reflects local excitation-inhibition (E/I) balance. In schizophrenia (SCZ), reduced task-evoked gamma suggests altered E/I dynamics, but it is unclear whether differences stem from input properties or systematic shifts in E/I operating point and gain. We coupled a cochlear-inspired speech front end to a Wilson-Cowan E/I model to simulate gamma responses across three conditions: Healthy, SCZ-speech, and SCZ-semantics. Metrics included event-related spectral perturbation (ERSP$_γ$) and threshold-time fraction ($γ%$). A stable hierarchy emerged: Healthy(speech/semantics) $>$ SCZ(speech) $>$ SCZ(semantics), robust under equal-energy control and gain perturbations. Network dynamics coincided with single-node solutions, supporting interpretability. Pharmacological analogs showed bidirectional effects: reduced inhibition lowered $γ$, while reduced excitation increased $γ$, with no self-sustained oscillations. Findings indicate SCZ gamma deficits align more with shifts in E/I operating point and gain than input differences. This pipeline provides a testable, reusable mechanistic framework for speech-evoked gamma and a baseline for cross-population studies."}
{"id": "2601.14514", "pdf": "https://arxiv.org/pdf/2601.14514", "abs": "https://arxiv.org/abs/2601.14514", "authors": ["Tony Chen", "Sam Cheyette", "Kelsey Allen", "Joshua Tenenbaum", "Kevin Smith"], "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation."}
{"id": "2601.14678", "pdf": "https://arxiv.org/pdf/2601.14678", "abs": "https://arxiv.org/abs/2601.14678", "authors": ["Justin Cheung", "Samuel Savine", "Calvin Nguyen", "Lin Lu", "Alhassan S. Yasin"], "title": "Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "q-bio.TO"], "comment": "8 pages, 6 figures, 3 table", "summary": "Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.\n  This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.\n  We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types."}
{"id": "2601.14869", "pdf": "https://arxiv.org/pdf/2601.14869", "abs": "https://arxiv.org/abs/2601.14869", "authors": ["Joshua Looker", "Kat S. Rock", "Louise Dyson"], "title": "Early warning signals of non-critical transitions from linearised time-varying dynamics with applications to epidemic systems", "categories": ["q-bio.PE"], "comment": null, "summary": "In the wake of the SARS-CoV-2 pandemic, there has been heightened interest from applied mathematicians in infectious disease modelling. Modelling efforts often focus on predicting whether diseases are likely to be eliminated or, instead, (re-)emerge, especially as a result of control measures.This tipping point between elimination and infection waves has been successfully anticipated in the literature through the use of early warning signals and such signals often rely on the theory of critical slowing down. Recent developments have shown that these signals (increases in fluctuation variance and return time) can emerge from the system geometry in the case of non-normal dynamics rather than a change in asymptotic stability. We show how such dynamical behaviour occurs in the fluctuations from the mean-field in general stochastic systems. Using the susceptible-infectious-recovered model as an example application, we analyse how critical-like behaviour can be exploited to anticipate infection waves in the absence of an equilibrium bifurcation."}
{"id": "2601.14624", "pdf": "https://arxiv.org/pdf/2601.14624", "abs": "https://arxiv.org/abs/2601.14624", "authors": ["Simeng Zhang", "Xinying Liu", "Jun Lou", "Mudi Jiang", "Quan Zou", "Zengyou He"], "title": "Biological Sequence Clustering: A Survey", "categories": ["q-bio.GN"], "comment": null, "summary": "The rapid development of high-throughput sequencing technologies has led to an explosive increase in biological sequence data, making sequence clustering a fundamental task in large-scale bioinformatics analyses. Unlike traditional clustering problems, biological sequence clustering faces unique challenges due to the lack of direct similarity measures, strict biological constraints, and demanding requirements for both scalability and accuracy. Over the past decades, a wide variety of methods have been developed, differing in how they model sequence similarity, construct clusters, and prioritize optimization objectives. In this review, we provide a comprehensive methodological overview of biological sequence clustering algorithms. We begin by summarizing the main strategies for modeling sequence similarity, which can be divided into three stages: sequence encoding, feature generation, and similarity measurement. Next, we discuss the major clustering paradigms, including greedy incremental, hierarchical, graph-based, model-based, partitional, and deep learning approaches, highlighting their methodological characteristics and practical trade-offs. We then discuss clustering objectives from three key perspectives: scalability and resource efficiency, biological interpretability, and robustness and clustering quality. Organizing existing methods along these dimensions allows us to explore the trade-offs in biological sequence clustering and clarify the contexts in which different approaches are most appropriate. Finally, we identify current limitations and challenges, providing guidance for researchers and directions for future method development."}
{"id": "2601.14314", "pdf": "https://arxiv.org/pdf/2601.14314", "abs": "https://arxiv.org/abs/2601.14314", "authors": ["Santiago Silva", "Ghiles Reguig", "Neil P Oxtoby", "Andre Altmann", "Marco Lorenzi"], "title": "Fed-ComBat: A Generalized Federated Framework for Batch Effect Harmonization in Collaborative Studies", "categories": ["q-bio.QM"], "comment": null, "summary": "The use of multi-centric analyses is crucial for obtaining sufficient sample sizes and representative clinical populations in experimental studies. In this setting, data harmonization techniques are typically employed to address systematic biases and ensure the interoperability of the data. State-of-the-art harmonisation approaches are based on the statistical theory of random effect modeling, allowing to account for either linear of non-linear biases and batch effects. However, optimizing these statistical methods generally requires data centralization at some point during the analysis pipeline, therefore introducing the risk of exposing individual patient information while posing significant data governance issues. To overcome this challenge, in this paper we present Fed-ComBat, a federated framework for batch effect harmonization on decentralized data. Fed-ComBat enables the preservation of nonlinear covariate effects without requiring centralization of data and without prior parametric hypothesis on the variables to account for. We demonstrate the effectiveness of Fed-ComBat against a comprehensive panel of existing approaches based on the state-of-the-art ComBat, along with distributed and nonlinear variants. Our experiments are based on extensive simulated data, and on the analysis of multiple cohorts based on 7 neuroimaging studies comprising healthy controls (CI) and subjects with various disorders such as Parkinson's disease (PD), Alzheimer's disease (AD), and autism spectrum disorder (ASD). Our results show that in a federated settings, Fed-ComBat harmonization exhibits comparable results to centralized methods for both linear and nonlinear cases. On real data, harmonized trajectories of the thickness ofthe right hippocampus across lifespan measured on a set of 7 public studies show comparable results between centralized and federated models and are consistent with the literature when using a nonlinear model. The code is publicly available at: https://gitlab.inria.fr/greguig/fedcombat"}
{"id": "2601.15091", "pdf": "https://arxiv.org/pdf/2601.15091", "abs": "https://arxiv.org/abs/2601.15091", "authors": ["Vuong Hung Truong", "Mariana Gabrielle Cangco Reyes", "Masatoshi Koizumi", "Jihwan Myung"], "title": "Circadian Modulation of Semantic Exploration in Social Media Language", "categories": ["cs.CL", "cs.CY", "cs.SI", "q-bio.NC"], "comment": "25 pages, 6 figures, 3 supplementary figures", "summary": "Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with \"rich-get-richer\" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain."}
{"id": "2601.15144", "pdf": "https://arxiv.org/pdf/2601.15144", "abs": "https://arxiv.org/abs/2601.15144", "authors": ["Thomas Van Giel", "Hanna Jaspaert", "Aisling J. Daly", "Bernard De Baets", "Jan M. Baetens"], "title": "Modification speed and radius of higher-order interactions alter the oscillatory dynamics in an agent-based model", "categories": ["q-bio.PE"], "comment": null, "summary": "Understanding the population dynamics of ecological systems is crucial for predicting shifts in biodiversity and ensuring the protection of these systems. Established models often focus on pairwise species interactions, yet recent studies have highlighted the importance of higher-order interactions (HOIs) in shaping community structure and function. In this study, we investigate the effects of HOIs in an agent-based model with three species engaged in intransitive competition. We introduce an HOI where one species modifies the competition between the other two. We explore the impact of the strength, radius of influence, and speed of this interaction modification on species abundances and oscillations thereof. Our results show that these abundances are not only greatly impacted by the strength, but also by the radius and speed of the interaction modification. A deeper investigation demonstrates that the changes in the oscillations are caused by the interaction modification itself, and not the change in pairwise interaction strength caused by the HOI. These results emphasize the importance of considering the spatio-temporal scales of higher-order interactions when assessing ecosystem stability, highlighting that such interactions can introduce complex dynamical behaviors that go beyond the predictions of traditional pairwise or simpler higher-order models"}
{"id": "2601.14969", "pdf": "https://arxiv.org/pdf/2601.14969", "abs": "https://arxiv.org/abs/2601.14969", "authors": ["Yiyao Yang"], "title": "Robust Machine Learning for Regulatory Sequence Modeling under Biological and Technical Distribution Shifts", "categories": ["q-bio.GN", "stat.ML"], "comment": "19 pages, 16 figures", "summary": "Robust machine learning for regulatory genomics is studied under biologically and technically induced distribution shifts. Deep convolutional and attention based models achieve strong in distribution performance on DNA regulatory sequence prediction tasks but are usually evaluated under i.i.d. assumptions, even though real applications involve cell type specific programs, evolutionary turnover, assay protocol changes, and sequencing artifacts. We introduce a robustness framework that combines a mechanistic simulation benchmark with real data analysis on a massively parallel reporter assay (MPRA) dataset to quantify performance degradation, calibration failures, and uncertainty based reliability. In simulation, motif driven regulatory outputs are generated with cell type specific programs, PWM perturbations, GC bias, depth variation, batch effects, and heteroscedastic noise, and CNN, BiLSTM, and transformer models are evaluated. Models remain accurate and reasonably calibrated under mild GC content shifts but show higher error, severe variance miscalibration, and coverage collapse under motif effect rewiring and noise dominated regimes, revealing robustness gaps invisible to standard i.i.d. evaluation. Adding simple biological structural priors motif derived features in simulation and global GC content in MPRA improves in distribution error and yields consistent robustness gains under biologically meaningful genomic shifts, while providing only limited protection against strong assay noise. Uncertainty-aware selective prediction offers an additional safety layer that risk coverage analyses on simulated and MPRA data show that filtering low confidence inputs recovers low risk subsets, including under GC-based out-of-distribution conditions, although reliability gains diminish when noise dominates."}
{"id": "2601.14577", "pdf": "https://arxiv.org/pdf/2601.14577", "abs": "https://arxiv.org/abs/2601.14577", "authors": ["Ariel Bruner", "Mona Singh"], "title": "FBApro: A fast, simple linear transformation for diverse metabolic modeling tasks", "categories": ["q-bio.QM"], "comment": "19 pages, 9 figures", "summary": "Constraint-based metabolic modeling is the predominant framework for simulating cellular metabolism. The central assumption of these models is that metabolism operates at a steady state, meaning that the production and consumption rates of each metabolite are balanced. This assumption imposes linear constraints on the fluxes of biochemical reactions. Flux Balance Analysis (FBA), a fundamental method in the field, is formulated as an optimization problem maximizing a cellular objective (e.g., growth) over the resulting linear subspace of steady state fluxes. Many other methods in the field are expressed either as a modification to FBA, or use FBA as a black box within an algorithm. Here, we propose a simple and general alternative to optimization that, for any flux vector, finds the closest flux distribution within the steady-state subspace. This operation corresponds to an orthogonal projection that enforces the steady-state assumption. We further introduce extensions to handle cases involving unknown or fixed fluxes through modified projections and tailored affine subspaces. The overall approach is computationally efficient, does not require a cellular objective, and is easy to implement. We validate our method and its variants on both synthetic and experimental datasets, demonstrating their speed and utility for denoising and imputing metabolic flux data, and for predicting steady-state fluxes from more readily available types of data.\n  Code availability: The code implementing FBApro is available at https://github.com/Singh-Lab/FBApro. All code required to reproduce the figures in the paper is available, although the data used must be sourced separately. The repository also contains toy models and examples."}
{"id": "2601.15219", "pdf": "https://arxiv.org/pdf/2601.15219", "abs": "https://arxiv.org/abs/2601.15219", "authors": ["Mareike Fischer", "Tom Niklas Hamann", "Kristina Wicke"], "title": "A height-based metaconcept for rooted tree balance and its implications for the $B_1$ index", "categories": ["q-bio.PE", "math.CO"], "comment": null, "summary": "Tree balance has received considerable attention in recent years, both in phylogenetics and in other areas. Numerous (im)balance indices have been proposed to quantify the (im)balance of rooted trees. A recent comprehensive survey summarized this literature and showed that many existing indices are based on similar underlying principles. To unify these approaches, three general metaconcepts were introduced, providing a framework to classify, analyze, and extend imbalance indices. In this context, a metaconcept is a function $Φ_f$ that depends on another function $f$ capturing some aspect of tree shape. In this manuscript, we extend this line of research by introducing a new metaconcept based on the heights of the pending subtrees of all inner vertices. We provide a thorough analysis of this metaconcept and use it to answer open questions concerning the well-known $B_1$ balance index. In particular, we characterize the tree shapes that maximize the $B_1$ index in two cases: (i) arbitrary rooted trees and (ii) binary rooted trees. For both cases, we also determine the corresponding maximum values of the index.\n  Finally, while the $B_1$ index is induced by a so-called third-order metaconcept, we explicitly introduce three new (im)balance indices derived from the first- and second-order height metaconcepts, respectively, thereby demonstrating that pending subtree heights give rise to a variety of novel (im)balance indices."}
{"id": "2601.14536", "pdf": "https://arxiv.org/pdf/2601.14536", "abs": "https://arxiv.org/abs/2601.14536", "authors": ["Tiantian Yang", "Yuxuan Wang", "Zhenwei Zhou", "Ching-Ti Liu"], "title": "engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "comment": "21 pages, 14 figures, 5 tables", "summary": "Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts."}
{"id": "2601.15273", "pdf": "https://arxiv.org/pdf/2601.15273", "abs": "https://arxiv.org/abs/2601.15273", "authors": ["Paul Van Liedekerke", "Jiří Pešek", "Kevin Alessandri", "Dirk Drasdo"], "title": "How high-resolution agent-based models can improve fundamental insights in tissue development and cell culturing methods", "categories": ["q-bio.QM"], "comment": null, "summary": "The fundamental understanding of how cells physically interact with each other and their environment is key to understanding their organisation in living tissues. Over the past decades several computational methods have been developed to decipher emergent multi-cellular behaviors. In particular agent-based (or cell-based) models that consider the individual cell as basic modeling unit tracked in space and time enjoy increasing interest across scientific communities. In this article we explore a particular class of cell-based models, so-called Deformable Cell Models (DCMs), that allow to simulate the biophysics of the cell with high realism. After situating this model among other model types, We give an overview of past and recent DCM developments and discuss new simulation results of several applications covering in-vitro and in-vivo systems. Our goal is to demonstrate how such models can generate quantitative added value in biological and biotechnological problems."}
{"id": "2601.14632", "pdf": "https://arxiv.org/pdf/2601.14632", "abs": "https://arxiv.org/abs/2601.14632", "authors": ["Min-Kyung Chae", "Woo-Sik Son", "Sang Hoon Lee"], "title": "The missing links: Evaluating contact tracing with incomplete data in large metropolitan areas during an epidemic", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "q-bio.PE"], "comment": "13 pages, 8 figures, 1 table", "summary": "Contact tracing (CT) plays a pivotal role in controlling early epidemic spread, particularly when a novel infectious disease emerges. However, the quantitative impact of missing information -- such as untraced cases or unnotified contacts -- on the effectiveness of CT remains insufficiently understood. Using a stochastic agent-based model with sociodemographics from metropolitan areas in South Korea, we simulate how different forms of information loss affect epidemic spreading dynamics. We construct information-loss scenarios based on two types: infector-omission (IO) and contact-omission (CO), including selective (SCO) and uniform (UCO) scenarios; IO corresponds to the omission of infected individuals (nodes) from the tracing process, leading to the loss of all movement trajectories and downstream transmission links originating from them, whereas CO corresponds to the omission of specific contact events (edges), in which infected individuals are identified but some of their transmission links fail to be detected or notified. The sensitivity of epidemic dynamics to increasing omission rates differs markedly between the two types: IO scenarios exhibit substantially stronger and more abrupt changes in transmission structure and epidemic outcomes, whereas CO scenarios produce more gradual effects. In both scenarios, the magnitude of these effects varies across cities, with a lower-population city (Busan) showing greater tolerance to information loss than the largest city (Seoul), underscoring the importance of regional tailoring in CT strategies. Both IO and CO scenarios also lead to an increase in the transmission network diameter as information loss grows, indicating that a small network diameter reflects effective contact tracing that limits the depth of transmission chains."}
{"id": "2601.14653", "pdf": "https://arxiv.org/pdf/2601.14653", "abs": "https://arxiv.org/abs/2601.14653", "authors": ["Yuyu Liu", "Jiannan Yang", "Ziyang Yu", "Weishen Pan", "Fei Wang", "Tengfei Ma"], "title": "Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github."}
{"id": "2601.14795", "pdf": "https://arxiv.org/pdf/2601.14795", "abs": "https://arxiv.org/abs/2601.14795", "authors": ["Naomi Sasaya", "Shigefumi Kishida", "Ryo Kikuchi", "Akira Tajima"], "title": "Validating Behavioral Proxies for Disease Risk Monitoring via Large-Scale E-commerce Data", "categories": ["cs.SI", "q-bio.PE"], "comment": "12 pages, 6 figures. Cross-domain validation of behavioral disease proxies using large-scale e-commerce data", "summary": "Digital traces of everyday behavior, such as e-commerce (EC) purchase logs, provide scalable signals for population-level monitoring, yet their epidemiological validity remains unclear due to weak links to clinical outcomes.\n  We propose a behavioral proxy for disease onset based on transitions from regular to therapeutic diets observed in EC purchase histories, and evaluate its validity through large-scale cross-domain analysis. Using EC purchase data (N = 55,645 users) and independent insurance-derived clinical records, we compare ingredient-level risk patterns and seasonal disease dynamics in feline lower urinary tract disease (FLUTD) as a case study.\n  The proxy-based estimates show strong agreement with clinical data, with correlations of r = 0.74 for ingredient-level risk patterns and r = 0.82 for seasonal variation. Both data sources consistently capture elevated disease risk during winter months. Moreover, analysis using EC data alone reproduces established domain knowledge, including the association between higher wet food consumption and lower disease risk.\n  Our results demonstrate that behavioral signals derived from large-scale EC data can serve as validated, cost-effective complements to traditional surveillance systems, and suggest broader applicability to monitoring lifestyle-related and chronic conditions."}
