{"id": "2602.15447", "pdf": "https://arxiv.org/pdf/2602.15447", "abs": "https://arxiv.org/abs/2602.15447", "authors": ["Seba Contreras", "Philipp Dönges", "Maciej Filinski", "Joel Wagner", "Viktor Bezborodov", "Marcin Bodych", "Barbara Pabjan", "Franciszek Rakowski", "Jan Pablo Burgard", "Tyll Krueger", "Viola Priesemann"], "title": "Household size can explain 40% of the variance in cumulative COVID-19 incidence across Europe", "categories": ["q-bio.PE", "math.DS", "physics.soc-ph"], "comment": null, "summary": "Household size impacts the spread of respiratory infectious diseases: Larger households tend to boost transmission by acquiring external infections more frequently and subsequently transmitting them back into the community. Furthermore, mandatory interventions primarily modulate contagion between households rather than within them. We developed an approach to quantify the role of household size in epidemics by separating within-household from out-household transmission, and found that household size explains 41% of the variability in cumulative COVID-19 incidence across 34 European countries (95% confidence interval: [15%, 46%]). The contribution of households to the overall dynamics can be quantified by a boost factor that increases with the effective household size, implying that countries with larger households require more stringent interventions to achieve the same levels of containment. This suggests that households constitute a structural (dis-)advantage that must be considered when designing and evaluating mitigation strategies."}
{"id": "2602.15451", "pdf": "https://arxiv.org/pdf/2602.15451", "abs": "https://arxiv.org/abs/2602.15451", "authors": ["Hayato Kunugi", "Mohsen Rahmani", "Yosuke Iyama", "Yutaro Hirono", "Akira Suma", "Matthew Woolway", "Vladimir Vargas-Calderón", "William Kim", "Kevin Chern", "Mohammad Amin", "Masaru Tateno"], "title": "Molecular Design beyond Training Data with Novel Extended Objective Functionals of Generative AI Models Driven by Quantum Annealing Computer", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "quant-ph"], "comment": "42 pages, 7 figures", "summary": "Deep generative modeling to stochastically design small molecules is an emerging technology for accelerating drug discovery and development. However, one major issue in molecular generative models is their lower frequency of drug-like compounds. To resolve this problem, we developed a novel framework for optimization of deep generative models integrated with a D-Wave quantum annealing computer, where our Neural Hash Function (NHF) presented herein is used both as the regularization and binarization schemes simultaneously, of which the latter is for transformation between continuous and discrete signals of the classical and quantum neural networks, respectively, in the error evaluation (i.e., objective) function. The compounds generated via the quantum-annealing generative models exhibited higher quality in both validity and drug-likeness than those generated via the fully-classical models, and was further indicated to exceed even the training data in terms of drug-likeness features, without any restraints and conditions to deliberately induce such an optimization. These results indicated an advantage of quantum annealing to aim at a stochastic generator integrated with our novel neural network architectures, for the extended performance of feature space sampling and extraction of characteristic features in drug design."}
{"id": "2602.15787", "pdf": "https://arxiv.org/pdf/2602.15787", "abs": "https://arxiv.org/abs/2602.15787", "authors": ["James Malkin", "Cian O'Donnell", "Conor Houghton"], "title": "Energy budgets govern synaptic precision and its regulation during plasticity", "categories": ["q-bio.NC"], "comment": "39 pages, 6 figures", "summary": "Synaptic transmission must balance the need for reliable signalling against the metabolic cost of achieving that reliability. How energetic constraints shape synaptic precision and its regulation during plasticity remains unclear. Here we develop an energy--constrained framework in which synapses minimise postsynaptic response variance subject to a fixed mean and an effective energy budget. Combinations of candidate physiological costs are used to estimate an energy cost for synaptic transmission; this cost is then inferred from quantal statistics. Analysing five published pre- and post-plasticity datasets, we find that observed synaptic mean--variance pairs cluster near a minimal-energy boundary, indicating that precision is limited by energetic availability. Model comparison identifies a dominant calcium pump-like cost paired with a smaller vesicle turnover-like cost, yielding a separable precision--energy relationship, $σ^{-2} \\propto E^5$. We further show that plasticity systematically updates synaptic energy budgets according to the scale-free magnitude of mean change, enabling accurate prediction of post-plasticity variance from energy allocation alone. These results provide direct experimental support for the hypothesis that synaptic precision is governed by energy budgets, establishing energy allocation as a fundamental principle linking metabolic constraints, synaptic reliability, and plasticity."}
{"id": "2602.15677", "pdf": "https://arxiv.org/pdf/2602.15677", "abs": "https://arxiv.org/abs/2602.15677", "authors": ["Neelay Velingker", "Alaia Solko-Breslin", "Mayank Keoliya", "Seewon Choi", "Jiayi Xin", "Anika Marathe", "Alireza Oraii", "Rajat Deo", "Sameed Khatana", "Rajeev Alur", "Mayur Naik", "Eric Wong"], "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events", "categories": ["cs.LG", "q-bio.QM"], "comment": "24 pages, 6 figures", "summary": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs)."}
{"id": "2602.15266", "pdf": "https://arxiv.org/pdf/2602.15266", "abs": "https://arxiv.org/abs/2602.15266", "authors": ["Pablo Padilla", "Oliver López-Corona", "Elvia Ramírez-Carrillo", "Ariadne Hernández Sánchez"], "title": "A golden-ratio partition of information and the balance between prediction and surprise: a neuro-cognitive route to antifragility", "categories": ["math.DS", "q-bio.NC"], "comment": "16 pages, 2 figures", "summary": "Adaptive systems must strike a balance between prediction and surprise to thrive in uncertain environments. We propose an information-theoretic balance function, $ f(p) = -(1 - p)\\ln(1 - p) + \\ln p $, which quantifies the net informational gain from contrasting explained variance $p$ with unexplained novelty $(1 - p)$. This function is strictly concave on $(0,1)$ and reaches its unique maximum at $ p^* \\approx 0.882$, revealing a regime where confidence is high but the residual uncertainty carries a disproportionate potential for surprise.\n  Independently of this maximum, imposing a self-similarity condition between known, unknown and total information, $p : (1-p) = 1 : p$, leads to the golden-ratio reciprocal $p = 1/\\varphi \\approx 0.618$, where $ \\varphi$ is the golden ratio. We interpret this value not as the maximizer of $f$, but as a structurally privileged \\emph{partition} in which known and unknown are proportionally nested across scales.\n  Embedding this dual structure into a Compute-Inference-Model-Action (CIMA) loop yields a dynamic process that maintains the system near a critical regime where prediction and surprise coexist. At this edge, neuronal dynamics exhibit power-law structure and maximal dynamic range, while the system's response to perturbations becomes convex at the level of its payoff function-fulfilling the formal definition of antifragility. We suggest that the golden-ratio partition is not merely a mathematical artifact, but a candidate design principle linking prediction, surprise, criticality, and antifragile adaptation across scales and domains, while the maximum of $f$ identifies the point of greatest informational vulnerability to being wrong."}
{"id": "2602.15740", "pdf": "https://arxiv.org/pdf/2602.15740", "abs": "https://arxiv.org/abs/2602.15740", "authors": ["Fatemeh Khalvandi", "Saadat Izadi", "Abdolah Chalechale"], "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "27 pages, 10 figures, 10 table", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis."}
