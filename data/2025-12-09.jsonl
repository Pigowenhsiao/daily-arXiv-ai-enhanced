{"id": "2512.06496", "pdf": "https://arxiv.org/pdf/2512.06496", "abs": "https://arxiv.org/abs/2512.06496", "authors": ["Stella Brown", "Nicolas Preisig", "Autumn Davis", "Brian Hutchinson", "Filip Jagodzinski"], "title": "PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning", "categories": ["q-bio.BM", "cs.AI", "cs.LG", "cs.NE", "q-bio.QM"], "comment": "Presented at Computational Structural Bioinformatics Workshop 2025", "summary": "Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects."}
{"id": "2512.06592", "pdf": "https://arxiv.org/pdf/2512.06592", "abs": "https://arxiv.org/abs/2512.06592", "authors": ["James King", "Lewis Cornwall", "Andrei Cristian Nica", "James Day", "Aaron Sim", "Neil Dalchau", "Lilly Wollman", "Joshua Meyers"], "title": "On fine-tuning Boltz-2 for protein-protein affinity prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": "MLSB 2025", "summary": "Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction."}
{"id": "2512.07692", "pdf": "https://arxiv.org/pdf/2512.07692", "abs": "https://arxiv.org/abs/2512.07692", "authors": ["Franz Görlich", "Julija Zavadlav"], "title": "Mapping Still Matters: Coarse-Graining with Machine Learning Potentials", "categories": ["physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Coarse-grained (CG) modeling enables molecular simulations to reach time and length scales inaccessible to fully atomistic methods. For classical CG models, the choice of mapping, that is, how atoms are grouped into CG sites, is a major determinant of accuracy and transferability. At the same time, the emergence of machine learning potentials (MLPs) offers new opportunities to build CG models that can in principle learn the true potential of the mean force for any mapping. In this work, we systematically investigate how the choice of mapping influences the representations learned by equivariant MLPs by studying liquid hexane, amino acids, and polyalanine. We find that when the length scales of bonded and nonbonded interactions overlap, unphysical bond permutations can occur. We also demonstrate that correctly encoding species and maintaining stereochemistry are crucial, as neglecting either introduces unphysical symmetries. Our findings provide practical guidance for selecting CG mappings compatible with modern architectures and guide the development of transferable CG models."}
{"id": "2512.07113", "pdf": "https://arxiv.org/pdf/2512.07113", "abs": "https://arxiv.org/abs/2512.07113", "authors": ["Kepeng Lin", "Qizhe Zhang", "Rui Wang", "Xuehai Hu", "Wei Xu"], "title": "PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes", "categories": ["cs.LG", "q-bio.GN"], "comment": "6 pages, 5 figures, accept to BIBM", "summary": "Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE"}
{"id": "2512.06808", "pdf": "https://arxiv.org/pdf/2512.06808", "abs": "https://arxiv.org/abs/2512.06808", "authors": ["Leonor V. C. Losa", "Temple A. Douglas", "Lia Santos", "Raquel Monteiro", "Isabel Calejo", "Raphael F. Canadas", "Jana B. Nieder"], "title": "Leveraging Pre-trained Neural Network Models for the Classification of Tumor Cells Analyzed by Label-free Phase Holotomographic Microscopy", "categories": ["physics.optics", "physics.app-ph", "physics.bio-ph", "q-bio.CB"], "comment": "20 pages, 7 figures, 3 tables, original research article", "summary": "Can a single label-free image reveal whether cancer cells were exposed to chemotherapy? We present an innovative methodology on the label-free and high-resolution imaging properties of phase holotomographic microscopy coupled with neural network models for the classification of cancer cells. Using 3D phase holotomographic microscopy, we imaged live A549 lung cancer cells with and without paclitaxel, converted stacks to 2D maximum-intensity projections, and evaluated pre-trained convolutional networks (VGG16, ResNet18, DenseNet121, and EfficientNet-B0) for binary classification of treatment status. EfficientNet-B0 achieved 96.9 % accuracy on unsegmented images. Refractive index analysis revealed bimodal distribution in treated cells, reflecting heterogeneous biophysical responses to paclitaxel exposure and supporting the network's ability to detect subtle, label-free indicators of drug action. As further proof-of-concept, the same pipeline separated holotomographic images of label-free, high versus low-graded urothelial cancer cells with high accuracy (90.6 %). These findings highlight the potential of integrating label-free holotomographic imaging with deep learning techniques for rapid and efficient classification of tumor cells, paving the way for advancements in treatment optimization and personalized diagnostic strategies."}
{"id": "2512.06294", "pdf": "https://arxiv.org/pdf/2512.06294", "abs": "https://arxiv.org/abs/2512.06294", "authors": ["Quentin Badolle", "Arthur Theuer", "Zhou Fang", "Ankit Gupta", "Mustafa Khammash"], "title": "Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability", "categories": ["q-bio.MN", "cs.LG", "math.PR", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations."}
{"id": "2512.07413", "pdf": "https://arxiv.org/pdf/2512.07413", "abs": "https://arxiv.org/abs/2512.07413", "authors": ["Allowen Evin", "Carly Ameen", "Colline Brassard", "Sophie Dennis", "Ekaterina E Antipina", "Vincent Bonhomme", "Myriam Boudadi-Maligne", "Kate Britton", "Francisco Gil Cano", "Ruth F Carden", "Julien Claude", "Lídia Colominas", "Stefan Curth", "Sergey Egorovich Fedorov", "Joan Frances", "Daniela C Kalthoff", "Andrew C Kitchener", "Rick Knecht", "Pavel Kosintsev", "Anna Linderholm", "Robert Losey", "Ilia Merts", "Viktor Merts", "Maria Mostadius", "Mark Omura", "Vedat Onar", "Alan K Outram", "Joris Peters", "André Rehazek", "Erika Rosengren", "Mikhail Sablin", "Paul Sciulli", "Maria Seguí", "Z. Jack Tseng", "Emma Usmanova", "Victor Varfolomeev", "Susan Crockford", "Yaroslav Kuzmin", "Laurent Frantz", "Keith Dobney", "Greger Larson"], "title": "The emergence and diversification of dog morphology", "categories": ["q-bio.PE"], "comment": null, "summary": "Dogs exhibit an exceptional range of morphological diversity as a result of their long-term association with humans. Attempts to identify when dog morphological variation began to expand have been constrained by the limited number of Pleistocene specimens, the fragmentary nature of remains, and difficulties in distinguishing early dogs from wolves on the basis of skeletal morphology. In this study, we used three-dimensional geometric morphometrics to analyze the size and shape of 643 canid crania spanning the past 50,000 years. Our analyses show that a distinctive dog morphology first appeared at about 11,000 calibrated years before present, and substantial phenotypic diversity already existed in early Holocene dogs. Thus, this variation emerged many millennia before the intense human-mediated selection shaping modern dog breeds beginning in the 19th century."}
{"id": "2512.06064", "pdf": "https://arxiv.org/pdf/2512.06064", "abs": "https://arxiv.org/abs/2512.06064", "authors": ["Nasla Saleem", "Talukder Zaki Jubery", "Yan Zhou", "Yawei Li", "Adarsh Krishnamurthy", "Patrick S. Schnable", "Baskar Ganapathysubramanian"], "title": "Algorithmic design of \"smart canopy\" maize architectures that maximize light use efficiency", "categories": ["q-bio.QM", "q-bio.PE"], "comment": "13 pages, 4 figures, 1 table", "summary": "We present a computational framework that integrates functional-structural plant modeling (FSPM) with an evolutionary algorithm to optimize three-dimensional maize canopy architecture for enhanced light interception under high-density planting. The optimization revealed an emergent ideotype characterized by two distinct strategies: a vertically stratified leaf profile (steep, narrow upper leaves for penetration; broad, horizontal lower leaves for capture) and a radially tiled azimuthal arrangement that breaks the conventional distichous symmetry of maize to minimize self and mutual shading. Reverse ray-tracing simulations show that this architecture intercepts significantly more photosynthetically active radiation (PAR) than virtual canopies parameterized from high-performing field hybrids, with gains that generalize across multiple U.S. latitudes and planting densities. The optimized trait combinations align with characteristics of modern density-tolerant cultivars, supporting biological plausibility. Because recent gene editing advances enable more independent control of architectural traits, the designs identified here are increasingly feasible. By uncovering effective, non-intuitive trait configurations, our approach provides a scalable, predictive tool to guide breeding targets, improve light-use efficiency, and ultimately support sustainable yield gains."}
{"id": "2512.06280", "pdf": "https://arxiv.org/pdf/2512.06280", "abs": "https://arxiv.org/abs/2512.06280", "authors": ["Azar Ghahari", "Uri T. Eden"], "title": "Assessing the Information Content of Individual Spikes in Population-Level Models of Neural Spiking Activity", "categories": ["q-bio.NC", "q-bio.QM", "stat.ME"], "comment": "Manuscript, 25 pages, 8 figures; preprint", "summary": "In the last decade, there have been major advances in clusterless decoding algorithms for neural data analysis. These algorithms use the theory of marked point processes to describe the joint activity of many neurons simultaneously, without the need for spike sorting. In this study, we examine information-theoretic metrics to analyze the information extracted from each observed spike under such clusterless models. In an analysis of spatial coding in the rat hippocampus, we compared the entropy reduction between spike-sorted and clusterless models for both individual spikes observed in isolation and when the prior information from all previously observed spikes is accounted for. Our analysis demonstrates that low-amplitude spikes, which are difficult to cluster and often left out of spike sorting, provide reduced information compared to sortable, high-amplitude spikes when considered in isolation, but the two provide similar levels of information when considering all the prior information available from past spiking. These findings demonstrate the value of combining information measures with state-space modeling and yield new insights into the underlying mechanisms of neural computation."}
{"id": "2512.07116", "pdf": "https://arxiv.org/pdf/2512.07116", "abs": "https://arxiv.org/abs/2512.07116", "authors": ["Renlei Jiang", "Chuanhou Gao", "Denis Dochain"], "title": "Structure-conditioned input-to-state stability for layer-by-layer molecular computations in parallel chemical reaction networks", "categories": ["q-bio.MN", "math.DS"], "comment": null, "summary": "Molecular computation in chemical reaction networks (CRNs) now constitutes a foundational framework for designing programmable biological systems. However, prevailing design methodologies primarily treat parallelism of chemical reactions as a liability, consequently motivating researchers to redirect research focus toward leveraging parallelism to implement layer-by-layer computations of composite functions in coupled mass-action systems (MASs). MASs exhibiting this property are termed composable. Present composability verification for MASs mainly depends on input-to-state stability (ISS) conditions, with structural characteristics of networks remaining underexplored. This paper investigates the structural conditions under which two MASs are composable. By leveraging ISS-Lyapunov functions, we identify a class of CRN architectures, whose reduced systems have zero deficiency, that guarantee composability with other networks. We also extend our conclusions to encompass some CRN architectures possessing nonzero deficiency. Some examples are presented to demonstrate the validity of our theoretical results. Finally, we employ our methods to devise an algorithm for constructing MASs capable of executing specified molecular computations."}
{"id": "2512.06021", "pdf": "https://arxiv.org/pdf/2512.06021", "abs": "https://arxiv.org/abs/2512.06021", "authors": ["Nuruzzaman Rahat", "Abid Hossain", "Muntasir Alam"], "title": "Investigating the effect of adaptive optimal control function in epidemic dynamics: predictions and strategy evolution based on SIR/V game theoretic framework", "categories": ["physics.soc-ph", "math.DS", "q-bio.PE"], "comment": null, "summary": "In this paper, we consider an adaptive optimal control problem for an SIR/V epidemic model with human behavioral effects.We develop a model where effective management of infectious diseases are monitored by the means of non pharmaceutical interventions.This study develops an adaptive optimal control function within an SIR/V framework embedding a non cooperative game theoretic mechanism to capture the dynamic interplay between individual vaccination behavior and population level transmission. We derive analytical expression for the optimal control trajectory under resource constrain and heterogeneous susceptibility and we validate our model using numerical simulations,calibrated with the real world epidemic parameters. We find that for the adaptive optimal policy for a generally known SIR/V model depending on the game theoretic epidemic state leads to substantial reduction in expenses compared to non adaptive policies. Moreover, our results demonstrate that, adaptive strategies significantly outperform the static policies by achieving lower peak infections and faster epidemic extinctions while evolutionary game dynamics identify critical behavioral thresholds that drive strategy evolution and inform timely policy adaptation"}
{"id": "2512.06181", "pdf": "https://arxiv.org/pdf/2512.06181", "abs": "https://arxiv.org/abs/2512.06181", "authors": ["Yanuo Zhou"], "title": "Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data", "categories": ["q-bio.QM", "cs.LG"], "comment": "16 pages, 8 figures. Reproducible pipeline for classifying biologically light from wearable spectral data. Manuscript in preparation for journal submission", "summary": "Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.\n  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.\n  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.\n  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.\n  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking."}
{"id": "2512.06492", "pdf": "https://arxiv.org/pdf/2512.06492", "abs": "https://arxiv.org/abs/2512.06492", "authors": ["Yujian Xiong", "Negar Jalili Mallak", "Yanshuai Tu", "Zhong-Lin Lu", "Yalin Wang"], "title": "Quantification of Planar Cortical Magnification with Optimal Transport and Topological Smoothing", "categories": ["q-bio.NC"], "comment": null, "summary": "The human visual system exhibits non-uniform spatial resolution across the visual field, which is characterized by the cortical magnification factor (CMF) that reflects its anatomical basis. However, current approaches for quantifying CMF using retinotopic maps derived from BOLD functional magnetic resonance imaging (fMRI) are limited by the inherent low signal-to-noise ratio of fMRI data and inaccuracies in the topological relationships of the retinotopic maps. In this study, we introduced a new pipeline to quantify planar CMF from retinotopic maps generated from the population receptive field (pRF) model. The pipeline projected the 3D pRF solutions onto a 2D planar disk, using optimal transport (OT) to preserve local cortical surface areas, and applied topological smoothing to ensure that the resulting retinotopic maps maintain their topology. We then estimated 2D CMF maps from the projected retinotopic maps on the planar disk using the 1-ring patch method. Applying this pipeline to the Human Connectome Project (HCP) 7T dataset, we revealed previously unobserved CMF patterns across the visual field and demonstrated individual differences among the 181 subjects. The pipeline was further validated on the New York University (NYU) 3T dataset, showing reliable and repeatable results. Our study provided new analytical methods and offered novel insights into visual processing."}
{"id": "2512.06064", "pdf": "https://arxiv.org/pdf/2512.06064", "abs": "https://arxiv.org/abs/2512.06064", "authors": ["Nasla Saleem", "Talukder Zaki Jubery", "Yan Zhou", "Yawei Li", "Adarsh Krishnamurthy", "Patrick S. Schnable", "Baskar Ganapathysubramanian"], "title": "Algorithmic design of \"smart canopy\" maize architectures that maximize light use efficiency", "categories": ["q-bio.QM", "q-bio.PE"], "comment": "13 pages, 4 figures, 1 table", "summary": "We present a computational framework that integrates functional-structural plant modeling (FSPM) with an evolutionary algorithm to optimize three-dimensional maize canopy architecture for enhanced light interception under high-density planting. The optimization revealed an emergent ideotype characterized by two distinct strategies: a vertically stratified leaf profile (steep, narrow upper leaves for penetration; broad, horizontal lower leaves for capture) and a radially tiled azimuthal arrangement that breaks the conventional distichous symmetry of maize to minimize self and mutual shading. Reverse ray-tracing simulations show that this architecture intercepts significantly more photosynthetically active radiation (PAR) than virtual canopies parameterized from high-performing field hybrids, with gains that generalize across multiple U.S. latitudes and planting densities. The optimized trait combinations align with characteristics of modern density-tolerant cultivars, supporting biological plausibility. Because recent gene editing advances enable more independent control of architectural traits, the designs identified here are increasingly feasible. By uncovering effective, non-intuitive trait configurations, our approach provides a scalable, predictive tool to guide breeding targets, improve light-use efficiency, and ultimately support sustainable yield gains."}
{"id": "2512.06116", "pdf": "https://arxiv.org/pdf/2512.06116", "abs": "https://arxiv.org/abs/2512.06116", "authors": ["Y. Park", "F. Wu", "X. Feng", "S. Yang", "E. H. Wang", "B. Yao", "C. Moon", "G. Xiao", "Q. Li"], "title": "Spatial Analysis for AI-segmented Histopathology Images: Methods and Implementation", "categories": ["stat.AP", "q-bio.QM"], "comment": "33 pages, 2 figures", "summary": "Quantitatively characterizing the spatial organization of cells and their interaction is essential for understanding cancer progression and immune response. Recent advances in machine intelligence have enabled large-scale segmentation and classification of cell nuclei from digitized histopathology slides, generating massive point pattern and marked point pattern datasets. However, accessible tools for quantitative analysis of such complex cellular spatial organization remain limited. In this paper, we first review 27 traditional spatial summary statistics, areal indices, and topological features applicable to point pattern data. Then, we introduce SASHIMI (Spatial Analysis for Segmented Histopathology Images using Machine Intelligence), a browser-based tool for real-time spatial analysis of artificial intelligence (AI)-segmented histopathology images. SASHIMI computes a comprehensive suite of mathematically grounded descriptors, including spatial statistics, proximity-based measures, grid-level similarity indices, spatial autocorrelation measures, and topological descriptors, to quantify cellular abundance and cell-cell interaction. Applied to two cancer datasets, oral potentially malignant disorders (OPMD) and non-small-cell lung cancer (NSCLC), SASHIMI identified multiple spatial features significantly associated with patient survival outcomes. SASHIMI provides an accessible and reproducible platform for single-cell-level spatial profiling of tumor morphological architecture, offering a robust framework for quantitative exploration of tissue organization across cancer types."}
{"id": "2512.06934", "pdf": "https://arxiv.org/pdf/2512.06934", "abs": "https://arxiv.org/abs/2512.06934", "authors": ["Jiangping Xie", "Ruohan Ren", "Xiao Zhou", "Ao Zheng", "Jiasong Zhu", "Wenyu Jiang", "Ziran Zhao"], "title": "Visual Function Profiles via Multi-Path Aggregation Reveal Neuron-Level Responses in the Drosophila Brain", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Accurately predicting individual neurons' responses and spatial functional properties in complex visual tasks remains a key challenge in understanding neural computation. Existing whole-brain connectome models of Drosophila often rely on parameter assumptions or deep learning approaches, yet remain limited in their ability to reliably predict dynamic neuronal responses. We introduce a Multi-Path Aggregation (MPA) framework, based on neural network steady-state theory, to build a whole-brain Visual Function Profiles (VFP) of Drosophila neurons and predict their responses under diverse visual tasks. Unlike conventional methods relying on redundant parameters, MPA combines visual input features with the whole-brain connectome topology. It uses adjacency matrix powers and finite-path optimization to efficiently predict neuronal function, including ON/OFF polarity, direction selectivity, and responses to complex visual stimuli. Our model achieves a Pearson correlation of 0.84+/-0.12 for ON/OFF responses, outperforming existing methods (0.33+/-0.59), and accurately captures neuron functional properties, including luminance and direction preferences, while allowing single-neuron or population-level blockade simulations. Replacing CNN modules with VFP-derived Lobula Columnar(LC) population responses in a Drosophila simulation enables successful navigation and obstacle avoidance, demonstrating the model's effectiveness in guiding embodied behavior. This study establishes a \"connectome-functional profile-behavior\" framework, offering a whole-brain quantitative tool to study Drosophila visual computation and a neuron-level guide for brain-inspired intelligence."}
{"id": "2512.06245", "pdf": "https://arxiv.org/pdf/2512.06245", "abs": "https://arxiv.org/abs/2512.06245", "authors": ["Chad M. Topaz"], "title": "How Conflict Aversion Can Enable Authoritarianism: An Evolutionary Dynamics Approach", "categories": ["physics.soc-ph", "nlin.AO", "q-bio.PE"], "comment": null, "summary": "We use evolutionary game theory to examine how conflict-averse centrism can unintentionally facilitate authoritarian success in polarized political conflicts. Many such conflicts are asymmetric: authoritarian actors can employ norm-breaking or coercive tactics, while democratic resistance faces stronger constraints on what counts as normatively acceptable behavior. Yet formal models typically treat opposing sides symmetrically and rarely examine conflict-averse behavior. Drawing on empirical research on protest backlash, civility norms, and authoritarian resilience, we model these dynamics as a three-strategy evolutionary game in which resistance, authoritarianism, and conflict-averse centrism interact under replicator dynamics. This framework yields two distinct outcomes -- cyclic resurgence of authoritarian strength through a heteroclinic cycle and a stable centrist-authoritarian coalition that excludes resistance -- depending on how actors respond to confrontation. The analysis shows how payoff differences can reorganize long-run dynamics in asymmetric conflicts. Our contribution is to demonstrate how an established dynamical framework, combined with empirically grounded behavioral assumptions, clarifies the strategic conditions under which conflict aversion can diminish the effectiveness of democratic resistance."}
{"id": "2512.06134", "pdf": "https://arxiv.org/pdf/2512.06134", "abs": "https://arxiv.org/abs/2512.06134", "authors": ["Georgi Hrusanov", "Duy-Thanh Vu", "Duy-Cat Can", "Sophie Tascedda", "Margaret Ryan", "Julien Bodelet", "Katarzyna Koscielska", "Carsten Magnus", "Oliver Y. Chén"], "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression."}
{"id": "2512.05990", "pdf": "https://arxiv.org/pdf/2512.05990", "abs": "https://arxiv.org/abs/2512.05990", "authors": ["Xin Li"], "title": "Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \\textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \\textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \\textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \\textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \\textbf{Search $\\to$ Closure $\\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \\textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \\textit{Dynamic Programming} in P) via the mechanism of \\textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance."}
{"id": "2512.06961", "pdf": "https://arxiv.org/pdf/2512.06961", "abs": "https://arxiv.org/abs/2512.06961", "authors": ["Thibaut Arnoulx de Pirey"], "title": "Self-organized criticality in complex model ecosystems", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "q-bio.PE"], "comment": null, "summary": "We show that spatial extensions of many-species population dynamics models, such as the Lotka-Volterra model with random interactions we focus on in this work, generically exhibit scale-free correlation functions of population sizes in the limit of an infinite number of species. Using dynamical mean-field theory, we describe the many-species system in terms of single-species dynamics with demographic and environmental noises. We show that the single-species model features a random mass term, or equivalently a random space-time averaged growth rate, poising some species very close to extinction. This introduces a hierarchy of ever larger correlation times and lengths as the extinction threshold is approached. In turn, every species, even those far from extinction, are coupled to these near-critical fields which combine to make fluctuations of population sizes generically scale-free. We argue that these correlations are described by exponents derived from those of directed percolation in spatial dimension $d=3$, but not in lower dimensions."}
{"id": "2512.06141", "pdf": "https://arxiv.org/pdf/2512.06141", "abs": "https://arxiv.org/abs/2512.06141", "authors": ["Farzad Molani", "Art E. Cho"], "title": "Synergistic Computational Approaches for Accelerated Drug Discovery: Integrating Quantum Mechanics, Statistical Thermodynamics, and Quantum Computing", "categories": ["physics.chem-ph", "q-bio.QM", "quant-ph"], "comment": null, "summary": "Accurately predicting protein-ligand binding free energies (BFEs) remains a central challenge in drug discovery, particularly because the most reliable methods, such as free energy perturbation (FEP), are computationally intensive and difficult to scale. Here, we introduce a hybrid quantum-classical framework that combines Mining Minima sampling with quantum mechanically refined ligand partial charges, QM/MM interaction evaluation, and variational quantum eigensolver (VQE)-based electronic energy correction. This design enables explicit treatment of polarization, charge redistribution, and electronic correlation effects that are often underestimated in purely classical scoring schemes, while retaining computational efficiency. Across 23 protein targets and 543 ligands, the method achieves a mean absolute error of about 1.10 kcal/mol with strong rank-order fidelity (Pearson R = 0.75, Spearman rho = 0.76, Kendall tau = 0.57), consistent with the performance of contemporary FEP protocols. Notably, the workflow requires only about 25 minutes per ligand on standard compute resources, resulting in an approximate 20-fold reduction in computational cost relative to alchemical free energy approaches. This level of accuracy and efficiency makes the method well-suited for high-throughput lead optimization and iterative design cycles in pharmaceutical discovery. The framework also provides a natural foundation for future integration with machine learning models to enable predictive, large-scale, and adaptive screening strategies."}
{"id": "2512.06134", "pdf": "https://arxiv.org/pdf/2512.06134", "abs": "https://arxiv.org/abs/2512.06134", "authors": ["Georgi Hrusanov", "Duy-Thanh Vu", "Duy-Cat Can", "Sophie Tascedda", "Margaret Ryan", "Julien Bodelet", "Katarzyna Koscielska", "Carsten Magnus", "Oliver Y. Chén"], "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression."}
{"id": "2512.06280", "pdf": "https://arxiv.org/pdf/2512.06280", "abs": "https://arxiv.org/abs/2512.06280", "authors": ["Azar Ghahari", "Uri T. Eden"], "title": "Assessing the Information Content of Individual Spikes in Population-Level Models of Neural Spiking Activity", "categories": ["q-bio.NC", "q-bio.QM", "stat.ME"], "comment": "Manuscript, 25 pages, 8 figures; preprint", "summary": "In the last decade, there have been major advances in clusterless decoding algorithms for neural data analysis. These algorithms use the theory of marked point processes to describe the joint activity of many neurons simultaneously, without the need for spike sorting. In this study, we examine information-theoretic metrics to analyze the information extracted from each observed spike under such clusterless models. In an analysis of spatial coding in the rat hippocampus, we compared the entropy reduction between spike-sorted and clusterless models for both individual spikes observed in isolation and when the prior information from all previously observed spikes is accounted for. Our analysis demonstrates that low-amplitude spikes, which are difficult to cluster and often left out of spike sorting, provide reduced information compared to sortable, high-amplitude spikes when considered in isolation, but the two provide similar levels of information when considering all the prior information available from past spiking. These findings demonstrate the value of combining information measures with state-space modeling and yield new insights into the underlying mechanisms of neural computation."}
{"id": "2512.06294", "pdf": "https://arxiv.org/pdf/2512.06294", "abs": "https://arxiv.org/abs/2512.06294", "authors": ["Quentin Badolle", "Arthur Theuer", "Zhou Fang", "Ankit Gupta", "Mustafa Khammash"], "title": "Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability", "categories": ["q-bio.MN", "cs.LG", "math.PR", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations."}
{"id": "2512.06496", "pdf": "https://arxiv.org/pdf/2512.06496", "abs": "https://arxiv.org/abs/2512.06496", "authors": ["Stella Brown", "Nicolas Preisig", "Autumn Davis", "Brian Hutchinson", "Filip Jagodzinski"], "title": "PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning", "categories": ["q-bio.BM", "cs.AI", "cs.LG", "cs.NE", "q-bio.QM"], "comment": "Presented at Computational Structural Bioinformatics Workshop 2025", "summary": "Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects."}
{"id": "2512.06502", "pdf": "https://arxiv.org/pdf/2512.06502", "abs": "https://arxiv.org/abs/2512.06502", "authors": ["Muhammed Muaaz Dawood", "Mohammad Zaid Moonsamy", "Kaela Kokkas", "Hairong Wang", "Robert F. Breiman", "Richard Klein", "Emmanuel K. Sekyi", "Bruce A. Bassett"], "title": "Small Language Models Can Use Nuanced Reasoning For Health Science Research Classification: A Microbial-Oncogenesis Case Study", "categories": ["cs.CE", "q-bio.QM"], "comment": "43 pages, 7 figures", "summary": "Artificially intelligent (AI) co-scientists must be able to sift through research literature cost-efficiently while applying nuanced scientific reasoning. We evaluate Small Language Models (SLMs, <= 8B parameters) for classifying medical research papers. Using literature on the oncogenic potential of HMTV/MMTV-like viruses in breast cancer as a case study, we assess model performance with both zero-shot and in-context learning (ICL; few-shot prompting) strategies against frontier proprietary Large Language Models (LLMs). Llama 3 and Qwen2.5 outperform GPT-5 (API, low/high effort), Gemini 3 Pro Preview, and Meerkat in zero-shot settings, though trailing Gemini 2.5 Pro. ICL leads to improved performance on a case-by-case basis, allowing Llama 3 and Qwen2.5 to match Gemini 2.5 Pro in binary classification. Systematic lexical-ablation experiments show that SLM decisions are often grounded in valid scientific cues but can be influenced by spurious textual artifacts, underscoring need for interpretability in high-stakes pipelines. Our results reveal both promise and limitations of modern SLMs for scientific triage; pairing SLMs with simple but principled prompting strategies can approach performance of the strongest LLMs for targeted literature filtering in co-scientist pipelines."}
{"id": "2512.06934", "pdf": "https://arxiv.org/pdf/2512.06934", "abs": "https://arxiv.org/abs/2512.06934", "authors": ["Jiangping Xie", "Ruohan Ren", "Xiao Zhou", "Ao Zheng", "Jiasong Zhu", "Wenyu Jiang", "Ziran Zhao"], "title": "Visual Function Profiles via Multi-Path Aggregation Reveal Neuron-Level Responses in the Drosophila Brain", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Accurately predicting individual neurons' responses and spatial functional properties in complex visual tasks remains a key challenge in understanding neural computation. Existing whole-brain connectome models of Drosophila often rely on parameter assumptions or deep learning approaches, yet remain limited in their ability to reliably predict dynamic neuronal responses. We introduce a Multi-Path Aggregation (MPA) framework, based on neural network steady-state theory, to build a whole-brain Visual Function Profiles (VFP) of Drosophila neurons and predict their responses under diverse visual tasks. Unlike conventional methods relying on redundant parameters, MPA combines visual input features with the whole-brain connectome topology. It uses adjacency matrix powers and finite-path optimization to efficiently predict neuronal function, including ON/OFF polarity, direction selectivity, and responses to complex visual stimuli. Our model achieves a Pearson correlation of 0.84+/-0.12 for ON/OFF responses, outperforming existing methods (0.33+/-0.59), and accurately captures neuron functional properties, including luminance and direction preferences, while allowing single-neuron or population-level blockade simulations. Replacing CNN modules with VFP-derived Lobula Columnar(LC) population responses in a Drosophila simulation enables successful navigation and obstacle avoidance, demonstrating the model's effectiveness in guiding embodied behavior. This study establishes a \"connectome-functional profile-behavior\" framework, offering a whole-brain quantitative tool to study Drosophila visual computation and a neuron-level guide for brain-inspired intelligence."}
{"id": "2512.07064", "pdf": "https://arxiv.org/pdf/2512.07064", "abs": "https://arxiv.org/abs/2512.07064", "authors": ["Jiannan Yang", "Veronika Thost", "Tengfei Ma"], "title": "Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs."}
{"id": "2512.07080", "pdf": "https://arxiv.org/pdf/2512.07080", "abs": "https://arxiv.org/abs/2512.07080", "authors": ["Madison D. Griffin", "Grace S. Chiu", "Roger L. Mann", "Melissa J. Southworth", "John K. Thomas"], "title": "Big shells, bigger data: cohort analysis of Chesapeake Bay Crassostrea virginica reefs", "categories": ["stat.AP", "q-bio.QM", "stat.CO"], "comment": "29 pages, 7 figures", "summary": "Oysters in Virginia Chesapeake Bay oyster reefs are \"age-truncated\", possibly due to a combination of historical overfishing, disease epizootics, environmental degradation, and climate change. Research has suggested that oysters exhibit resilience to environmental stressors; however, that evidence is based on the current limited understanding of oyster lifespan. Until this paper, the Virginia Oyster Stock Assessment and Replenishment Archive (VOSARA), a spatially and temporally expansive dataset (222 reefs across 2003-2023) of shell lengths (SL, mm), had yet to be examined comprehensively in the context of resilience. We develop a novel method using Gaussian mixture modeling (GMM) to identify the age groups in each reef using yearly SL data and then link those age groups over time to identify cohorts and estimate their lifespan. Sixty-four reefs (29%) are deemed to have sufficient data (at least 300 oysters sampled for a minimum of 8 consecutive years) for this analysis. We fit univariate GMMs for each year ($t$) and reef ($r$) for each of the seven river strata ($R$) to estimate 1) the mean and standard deviation of SL for each $a_{Rrt}$th age group, and 2) the mixture percentage of each $a_{Rrt}$th age group. We link age groups across time to infer age cohorts by developing a mechanistic algorithm that prevents the shrinking of shell length when an $a_{Rrt}$th group becomes an ($a_{R,r,t+1}$)th group. Our method shows promise in identifying oyster cohorts and estimating lifespan solely using SL data. Our results show signals of resiliency in almost all river systems: oyster cohorts live longer and grow larger in the mid-to-late 2010s compared to the early 2000s."}
{"id": "2512.07153", "pdf": "https://arxiv.org/pdf/2512.07153", "abs": "https://arxiv.org/abs/2512.07153", "authors": ["Maatla Sefawe", "Sravya Ganti", "Julianna Segalla", "Erwei He", "Isaac Tourner", "Julia Gersey"], "title": "A Structured Review of Fixed and Multimodal Sensing Techniques for Bat Monitoring", "categories": ["eess.SY", "q-bio.QM"], "comment": null, "summary": "Effective monitoring of mobile animal populations is crucial for ecological research, wildlife management, and agricultural applications. Monitoring of bats specifically can help understand the spread of disease as well as shine light on bat migration patterns, population dynamics, and the impacts of environmental changes on bat colonies. Fixed sensing modalities, such as infrared sensors, cameras, radar, and acoustic detectors, play a pivotal role in tracking and understanding animal behavior. This survey goes over context-informing details about bat biology, and then reviews these fixed sensing modalities, discussing the unique challenges and contributions of each approach. We highlight the coverage, applications, accuracy, and limitations associated with each of these sensing modalities. By synthesizing recent advances, we provide a comprehensive overview to guide future research in this area."}
{"id": "2512.07772", "pdf": "https://arxiv.org/pdf/2512.07772", "abs": "https://arxiv.org/abs/2512.07772", "authors": ["Udo Seifert"], "title": "Universal bounds on entropy production from fluctuating coarse-grained trajectories", "categories": ["cond-mat.stat-mech", "q-bio.QM"], "comment": null, "summary": "Entropy production is arguably the most universally applicable measure of non-equilibrium behavior, particularly for systems coupled to a heat bath. This setting encompasses driven soft matter as well as biomolecular, biochemical, and biophysical systems. Despite its central role, direct measurements of entropy production remain challenging - especially in small systems dominated by fluctuations. The main difficulty arises because not all degrees of freedom contributing to entropy production are experimentally accessible. A key question, therefore, is how to infer entropy production from coarse-grained observations, such as time series of experimentally measurable variables. Over the past decade, stochastic thermodynamics has provided several inequalities that yield model-free lower bounds on entropy production from such coarse-grained data. The major approaches rely on observations of coarse-grained states, fluctuating currents or ticks, correlation functions of coarse-grained observables, and waiting-time distributions between so-called Markovian events, which correspond to transitions between mesoscopic states. Here, we systematically review these techniques valid under the sole assumption of a Markovian, i.e., memoryless, dynamics on an underlying, not necessarily observable, network of states or following a possibly high-dimensional Langevin equation. We discuss in detail the large class of non-equilibrium steady states and highlight extensions of these methods to time-dependent and relaxing systems. While our focus is on mean entropy production, we also summarize recent progress in quantifying entropy production along individual coarse-grained trajectories."}
