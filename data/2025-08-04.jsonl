{"id": "2508.00578", "pdf": "https://arxiv.org/pdf/2508.00578", "abs": "https://arxiv.org/abs/2508.00578", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gr√§ter", "Pascal Friederich"], "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments."}
{"id": "2508.00781", "pdf": "https://arxiv.org/pdf/2508.00781", "abs": "https://arxiv.org/abs/2508.00781", "authors": ["Niusha Mirhakimi", "Yohan Chatelain", "Jean-Baptiste Poline", "Tristan Glatard"], "title": "Numerical Uncertainty in Linear Registration: An Experimental Study", "categories": ["q-bio.QM", "eess.IV"], "comment": null, "summary": "While linear registration is a critical step in MRI preprocessing pipelines,\nits numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA)\nsimulations, we assessed the most commonly used linear registration tools\nwithin major software packages (SPM, FSL, and ANTs) across multiple image\nsimilarity measures, two brain templates, and both healthy control (HC, n=50)\nand Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the\ninfluence of linear registration tools and similarity measures on numerical\nstability. Among the evaluated tools and with default similarity measures, SPM\nexhibited the highest stability. FSL and ANTs showed greater and similar ranges\nof variability, with ANTs demonstrating particular sensitivity to numerical\nperturbations that occasionally led to registration failure. Furthermore, no\nsignificant differences were observed between healthy and PD cohorts,\nsuggesting that numerical stability analyses obtained with healthy subjects may\ngeneralise to clinical populations. Finally, we also demonstrated how numerical\nuncertainty measures may support automated quality control (QC) of linear\nregistration results. Overall, our experimental results characterize the\nnumerical stability of linear registration experimentally and can serve as a\nbasis for future uncertainty analyses."}
{"id": "2508.00150", "pdf": "https://arxiv.org/pdf/2508.00150", "abs": "https://arxiv.org/abs/2508.00150", "authors": ["Poulami Chatterjee", "Cesar Nieto", "Juan Manuel Pedraza", "Abhyudai Singh"], "title": "Information and fitness in two-state systems: self-replicating individuals in a fluctuating environment", "categories": ["q-bio.PE"], "comment": null, "summary": "A population of individuals with the same genes can present heterogeneous\ntraits (phenotypes). The prevalence of this heterogeneity can be explained as a\nbet-hedging strategy that improves the population proliferation rate (fitness)\nin fluctuating environments. The phenotype distribution is influenced by\nfactors such as competition between phenotypes, the duration of environmental\nstates, and the rate of phenotype-switching. We illustrate these effects in a\nsystem where both the environment and the phenotype can adopt two states. This\nsystem includes scenarios such as symmetric bet-hedging and\ndormant-proliferating phenotypes. We examine how environmental and phenotypic\nstates share mutual information, measured in bits, and explore the relationship\nbetween this information and population fitness. We propose that when fitness\nis measured relative to the case where phenotype and environment are\nindependent, information and fitness can be treated as equivalent measures. We\ninvestigate strategies that individuals can use to improve this information,\nsuch as adjusting the rates of proliferation and phenotype-switching relative\nto the environmental fluctuation rate. Through these strategies, with fixed\nmarginal distributions, an increase in information implies an increase in\npopulation fitness. We also identify limits to the maximum achievable fitness\nand information and discuss the value of the information in terms of this new\nnormalized fitness. Our framework offers new insights into how organisms adapt\nto fluctuating environmental conditions."}
{"id": "2508.00190", "pdf": "https://arxiv.org/pdf/2508.00190", "abs": "https://arxiv.org/abs/2508.00190", "authors": ["Larissa Albantakis"], "title": "On the utility of toy models for theories of consciousness", "categories": ["q-bio.NC"], "comment": "To appear in the forthcoming Springer-Nature book \"The Scientific\n  Study of Consciousness: Experimental and Theoretical Approaches.\"", "summary": "Toy models are highly idealized and deliberately simplified models that\nretain only the essential features of a system in order to explore specific\ntheoretical questions. Long used in physics and other sciences, they have\nrecently begun to play a more visible role in consciousness research. This\nchapter examines the potential utility of toy models for developing and\nevaluating scientific theories of consciousness in terms of their ability to\nclarify theoretical frameworks, test assumptions, and illuminate philosophical\nchallenges. Drawing primarily on examples from Integrated Information Theory\n(IIT) and Global Workspace Theory (GWT), I show how these simplified systems\ncould make abstract concepts more tangible, enabling researchers to probe the\ncoherence, consistency, and implications of competing frameworks. In addition\nto supporting theory development, toy models can also address specific features\nof experience, as exemplified by the account of spatial extendedness and\ntemporal flow provided by integrated information theory (IIT) and recent\ntheory-independent structural approaches. Moreover, toy models bring\nphilosophical debates into sharper focus, such as the distinction between\nfunctional and structural theories of consciousness. By bridging abstract\nclaims and empirical inquiry, toy models provide essential insights into the\nchallenges of building comprehensive theories of consciousness."}
{"id": "2508.00769", "pdf": "https://arxiv.org/pdf/2508.00769", "abs": "https://arxiv.org/abs/2508.00769", "authors": ["Allison E. Andrews", "Hugh Dickinson", "James P. Hague"], "title": "Designing cultured tissue moulds using evolutionary strategies", "categories": ["physics.bio-ph", "q-bio.TO"], "comment": "9 pages, 7 figures, 4 tables", "summary": "There is an unmet need for artificial intelligence techniques that can speed\nup the design of growth strategies for cultured tissues. Cultured tissue is\nincreasingly important for a range of applications such as cultivated meat,\npharmaceutical assays and regenerative medicine. In this paper, we introduce a\nmethod based around evolutionary strategies, machine learning and biophysical\nsimulations that can be used to speed up the process of identifying new tissue\ngrowth strategies for these diverse applications. We demonstrate the method by\ndesigning tethering strategies to grow tissues containing various cell types\nwith desirable properties such as high cellular alignment and uniform density."}
{"id": "2508.00164", "pdf": "https://arxiv.org/pdf/2508.00164", "abs": "https://arxiv.org/abs/2508.00164", "authors": ["Sourya Sengupta", "Jianquan Xu", "Phuong Nguyen", "Frank J. Brooks", "Yang Liu", "Mark A. Anastasio"], "title": "On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity", "categories": ["eess.IV", "q-bio.QM"], "comment": null, "summary": "Virtual staining, or in-silico-labeling, has been proposed to computationally\ngenerate synthetic fluorescence images from label-free images by use of deep\nlearning-based image-to-image translation networks. In most reported studies,\nvirtually stained images have been assessed only using traditional image\nquality measures such as structural similarity or signal-to-noise ratio.\nHowever, in biomedical imaging, images are typically acquired to facilitate an\nimage-based inference, which we refer to as a downstream biological or clinical\ntask. This study systematically investigates the utility of virtual staining\nfor facilitating clinically relevant downstream tasks (like segmentation or\nclassification) with consideration of the capacity of the deep neural networks\nemployed to perform the tasks. Comprehensive empirical evaluations were\nconducted using biological datasets, assessing task performance by use of\nlabel-free, virtually stained, and ground truth fluorescence images. The\nresults demonstrated that the utility of virtual staining is largely dependent\non the ability of the segmentation or classification task network to extract\nmeaningful task-relevant information, which is related to the concept of\nnetwork capacity. Examples are provided in which virtual staining does not\nimprove, or even degrades, segmentation or classification performance when the\ncapacity of the associated task network is sufficiently large. The results\ndemonstrate that task network capacity should be considered when deciding\nwhether to perform virtual staining."}
{"id": "2508.00527", "pdf": "https://arxiv.org/pdf/2508.00527", "abs": "https://arxiv.org/abs/2508.00527", "authors": ["Roaa Mohmmed Yagb Omer", "Onofrio Mazzarisi", "Martina Dal Bello", "Jacopo Grilli"], "title": "Evolutionary learning of microbial populations in partially predictable environments", "categories": ["q-bio.PE", "nlin.AO", "physics.bio-ph", "q-bio.CB"], "comment": null, "summary": "Populations evolving in fluctuating environments face the fundamental\nchallenge of balancing adaptation to current conditions against preparation for\nuncertain futures. Here, we study microbial evolution in partially predictable\nenvironments using proteome allocation models that capture the trade-off\nbetween growth rate and lag time during environmental transitions. We\ndemonstrate that evolution drives populations toward an evolutionary stable\nallocation strategy that minimizes resource depletion time, thereby balancing\nfaster growth with shorter adaptation delays. In environments with temporal\nstructure, populations evolve to learn the statistical patterns of\nenvironmental transitions through proteome pre-allocation, with the evolved\nallocations reflecting the transition probabilities between conditions. Our\nframework reveals how microbial populations can extract and exploit\nenvironmental predictability without explicit neural computation, using the\nproteome as a distributed memory system that encodes environmental patterns.\nThis work demonstrates how information-theoretic principles govern cellular\nresource allocation and provides a mechanistic foundation for understanding\nlearning-like behavior in evolving biological systems."}
{"id": "2508.00191", "pdf": "https://arxiv.org/pdf/2508.00191", "abs": "https://arxiv.org/abs/2508.00191", "authors": ["Kevin S. Chen", "Andrew M. Leifer", "Jonathan W. Pillow"], "title": "State-switching navigation strategies in C. elegans are beneficial for chemotaxis", "categories": ["q-bio.NC"], "comment": "25 pages, 15 figures", "summary": "Animals employ different strategies for relating sensory input to behavioral\noutput to navigate sensory environments, but what strategy to use, when to\nswitch and why remain unclear. In C. elegans, navigation is composed of\n'steering' and 'turns', corresponding to small heading changes and large\nreorientation events, respectively. It is unclear whether transitions between\nthese elements are driven solely by sensory input or are influenced by internal\nstates that persist over time. It also remains unknown how worms accomplish\nseemingly surprising feats of navigation--for example, worms appear to exit\nturns correctly oriented toward a goal, despite their presumed lack of spatial\nawareness during the turn. Here, we resolve these questions using detailed\nmeasurements of sensory-guided navigation and a novel statistical model of\nstate-dependent navigation. We show that the worm's navigation is well\ndescribed by a sensory-driven state-switching model with two distinct states,\neach persisting over many seconds and producing different mixtures of\nsensorimotor relations. One state is enriched for steering, while the other is\nenriched for turning. This hierarchical, temporal organization of strategies\nchallenges the previous assumption that strategies are static over time and\ndriven solely by immediate sensory input. Sensory input causally drives\ntransitions between these persistent internal states, and creates the\nappearance of 'directed turns.' Genetic perturbations and a data-constrained\nreinforcement learning model demonstrate that state-switching enhances\ngradient-climbing performance. By combining measurement, perturbation, and\nmodeling, we show that state-switching plays a functionally beneficial role in\norganizing behavior over time--a principle likely to generalize across species\nand contexts."}
{"id": "2508.00531", "pdf": "https://arxiv.org/pdf/2508.00531", "abs": "https://arxiv.org/abs/2508.00531", "authors": ["Jack A. Kilgallen", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "title": "The Repeated-Stimulus Confound in Electroencephalography", "categories": ["q-bio.NC", "cs.CV", "62K99, 68T05"], "comment": "15 pages, 6 figures, 8 tables, in submission to IEEE", "summary": "In neural-decoding studies, recordings of participants' responses to stimuli\nare used to train models. In recent years, there has been an explosion of\npublications detailing applications of innovations from deep-learning research\nto neural-decoding studies. The data-hungry models used in these experiments\nhave resulted in a demand for increasingly large datasets. Consequently, in\nsome studies, the same stimuli are presented multiple times to each participant\nto increase the number of trials available for use in model training. However,\nwhen a decoding model is trained and subsequently evaluated on responses to the\nsame stimuli, stimulus identity becomes a confounder for accuracy. We term this\nthe repeated-stimulus confound. We identify a susceptible dataset, and 16\npublications which report model performance based on evaluation procedures\naffected by the confound. We conducted experiments using models from the\naffected studies to investigate the likely extent to which results in the\nliterature have been misreported. Our findings suggest that the decoding\naccuracies of these models were overestimated by between 4.46-7.42%. Our\nanalysis also indicates that per 1% increase in accuracy under the confound,\nthe magnitude of the overestimation increases by 0.26%. The confound not only\nresults in optimistic estimates of decoding performance, but undermines the\nvalidity of several claims made within the affected publications. We conducted\nfurther experiments to investigate the implications of the confound in\nalternative contexts. We found that the same methodology used within the\naffected studies could also be used to justify an array of pseudoscientific\nclaims, such as the existence of extrasensory perception."}
{"id": "2508.00425", "pdf": "https://arxiv.org/pdf/2508.00425", "abs": "https://arxiv.org/abs/2508.00425", "authors": ["Maxx Yung"], "title": "Design, Simulation, and Fabrication of a Hexagonal Microfluidic Platform for Culturing Neurons", "categories": ["physics.flu-dyn", "physics.bio-ph", "q-bio.NC"], "comment": "10 pages, 17 figures", "summary": "Developing an organoid computing platform from neurons in vitro demands\nstable, precisely controlled microenvironments. To address this requirement, we\ndesigned, simulated, and fabricated a microfluidic device featuring hexagonal\nwells ($34.64\\,\\mathrm{\\mu m}$ side length) in a honeycomb array connected by\n$20\\,\\mathrm{\\mu m}$ channels. Computational fluid dynamics (CFD) modeling,\nvalidated by high mesh quality ($0.934$ orthogonal quality) and robust\nconvergence, confirmed the architecture supports flow regimes ideal for\nensuring cell viability. At target flow rates of $0.1$ - $1\\,\\mathrm{\\mu\nL/min}$, simulations revealed the extrapolated pressure differential across the\nfull $50{,}000\\,\\mathrm{\\mu m}$ device remains within stable operating limits\nat $177\\,\\mathrm{kPa}$ (average) and $329\\,\\mathrm{kPa}$ (maximum).\nPhotolithography successfully produced this architecture, with only minor\ncorner rounding observed at feature interfaces. This work therefore establishes\na computationally validated and fabricated platform, paving the way for\nexperimental flow characterization and subsequent neural integration."}
